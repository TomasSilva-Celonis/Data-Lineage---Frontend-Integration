{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0be7e7",
   "metadata": {},
   "source": [
    "## Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8eb48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pycelonis import get_celonis\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pycelonis.ems import ColumnTransport, ColumnType\n",
    "import traceback\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a32424",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.environ['CELONIS_URL'] = 'https://lineage.develop.celonis.cloud'\n",
    "#API Key for this specific url\n",
    "api_token = os.environ['CELONIS_API_TOKEN'] = 'MDE5YTE2M2UtZmU5YS03NTFkLWFjYmYtZGQ0NWQxODJjZmYzOmc2ZTY2UUkyU3R2RFkxVTA2L3VNc0tiZUxaVmZneHR0RVRuVS9ETFJWS3or'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99dd42a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" url = os.environ['CELONIS_URL'] = 'https://snap-tiger-team-celonis-com.eu-1.celonis.cloud'\\n#API Key for this specific url\\napi_token = os.environ['CELONIS_API_TOKEN'] = 'Nzk3MTliYWUtM2U4ZC00MmJmLWFlMGMtMzY5YmIxZWRhNDgxOjVMQ1lxaGwyeGZLbWxvbGVOeTkzYzcrZEQ1cTgzSnlDN29CUXpFNlRRMXZG' \""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" url = os.environ['CELONIS_URL'] = 'https://snap-tiger-team-celonis-com.eu-1.celonis.cloud'\n",
    "#API Key for this specific url\n",
    "api_token = os.environ['CELONIS_API_TOKEN'] = 'Nzk3MTliYWUtM2U4ZC00MmJmLWFlMGMtMzY5YmIxZWRhNDgxOjVMQ1lxaGwyeGZLbWxvbGVOeTkzYzcrZEQ1cTgzSnlDN29CUXpFNlRRMXZG' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" url = os.environ['CELONIS_URL'] = 'https://ems-two.eu-1.celonis.cloud'\n",
    "#API Key for this specific url\n",
    "api_token = os.environ['CELONIS_API_TOKEN'] = 'MWFkMTk0YzctMmJjNC00MWMwLTg2NzAtY2M5YjdjOGViZDE4OkpSVFVkQVpIQ0lGcGd4RExQMjN6RUhPcEw2MVFHV2lDMm0ydFlDSStIaFBV' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5e5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Celonis object\n",
    "c = get_celonis(base_url = url, api_token = api_token, key_type='USER_KEY') # adjust base_url and api_token accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa002f1",
   "metadata": {},
   "source": [
    "## Studio Lineage (Views + Knowledge Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1969e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge_model_key(view_node):\n",
    "    km_key = json.loads(view_node.serialized_content)['metadata'].get('knowledgeModelKey')\n",
    "    return km_key\n",
    "\n",
    "def get_data_model_variable(knowledge_model):\n",
    "    try:\n",
    "        serialized = json.loads(knowledge_model.serialized_content)\n",
    "        data_model_expr = serialized.get(\"dataModelId\")\n",
    "        match = None\n",
    "        if isinstance(data_model_expr, str):\n",
    "            match = re.match(r'\\${{([a-zA-Z0-9_]+)}}', data_model_expr)\n",
    "        if not match:\n",
    "            print('ERROR: No valid Data Model Variable')\n",
    "            return None\n",
    "        return match.group(1)\n",
    "    except (KeyError, json.JSONDecodeError, AttributeError) as e:\n",
    "        print('ERROR: No Data Model Assigned')\n",
    "        return None\n",
    "\n",
    "data_pools = c.data_integration.get_data_pools()\n",
    "\n",
    "def find_data_pool_id(data_model_id):\n",
    "    data_pool_id = None\n",
    "    for data_pool in data_pools:\n",
    "        data_models = data_pool.get_data_models()\n",
    "        for data_model in data_models:\n",
    "            if data_model.id == data_model_id:\n",
    "                data_pool_id = data_pool.id\n",
    "                break\n",
    "    return data_pool_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7356550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineage API call\n",
    "def get_lineage(celonis, knowledge_model_key):\n",
    "    try:\n",
    "        lineage = celonis.client.request(\n",
    "            method='get',\n",
    "            url=f'/semantic-layer/api/usage/by-semantic-model/{knowledge_model_key}',\n",
    "            parse_json =True\n",
    "        )\n",
    "        return lineage\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63c02b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_km_attribute_details(knowledge_model, data_model): \n",
    "    \"\"\"\n",
    "    Extracts attribute details and finds the 'table_schema' for each table.\n",
    "    FIXED: Manually constructs schema string to avoid 'bound method' errors.\n",
    "    \"\"\"\n",
    "    attribute_details = {}\n",
    "    kpi_details = {}\n",
    "    \n",
    "    try:\n",
    "        km_content = json.loads(knowledge_model.serialized_content)\n",
    "        \n",
    "        # --- Create a lookup map for table properties ---\n",
    "        table_properties = {}\n",
    "        if data_model:\n",
    "            pool_id = find_data_pool_id(data_model.id)\n",
    "            for table in data_model.get_tables():\n",
    "                # SAFELY construct the schema string\n",
    "                # This matches your Backend logic: pool_id + \"_\" + data_source_id\n",
    "                if table.data_source_id:\n",
    "                    safe_schema = f\"{pool_id}_{table.data_source_id}\"\n",
    "                else:\n",
    "                    safe_schema = pool_id\n",
    "\n",
    "                # Store schema by lowercase table alias\n",
    "                table_properties[table.alias_or_name.lower()] = {\n",
    "                    'table_schema': safe_schema\n",
    "                }\n",
    "        \n",
    "        # Process record attributes\n",
    "        if 'records' in km_content:\n",
    "            for record in km_content['records']:\n",
    "                record_id = record.get('id') # This is the table alias\n",
    "                table_info = table_properties.get(record_id.lower(), {})\n",
    "                \n",
    "                if 'attributes' not in record: continue\n",
    "                \n",
    "                for attr in record['attributes']:\n",
    "                    attr_id = attr.get('id')\n",
    "                    pql = attr.get('pql')\n",
    "                    key = f\"{record_id}.{attr_id}\".lower()\n",
    "                    \n",
    "                    if pql and pql.strip(): source_type = 'CALCULATED'\n",
    "                    else: source_type = 'AUTO_GENERATED'\n",
    "                    \n",
    "                    attribute_details[key] = {\n",
    "                        'source_type': source_type,\n",
    "                        'pql': pql if pql else '',\n",
    "                        'table_schema': table_info.get('table_schema', '') # Defaults to empty string, not None\n",
    "                    }\n",
    "        \n",
    "        # Process KPIs \n",
    "        if 'kpis' in km_content:\n",
    "            for i, kpi in enumerate(km_content['kpis']):\n",
    "                kpi_id = kpi.get('id')\n",
    "                kpi_pql = kpi.get('pql', '')\n",
    "                kpi_name = kpi.get('displayName', kpi_id)\n",
    "                if kpi_id:\n",
    "                    kpi_details[kpi_id] = {'name': kpi_name, 'pql': kpi_pql, 'format': kpi.get('format', '')}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract details: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "        \n",
    "    return attribute_details, kpi_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd68fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_view_usages(lineage, usage: list, metadata: dict, attribute_details: dict, kpi_details: dict):\n",
    "    ## View usages\n",
    "    \n",
    "    # Record --> View\n",
    "    parent_key = 'viewUsages'\n",
    "    key = \"recordAttributeReferences\"\n",
    "    for record, attributes in lineage[parent_key][key].items():\n",
    "        for attribute, views in attributes.items():\n",
    "            # Get attribute details\n",
    "            attr_key = f\"{record}.{attribute}\".lower()\n",
    "            attr_info = attribute_details.get(attr_key, {'source_type': 'AUTO_GENERATED', 'pql': ''})\n",
    "            \n",
    "            for view in views:\n",
    "                usage.append({\n",
    "                    'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                    'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                    'SOURCE_ID': f'{record}.{attribute}', \n",
    "                    'SOURCE_NAME': record,\n",
    "                    'SOURCE_ATTRIBUTE': attribute,\n",
    "                    'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                    'SOURCE_TYPE': attr_info['source_type'],\n",
    "                    'SOURCE_PQL': attr_info['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                    'TARGET_ID': view.get('nodeId'),\n",
    "                    'TARGET_NAME': view.get('assetName'),\n",
    "                    'TARGET_ATTRIBUTE': None,\n",
    "                    'TARGET_NODE_TYPE': 'VIEW',\n",
    "                    'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                    'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                    'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                    'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                    'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                })\n",
    "\n",
    "    # KPI --> View\n",
    "    key = 'kpiReferences'\n",
    "    \n",
    "    for kpi, views in lineage[parent_key][key].items():\n",
    "        kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "        \n",
    "        for view in views:\n",
    "            usage.append({\n",
    "                'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                'SOURCE_ID': f'{kpi}', \n",
    "                'SOURCE_NAME': kpi,\n",
    "                'SOURCE_ATTRIBUTE': None,\n",
    "                'SOURCE_NODE_TYPE': 'KPI',\n",
    "                'SOURCE_TYPE': '',\n",
    "                'SOURCE_PQL': kpi_info['pql'],\n",
    "                'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                'TARGET_ID': view.get('nodeId'),\n",
    "                'TARGET_NAME': view.get('assetName'),\n",
    "                'TARGET_NODE_TYPE': 'VIEW',\n",
    "                'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                'DATA_SOURCE_ID': '',\n",
    "                'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "            })\n",
    "    \n",
    "    return usage\n",
    "\n",
    "def update_with_km_usages(lineage, usage, metadata, attribute_details: dict, kpi_details: dict):\n",
    "    ## Knowledge Model usages\n",
    "    \n",
    "    # Record --> KM property\n",
    "    parent_key = 'knowledgeModelUsages'\n",
    "    key = \"recordAttributeUsages\"\n",
    "    properties = ['kpis', 'filters', 'attributes', 'flags']\n",
    "    map_id = {'kpis': 'id', 'filters': 'id', 'attributes': 'recordId', 'flags': 'id'}\n",
    "\n",
    "    for property in properties:\n",
    "        for record, attributes in lineage[parent_key][key][property].items():\n",
    "            for attribute, props in attributes.items():\n",
    "                # Get attribute details\n",
    "                attr_key = f\"{record}.{attribute}\".lower()\n",
    "                attr_info = attribute_details.get(attr_key, {'source_type': 'AUTO_GENERATED', 'pql': ''})\n",
    "                \n",
    "                for prop in props:\n",
    "                    prop_id = prop.get(map_id.get(property), 'id')\n",
    "                    prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                    usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{record}.{attribute}'.lower(), \n",
    "                        'SOURCE_NAME': record,\n",
    "                        'SOURCE_ATTRIBUTE': attribute,\n",
    "                        'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                        'SOURCE_TYPE': attr_info['source_type'],\n",
    "                        'SOURCE_PQL': attr_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "    \n",
    "    # KPI --> KM property\n",
    "    key = 'kpiUsages'\n",
    "    for property in properties:\n",
    "        for kpi, props in lineage[parent_key][key][property].items():\n",
    "            kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "            \n",
    "            for prop in props:\n",
    "                prop_id = prop.get(map_id.get(property), 'id')\n",
    "                prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{kpi}'.lower(), \n",
    "                        'SOURCE_NAME': kpi,\n",
    "                        'SOURCE_ATTRIBUTE': None,\n",
    "                        'SOURCE_NODE_TYPE': 'KPI',\n",
    "                        'SOURCE_TYPE': '',\n",
    "                        'SOURCE_PQL': kpi_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "                \n",
    "    return usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c08ce",
   "metadata": {},
   "source": [
    "## Bridge Table - Integration between backend and frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a5c3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bridge_links(data_model, knowledge_model_id, pool_table_lookup, data_pool_id, data_pool_name):\n",
    "    \"\"\"\n",
    "    Optimized version that accepts data_pool_id directly instead of searching for it.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        dm_id = data_model.id\n",
    "        \n",
    "        # Iterate tables in the Data Model (Logical layer)\n",
    "        for table in data_model.get_tables():\n",
    "            dm_table_alias = table.alias_or_name\n",
    "            \n",
    "            # Look up physical info from our cache (Physical layer)\n",
    "            physical_info = pool_table_lookup.get(table.name.lower())\n",
    "            \n",
    "            if physical_info:\n",
    "                dict_schema = physical_info['schema'].lower()\n",
    "                dict_name = physical_info['name'].lower()\n",
    "                \n",
    "                source_node_raw = f\"{dict_schema}_{dict_name}\"\n",
    "                source_node_prefixed = f\"DATA_MODEL_TABLE_{dict_schema}_{dm_id}_{dm_table_alias}\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Create Links for Columns\n",
    "            for column in table.get_columns():\n",
    "                target_node = f\"{knowledge_model_id}.{dm_table_alias}.{column.name}\".lower()\n",
    "                \n",
    "                link_data = {\n",
    "                    \"target_node\": target_node,\n",
    "                    \"task_target\": \"DATA_MODEL_COLUMN\",\n",
    "                    \"data_pool_id\": data_pool_id,\n",
    "                    \"data_pool_name\": data_pool_name,\n",
    "                    \"data_model_id\": dm_id, \n",
    "                    \"data_model_name\": data_model.name\n",
    "                }\n",
    "                \n",
    "                links.append({\"source_node\": source_node_raw, **link_data})\n",
    "                links.append({\"source_node\": source_node_prefixed, **link_data})\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not create bridge links for DM {data_model.name}: {e}\")\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa3aac",
   "metadata": {},
   "source": [
    "## Execution Block - Scan the environment and generate lineage and link table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c716c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Pre-mapping Data Models to Data Pools...\n",
      "Mapped 27 Data Models. Starting Scan...\n",
      "Space: ahilmer - lineage test\n",
      "  Package: Monitoring\n",
      "ERROR: No valid Data Model Variable\n",
      "ERROR: No valid Data Model Variable\n",
      "ERROR: No valid Data Model Variable\n",
      "Space: Flo\n",
      "  Package: Monitoring\n",
      "ERROR: No valid Data Model Variable\n",
      "ERROR: No valid Data Model Variable\n",
      "ERROR: No valid Data Model Variable\n",
      "Space: FS\n",
      "  Package: Data Lineage\n",
      "    ...Caching tables for pool: Monitoring Pool\n",
      "      Success: KM - Data Lineage\n",
      "Space: Paula's Space\n",
      "  Package: Data Lineage - Test\n",
      "    ...Caching tables for pool: test pool - data lineage app [PC]\n",
      "      Success: KM\n",
      "  Package: Data Lineage - Monitoring Pool version\n",
      "      Success: KM - Data Lineage\n",
      "  Package: Monitoring - Data Lineage\n",
      "      Success: KM - Data Lineage\n",
      "  Package: Monitoring - Data Lineage Sarvesh Test\n",
      "    ...Caching tables for pool: Sarvesh Monitoring Pool\n",
      "      Success: KM - Data Lineage\n",
      "      Success: Studio Lineage\n",
      "  Package: Demo\n",
      "    ...Caching tables for pool: OCPM Data Pool\n",
      "      Success: test:perspective_celonis_AccountsReceivable KM\n",
      "  Package: Monitoring - Data Lineage Copy\n",
      "      Success: KM - Data Lineage\n",
      "  Package: Data Lineage - New Version\n",
      "  Package: Celonis Data Lineage Latest\n",
      "      Success: KM - Data Lineage\n",
      "  Package: full lineage\n",
      "      Success: lineage KM\n",
      "  Package: dummy_package_tomas\n",
      "      Success: dummy_peopleDM KM\n",
      "  Package: test_lineageV2\n",
      "      Success: Data Lineage Monitoring - Latest_V2 KM\n",
      "Space: TomÃ¡s\n",
      "  Package: E2E Lineage\n",
      "    ...Caching tables for pool: lineage_tomas\n",
      "      Success: full_lineage KM\n",
      "  Package: testtable\n",
      "      Success: tomas_datamodel KM\n",
      "  Package: backend lineage\n",
      "      Success: Data Lineage Monitoring - Latest KM\n",
      "\n",
      "Processing complete!\n",
      "Successfully processed 15 knowledge models\n",
      "Saved bridge_lineage_mapping.csv with 6114 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-mapping Data Models to Data Pools...\")\n",
    "dm_id_mapping = {} \n",
    "\n",
    "all_pools = c.data_integration.get_data_pools()\n",
    "for pool in all_pools:\n",
    "    try:\n",
    "        # Pre-fetch models so we don't have to search later\n",
    "        for dm in pool.get_data_models():\n",
    "            dm_id_mapping[dm.id] = {'pool': pool, 'model': dm}\n",
    "    except:\n",
    "        pass\n",
    "print(f\"Mapped {len(dm_id_mapping)} Data Models. Starting Scan...\")\n",
    "\n",
    "\n",
    "# Main Execution Block ---\n",
    "pool_table_cache = {} \n",
    "usage = []\n",
    "all_bridge_links = []\n",
    "all_metadata = []\n",
    "error_km_count = 0\n",
    "processed_km_count = 0\n",
    "\n",
    "for space in c.studio.get_spaces():\n",
    "    print(f\"Space: {space.name}\")\n",
    "    \n",
    "    for package in space.get_packages():\n",
    "        print(f\"  Package: {package.name}\")\n",
    "\n",
    "        try:\n",
    "            knowledge_models = package.get_knowledge_models()\n",
    "            if not knowledge_models: continue\n",
    "            \n",
    "            # Get variables once per package\n",
    "            try:\n",
    "                package_variables = package.get_variables()\n",
    "            except:\n",
    "                package_variables = []\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error accessing package: {e}\")\n",
    "            continue\n",
    "        \n",
    "        for knowledge_model in knowledge_models:\n",
    "            try:\n",
    "                # 1. Resolve Data Model ID\n",
    "                dm_variable = get_data_model_variable(knowledge_model)\n",
    "                if not dm_variable: continue # Skip silently\n",
    "                \n",
    "                data_model_id = None\n",
    "                try:\n",
    "                    if package_variables:\n",
    "                        variable_obj = package_variables.find(dm_variable, \"key\")\n",
    "                        if variable_obj: data_model_id = variable_obj.value\n",
    "                except: pass\n",
    "\n",
    "                if not data_model_id: continue\n",
    "\n",
    "                # 2. FAST LOOKUP (No API calls)\n",
    "                cached_data = dm_id_mapping.get(data_model_id)\n",
    "                \n",
    "                if cached_data:\n",
    "                    data_pool = cached_data['pool']\n",
    "                    data_model = cached_data['model']\n",
    "                    data_pool_id = data_pool.id\n",
    "\n",
    "                    # 3. POOL TABLE CACHE\n",
    "                    if data_pool_id in pool_table_cache:\n",
    "                        pool_table_lookup = pool_table_cache[data_pool_id]\n",
    "                    else:\n",
    "                        print(f\"    Caching tables for pool: {data_pool.name}\")\n",
    "                        pool_table_lookup = {}\n",
    "                        try:\n",
    "                            # Robust table fetching\n",
    "                            for pool_table in data_pool.get_tables():\n",
    "                                phys_schema = getattr(pool_table, 'schema_name', f\"{data_pool.id}_{pool_table.data_source_id}\")\n",
    "                                pool_table_lookup[pool_table.name.lower()] = {'schema': phys_schema, 'name': pool_table.name}\n",
    "                            pool_table_cache[data_pool_id] = pool_table_lookup\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error caching tables: {e}\")\n",
    "                    \n",
    "                    # 4. Generate Links\n",
    "                    bridge_links = create_bridge_links(\n",
    "                        data_model, \n",
    "                        knowledge_model.id, \n",
    "                        pool_table_lookup,\n",
    "                        data_pool.id,      # Pass ID directly\n",
    "                        data_pool.name     # Pass Name directly\n",
    "                    )\n",
    "                    all_bridge_links.extend(bridge_links)\n",
    "\n",
    "                    # 5. Metadata & Lineage\n",
    "                    metadata = {\n",
    "                        'data_model_id': data_model_id, \n",
    "                        'data_model_name': data_model.name,\n",
    "                        'data_pool_id': data_pool.id,\n",
    "                        'data_pool_name': data_pool.name,\n",
    "                        'root_id': package.id, \n",
    "                        'space_id': space.id,\n",
    "                        'knowledge_model_key': knowledge_model.key,\n",
    "                        'knowledge_model_id': knowledge_model.id\n",
    "                    }\n",
    "\n",
    "                    attribute_details, kpi_details = get_km_attribute_details(knowledge_model, data_model)\n",
    "                    lineage = get_lineage(c, knowledge_model.root_with_key)\n",
    "                    \n",
    "                    if lineage and 'viewUsages' in lineage:\n",
    "                        update_with_view_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "                        update_with_km_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "                        all_metadata.append(metadata)\n",
    "                        processed_km_count += 1\n",
    "                        print(f\"      Success: {knowledge_model.name}\")\n",
    "                    else:\n",
    "                        print(f\"      Skipped (No Lineage): {knowledge_model.name}\")\n",
    "\n",
    "                else:\n",
    "                    # DM ID exists but wasn't found in our pre-scan (permission issue?)\n",
    "                    error_km_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                # DEBUG PRINT ENABLED\n",
    "                print(f\"    Error processing KM '{knowledge_model.name}': {e}\")\n",
    "                error_km_count += 1\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Successfully processed {processed_km_count} knowledge models\")\n",
    "\n",
    "df_studio = pd.DataFrame(usage)\n",
    "df_bridge = pd.DataFrame(all_bridge_links).drop_duplicates()\n",
    "df_bridge.to_csv('bridge_lineage_mapping.csv', index=False)\n",
    "print(f\"Saved bridge_lineage_mapping.csv with {len(df_bridge)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d92f8",
   "metadata": {},
   "source": [
    "## Data Models Loads - Extract all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4695b0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '34ca0a04-a73c-4ea8-ba73-f1863cf8471d',\n",
       "  'space_id': '24b90297-f60c-4ad2-b700-87ad401b9961',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'dd35ad1d-5ba1-44dc-b9a0-533587643b35'},\n",
       " {'data_model_id': '1e846159-f3a9-42a3-9d91-b3c7c0c7f904',\n",
       "  'data_model_name': 'data lineage - test',\n",
       "  'data_pool_id': 'ebdcfc35-b1c0-4a8c-bf5c-ae3c5f995bab',\n",
       "  'data_pool_name': 'test pool - data lineage app [PC]',\n",
       "  'root_id': 'c9b98018-21f1-4f3b-86b0-e1ba393125a5',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_data_lineage_test-km',\n",
       "  'knowledge_model_id': '59b95b69-c4b2-4696-b4a7-b1a4cf1235b5'},\n",
       " {'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '75324784-1854-4a68-8713-82ea390a2a79',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'b2afef54-36f0-438c-9074-f92929c0b606'},\n",
       " {'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '7fc4fd1b-6407-4cf3-8b9d-50b21397ac1b',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'dba5e290-fa7e-463e-8d1f-267762c01a41'},\n",
       " {'data_model_id': '24562740-9c61-4566-93ac-21f23f95a157',\n",
       "  'data_model_name': 'Data Lineage Minotring',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '680a5c61-f067-4b56-8dcd-e94fc0854a7c',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '75b1bc36-70b5-4f57-91d7-bbdef6ed410c'},\n",
       " {'data_model_id': '80fea3cc-e1ba-4bbf-a29a-da1d2e50557f',\n",
       "  'data_model_name': 'Test Data Model',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '680a5c61-f067-4b56-8dcd-e94fc0854a7c',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'studio-lineage',\n",
       "  'knowledge_model_id': '75fa6a46-61c2-455b-b04b-c4e42762ce5a'},\n",
       " {'data_model_id': '0ad397d5-ca2a-4a57-a0ab-76dc042117a0',\n",
       "  'data_model_name': 'test:perspective_celonis_AccountsReceivable',\n",
       "  'data_pool_id': '6cd287bb-c8da-419d-9faa-2b42e8a08ec4',\n",
       "  'data_pool_name': 'OCPM Data Pool',\n",
       "  'root_id': '8fc8ff44-1947-4cf0-8ef8-d0e4d19782fe',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_test_perspective_celonis_accountsreceivable-km',\n",
       "  'knowledge_model_id': '7de213a8-ba42-4b92-8d16-ab35f98a8b38'},\n",
       " {'data_model_id': '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '575272bd-cfd6-4849-8bab-078506b77b00',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '4d63f4e9-1e92-454e-a128-f161f43f18d6'},\n",
       " {'data_model_id': '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': 'dd496349-6fe5-47d9-b3d6-2e17906a8513',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '6fb03d9f-d533-4eb9-bcd3-3560729240f5'},\n",
       " {'data_model_id': '088283c6-9918-4b59-9a4b-96d9cb38bb15',\n",
       "  'data_model_name': 'full lineage',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '12c91533-5e38-4933-adc5-32785200bd97',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_lineage-km',\n",
       "  'knowledge_model_id': '423975bc-05cb-45fe-b0e3-58cb7cd1c4ed'},\n",
       " {'data_model_id': '13483c35-4b6a-4e90-b06c-9230b25e7b02',\n",
       "  'data_model_name': 'dummy_peopleDM',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': 'c937d7d5-54fc-444e-8df8-5c269d7f9d19',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_dummy_peopledm-km',\n",
       "  'knowledge_model_id': '25d4909d-d74a-45ca-96af-0d3044dd6e00'},\n",
       " {'data_model_id': '5575dbb7-41c4-4828-9fcc-314c7a771410',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest_V2',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '16b4618f-6e5b-4c74-a241-43bcca83df8c',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_data_lineage_monitoring_latest_v2-km',\n",
       "  'knowledge_model_id': '11ede6d9-76bc-458e-b2e7-4f6a83f8f988'},\n",
       " {'data_model_id': 'bbcfb903-9134-433a-9c91-08fb75ad3d3f',\n",
       "  'data_model_name': 'full_lineage',\n",
       "  'data_pool_id': '21a3ed98-f1f1-452d-9735-e58b34bac57e',\n",
       "  'data_pool_name': 'lineage_tomas',\n",
       "  'root_id': '46e5eb37-062d-4f53-9add-3d24876f0a94',\n",
       "  'space_id': 'a871e45c-16c5-4e85-9e00-38f3853d5149',\n",
       "  'knowledge_model_key': 'dm_full_lineage-km',\n",
       "  'knowledge_model_id': '97ac8f46-c6f4-4b60-ba05-e7bf59863ddf'},\n",
       " {'data_model_id': '1c366528-fd3e-479a-b304-63aa1c0db354',\n",
       "  'data_model_name': 'tomas_datamodel',\n",
       "  'data_pool_id': '21a3ed98-f1f1-452d-9735-e58b34bac57e',\n",
       "  'data_pool_name': 'lineage_tomas',\n",
       "  'root_id': 'e5a87610-bb0a-4807-a490-f74f9b345d14',\n",
       "  'space_id': 'a871e45c-16c5-4e85-9e00-38f3853d5149',\n",
       "  'knowledge_model_key': 'dm_tomas_datamodel-km',\n",
       "  'knowledge_model_id': '88efd602-7ee7-4fb6-97b3-5e1bb9f113f3'},\n",
       " {'data_model_id': '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '5c904f3c-d04e-4a94-a8bf-70a2823e9a3d',\n",
       "  'space_id': 'a871e45c-16c5-4e85-9e00-38f3853d5149',\n",
       "  'knowledge_model_key': 'dm_data_lineage_monitoring_latest-km',\n",
       "  'knowledge_model_id': 'c23d579b-e085-4181-83cc-6a756e3cea7f'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2796b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 unique Data Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'088283c6-9918-4b59-9a4b-96d9cb38bb15',\n",
       " '0ad397d5-ca2a-4a57-a0ab-76dc042117a0',\n",
       " '13483c35-4b6a-4e90-b06c-9230b25e7b02',\n",
       " '1c366528-fd3e-479a-b304-63aa1c0db354',\n",
       " '1e846159-f3a9-42a3-9d91-b3c7c0c7f904',\n",
       " '24562740-9c61-4566-93ac-21f23f95a157',\n",
       " '5575dbb7-41c4-4828-9fcc-314c7a771410',\n",
       " '80fea3cc-e1ba-4bbf-a29a-da1d2e50557f',\n",
       " '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       " 'bbcfb903-9134-433a-9c91-08fb75ad3d3f',\n",
       " 'da1f57a6-3485-4eb3-944d-12825f56687e'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_data_model_ids = {item['data_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_data_model_ids)} unique Data Models to scan for.\")\n",
    "unique_data_model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04eea8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 unique Knowledge Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'11ede6d9-76bc-458e-b2e7-4f6a83f8f988',\n",
       " '25d4909d-d74a-45ca-96af-0d3044dd6e00',\n",
       " '423975bc-05cb-45fe-b0e3-58cb7cd1c4ed',\n",
       " '4d63f4e9-1e92-454e-a128-f161f43f18d6',\n",
       " '59b95b69-c4b2-4696-b4a7-b1a4cf1235b5',\n",
       " '6fb03d9f-d533-4eb9-bcd3-3560729240f5',\n",
       " '75b1bc36-70b5-4f57-91d7-bbdef6ed410c',\n",
       " '75fa6a46-61c2-455b-b04b-c4e42762ce5a',\n",
       " '7de213a8-ba42-4b92-8d16-ab35f98a8b38',\n",
       " '88efd602-7ee7-4fb6-97b3-5e1bb9f113f3',\n",
       " '97ac8f46-c6f4-4b60-ba05-e7bf59863ddf',\n",
       " 'b2afef54-36f0-438c-9074-f92929c0b606',\n",
       " 'c23d579b-e085-4181-83cc-6a756e3cea7f',\n",
       " 'dba5e290-fa7e-463e-8d1f-267762c01a41',\n",
       " 'dd35ad1d-5ba1-44dc-b9a0-533587643b35'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_KM_ids = {item['knowledge_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_KM_ids)} unique Knowledge Models to scan for.\")\n",
    "unique_KM_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bba2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_datamodel_metadata(all_metadata, dm_id_mapping):\n",
    "    \"\"\"\n",
    "    Optimized version:\n",
    "    1. Uses pre-fetched dm_id_mapping to avoid re-fetching Data Pools/Models.\n",
    "    2. Uses Threading to fetch columns for multiple tables in parallel.\n",
    "    \"\"\"\n",
    "    print(\"Starting Optimized Data Model Scan...\")\n",
    "    \n",
    "    # 1. Identify unique Data Model IDs we need to process\n",
    "    unique_dm_ids = set(m['data_model_id'] for m in all_metadata if m.get('data_model_id'))\n",
    "    print(f\"Found {len(unique_dm_ids)} unique Data Models to scan.\")\n",
    "\n",
    "    final_data = []\n",
    "\n",
    "    # Helper function for threading\n",
    "    def process_table_columns(table, dm_obj, pool_obj):\n",
    "        \"\"\"Fetches columns for a single table. Runs in a thread.\"\"\"\n",
    "        rows = []\n",
    "        try:\n",
    "            # This is the slow API call we are parallelizing\n",
    "            columns = table.get_columns() \n",
    "            \n",
    "            for column in columns:\n",
    "                unique_id = f\"{dm_obj.id}.{table.name}.{column.name}\".lower()\n",
    "                rows.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"d_pool_id\": pool_obj.id,\n",
    "                    \"d_pool_name\": pool_obj.name,\n",
    "                    \"d_model_id\": dm_obj.id,\n",
    "                    \"d_model_name\": dm_obj.name,\n",
    "                    \"table_name\": table.name,\n",
    "                    \"column_name\": column.name\n",
    "                })\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: Error fetching columns for table {table.name}: {e}\")\n",
    "            pass\n",
    "        return rows\n",
    "\n",
    "    # 2. Main Loop\n",
    "    for dm_id in unique_dm_ids:\n",
    "        # Fast Lookup from previous step\n",
    "        cached_entry = dm_id_mapping.get(dm_id)\n",
    "        \n",
    "        if not cached_entry:\n",
    "            print(f\"  Skipping DM {dm_id} (Not found in cache - permission issue?)\")\n",
    "            continue\n",
    "            \n",
    "        data_model = cached_entry['model']\n",
    "        data_pool = cached_entry['pool']\n",
    "        \n",
    "        print(f\"  Scanning Data Model: {data_model.name}...\")\n",
    "\n",
    "        try:\n",
    "            # Get all tables (usually one API call)\n",
    "            tables = data_model.get_tables()\n",
    "            \n",
    "            # 3. Parallel Execution for Columns\n",
    "            # We fetch columns for up to 10 tables at once\n",
    "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = []\n",
    "                for table in tables:\n",
    "                    futures.append(executor.submit(process_table_columns, table, data_model, data_pool))\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    result_rows = future.result()\n",
    "                    final_data.extend(result_rows)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"    Error accessing tables for DM {data_model.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    print(f\"\\nData Model scan complete. Found {len(final_data)} columns.\")\n",
    "    return pd.DataFrame(final_data).drop_duplicates(subset=['unique_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optimized Data Model Scan...\n",
      "Found 11 unique Data Models to scan.\n",
      "  Scanning Data Model: data lineage - test...\n",
      "  Scanning Data Model: Data Lineage Monitoring - Latest_V2...\n",
      "  Scanning Data Model: full lineage...\n",
      "  Scanning Data Model: full_lineage...\n",
      "  Scanning Data Model: Data Lineage Minotring...\n",
      "  Scanning Data Model: tomas_datamodel...\n",
      "  Scanning Data Model: Data Lineage Monitoring...\n",
      "  Scanning Data Model: test:perspective_celonis_AccountsReceivable...\n",
      "  Scanning Data Model: Data Lineage Monitoring - Latest...\n",
      "  Scanning Data Model: dummy_peopleDM...\n",
      "  Scanning Data Model: Test Data Model...\n",
      "\n",
      "Data Model scan complete. Found 1933 columns.\n",
      "Saved data_models.csv\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "# Pass the 'dm_id_mapping' we created in the beginning\n",
    "df_data_models = get_datamodel_metadata(all_metadata, dm_id_mapping)\n",
    "\n",
    "# Save\n",
    "df_data_models.to_csv('data_models.csv', index=False)\n",
    "print(\"Saved data_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7ee5f",
   "metadata": {},
   "source": [
    "## Studio Lineage - Action Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc31c25",
   "metadata": {},
   "source": [
    "### Extract Blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d846cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blueprint_api(\n",
    "    source, # This should be a pycelonis connection object\n",
    "    package, # This should be a pycelonis package object\n",
    "    node_data: dict,\n",
    "    source_space: object, # This is your custom config object\n",
    "    published: bool,\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"Exports blueprint and stores it in blueprint folder with User key\n",
    "\n",
    "    Args:\n",
    "        package: package where blueprint is\n",
    "        node_data: dict containing serializedContent\n",
    "        source_space: config object with blueprint paths\n",
    "        published: whether to get published or draft version\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_blueprint_api(\n",
    "        source,\n",
    "        package,\n",
    "        parsed_node_data: dict,  # Already parsed JSON\n",
    "        published: bool,\n",
    "    ) -> dict:\n",
    "        \"\"\"Function extracts the blueprint given the Action Flow data\n",
    "\n",
    "        Args:\n",
    "            source: Celonis object for source Team\n",
    "            package: Celonis object for package where the Action Flow is stored\n",
    "            parsed_node_data: Already parsed node data from serializedContent\n",
    "\n",
    "        Returns:\n",
    "            a json with the blueprint of the Action Flow\n",
    "        \"\"\"\n",
    "\n",
    "        def get_blueprint_versions(source, package, parsed_node_data):\n",
    "            base_url = source.client.base_url\n",
    "            blueprint_url = f\"{base_url}/ems-automation/api/root/{parsed_node_data['rootNodeId']}/asset/{parsed_node_data['key']}/proxy/api/v2/scenarios/{parsed_node_data['scenarioId']}/blueprints\"\n",
    "            blueprints_version = source.client.request(\n",
    "                url=blueprint_url, method=\"get\", parse_json=True\n",
    "            )\n",
    "            return blueprints_version\n",
    "\n",
    "        def convert_to_timestamp(timestamp_string):\n",
    "            timestamp = datetime.strptime(timestamp_string, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            return int(timestamp.timestamp())\n",
    "\n",
    "        base_url = source.client.base_url\n",
    "\n",
    "        # Get the blueprint's versions\n",
    "        blueprints = get_blueprint_versions(source, package, parsed_node_data)\n",
    "        blueprints[\"scenariosBlueprints\"] = sorted(\n",
    "            blueprints[\"scenariosBlueprints\"],\n",
    "            key=lambda x: x[\"created\"],\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        blueprint_url = f\"{base_url}/ems-automation/api/root/{parsed_node_data['rootNodeId']}/asset/{parsed_node_data['key']}/proxy/api/v2/scenarios/{parsed_node_data['scenarioId']}/blueprint\"\n",
    "        \n",
    "        if published:\n",
    "            # try getting the published version\n",
    "            blueprint_info = [\n",
    "                b for b in blueprints[\"scenariosBlueprints\"] if not b[\"draft\"]\n",
    "            ]\n",
    "            # if not possible -> get the draft version\n",
    "            if not blueprint_info:\n",
    "                blueprint_info = [\n",
    "                    b for b in blueprints[\"scenariosBlueprints\"] if b[\"draft\"]\n",
    "                ]\n",
    "        else:\n",
    "            # try getting the draft version\n",
    "            blueprint_info = [\n",
    "                b for b in blueprints[\"scenariosBlueprints\"] if b[\"draft\"]\n",
    "            ]\n",
    "            # if not possible -> get the published version\n",
    "            if not blueprint_info:\n",
    "                blueprint_info = [\n",
    "                    b for b in blueprints[\"scenariosBlueprints\"] if not b[\"draft\"]\n",
    "                ]\n",
    "\n",
    "        if blueprint_info:\n",
    "            id_timestamp = convert_to_timestamp(blueprint_info[0].get(\"created\"))\n",
    "            query_url = (\n",
    "                f\"blueprintId={blueprint_info[0].get('version')}&_={id_timestamp}\"\n",
    "            )\n",
    "\n",
    "            blueprint = source.client.request(\n",
    "                url=f\"{blueprint_url}?{query_url}\", method=\"get\", parse_json=True\n",
    "            )\n",
    "\n",
    "            if \"response\" in blueprint:\n",
    "                return blueprint[\"response\"][\"blueprint\"]\n",
    "\n",
    "    # Parse the serialized content ONCE at the beginning\n",
    "    parsed_node_data = json.loads(node_data[\"serializedContent\"])\n",
    "    \n",
    "    # Extract the key from parsed data\n",
    "    AF_key = parsed_node_data[\"key\"]\n",
    "    \n",
    "    # Get the blueprint using the parsed data\n",
    "    blueprint = extract_blueprint_api(source, package, parsed_node_data, published)\n",
    "    \n",
    "    # All blueprints (both draft and published) go directly into \"blueprints\" folder\n",
    "    bp_path = \"blueprints\"\n",
    "        \n",
    "    if not blueprint:\n",
    "        # Create a blank blueprint\n",
    "        blueprint = json.loads(\n",
    "            \"\"\"\n",
    "            {\n",
    "                \"flow\": [\n",
    "                    {\n",
    "                        \"id\": null,\n",
    "                        \"module\": \"placeholder:Placeholder\",\n",
    "                        \"metadata\": {\n",
    "                            \"designer\": {\n",
    "                                \"x\": 0,\n",
    "                                \"y\": 0\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"metadata\": {\n",
    "                    \"instant\": false,\n",
    "                    \"version\": 1,\n",
    "                    \"scenario\": {\n",
    "                        \"roundtrips\": 1,\n",
    "                        \"maxErrors\": 3,\n",
    "                        \"autoCommit\": true,\n",
    "                        \"autoCommitTriggerLast\": true,\n",
    "                        \"sequential\": false,\n",
    "                        \"confidential\": false,\n",
    "                        \"dataloss\": false,\n",
    "                        \"dlq\": false\n",
    "                    },\n",
    "                    \"designer\": {\n",
    "                        \"orphans\": []\n",
    "                    },\n",
    "                    \"zone\": \"integromat.try.k8s.celonis.cloud\"\n",
    "                }\n",
    "            }\"\"\"\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        file_name = f\"{AF_key}&{package.key}.json\"\n",
    "        print(f\"Blueprint extracted. Saving to: {file_name}\")\n",
    "\n",
    "    with open(bp_path + f\"/{AF_key}&{package.key}.json\", \"w\") as out:\n",
    "        json.dump(blueprint, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb636271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_blueprints():\n",
    "    \"\"\"\n",
    "    Download all Action Flow blueprints from all spaces and packages.\n",
    "    First deletes the existing blueprints directory and then creates a fresh one.\n",
    "    \"\"\"\n",
    "    print(\"-\"*80)\n",
    "    print(\"DOWNLOADING ACTION FLOW BLUEPRINTS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Delete existing blueprints directory if it exists\n",
    "    if os.path.exists(\"blueprints\"):\n",
    "        print(\"Removing existing blueprints directory...\")\n",
    "        shutil.rmtree(\"blueprints\")\n",
    "        print(\"Existing blueprints directory removed.\")\n",
    "    \n",
    "    # Create fresh blueprints directory\n",
    "    print(\"Creating new blueprints directory...\")\n",
    "    os.makedirs(\"blueprints\")\n",
    "    \n",
    "    total_blueprints_downloaded = 0\n",
    "    \n",
    "    # Iterate through all spaces and packages\n",
    "    for space in c.studio.get_spaces():\n",
    "        print(f\"\\nSpace: {space.name}\")\n",
    "        \n",
    "        for package in space.get_packages():\n",
    "            print(f\"  Package: {package.name}\")\n",
    "            \n",
    "            try:\n",
    "                # Get all nodes in the package\n",
    "                nodes = package.get_content_nodes()\n",
    "                \n",
    "                # Filter for SCENARIO nodes (Action Flows)\n",
    "                scenario_nodes = [node for node in nodes if hasattr(node, 'asset_type') and node.asset_type == 'SCENARIO']\n",
    "                \n",
    "                if not scenario_nodes:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"    Found {len(scenario_nodes)} Action Flow(s)\")\n",
    "                \n",
    "                # Process each Action Flow\n",
    "                for node in scenario_nodes:\n",
    "                    try:\n",
    "                        print(f\"      Processing: {node.name}\")\n",
    "                        \n",
    "                        # Prepare node_data dict\n",
    "                        node_data = {\n",
    "                            'serializedContent': node.serialized_content\n",
    "                        }\n",
    "                        \n",
    "                        # Download blueprint\n",
    "                        download_blueprint_api(\n",
    "                            source=c,\n",
    "                            package=package,\n",
    "                            node_data=node_data,\n",
    "                            source_space=None,\n",
    "                            published=False\n",
    "                        )\n",
    "                        \n",
    "                        total_blueprints_downloaded += 1\n",
    "                        print(f\"        Blueprint downloaded\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"        Error: {str(e)[:100]}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error accessing package: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"SUMMARY: Downloaded {total_blueprints_downloaded} blueprints\")\n",
    "    print(f\"{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DOWNLOADING ACTION FLOW BLUEPRINTS\n",
      "--------------------------------------------------------------------------------\n",
      "Removing existing blueprints directory...\n",
      "Existing blueprints directory removed.\n",
      "Creating new blueprints directory...\n",
      "\n",
      "Space: ahilmer - lineage test\n",
      "  Package: Monitoring\n",
      "\n",
      "Space: Flo\n",
      "  Package: Monitoring\n",
      "\n",
      "Space: FS\n",
      "  Package: Data Lineage\n",
      "\n",
      "Space: Paula's Space\n",
      "  Package: Data Lineage - Test\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: action_flow_test\n",
      "Blueprint extracted. Saving to: 08ae264c_d22b_4da9_b80f_c25fc9eb004a.action-flow-test&08ae264c_d22b_4da9_b80f_c25fc9eb004a.json\n",
      "        Blueprint downloaded\n",
      "  Package: Data Lineage - Monitoring Pool version\n",
      "  Package: Monitoring - Data Lineage\n",
      "  Package: Monitoring - Data Lineage Sarvesh Test\n",
      "  Package: Demo\n",
      "  Package: Monitoring - Data Lineage Copy\n",
      "  Package: Data Lineage - New Version\n",
      "  Package: Celonis Data Lineage Latest\n",
      "  Package: full lineage\n",
      "  Package: dummy_package_tomas\n",
      "    Found 2 Action Flow(s)\n",
      "      Processing: getKPI_dummy\n",
      "Blueprint extracted. Saving to: fa06f3b7_1ced_4311_b69f_84bdcbc611d4.getkpi-dummy&fa06f3b7_1ced_4311_b69f_84bdcbc611d4.json\n",
      "        Blueprint downloaded\n",
      "      Processing: write data in data pool\n",
      "Blueprint extracted. Saving to: fa06f3b7_1ced_4311_b69f_84bdcbc611d4.write-data-in-data-pool&fa06f3b7_1ced_4311_b69f_84bdcbc611d4.json\n",
      "        Blueprint downloaded\n",
      "  Package: test_lineageV2\n",
      "\n",
      "Space: TomÃ¡s\n",
      "  Package: E2E Lineage\n",
      "  Package: testtable\n",
      "  Package: backend lineage\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SUMMARY: Downloaded 3 blueprints\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "download_all_blueprints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440a3d9",
   "metadata": {},
   "source": [
    "### Extract celonis modules from blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03781e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celonis_module_positions(blueprint: str) -> list:\n",
    "    \"\"\"\n",
    "    Recursively searches through entire JSON structure to find all modules \n",
    "    that start with 'celonis:', regardless of nesting depth.\n",
    "    \n",
    "    Args:\n",
    "        blueprint: Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing module data for each Celonis module\n",
    "    \"\"\"\n",
    "    def find_celonis_modules(obj, path=\"\"):\n",
    "        \"\"\"Recursively search for celonis modules in nested structures\"\"\"\n",
    "        modules = []\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            # Check if this dict has a 'module' key starting with 'celonis:'\n",
    "            if 'module' in obj and isinstance(obj['module'], str) and obj['module'].startswith('celonis:'):\n",
    "                # Found a Celonis module - store the entire object\n",
    "                modules.append({\n",
    "                    'data': obj,\n",
    "                    'path': path,\n",
    "                    'module': obj['module'],\n",
    "                    'id': obj.get('id')\n",
    "                })\n",
    "            \n",
    "            # Recursively search all values in this dict\n",
    "            for key, value in obj.items():\n",
    "                new_path = f\"{path}.{key}\" if path else key\n",
    "                modules.extend(find_celonis_modules(value, new_path))\n",
    "        \n",
    "        elif isinstance(obj, list):\n",
    "            # Recursively search all items in this list\n",
    "            for i, item in enumerate(obj):\n",
    "                new_path = f\"{path}[{i}]\"\n",
    "                modules.extend(find_celonis_modules(item, new_path))\n",
    "        \n",
    "        return modules\n",
    "    \n",
    "    try:\n",
    "        with open(blueprint, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Start recursive search from root\n",
    "        celonis_modules = find_celonis_modules(data)\n",
    "        \n",
    "        return celonis_modules\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9abd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_celonis_module_details(blueprint: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts details from Celonis modules in an Action Flow blueprint.\n",
    "    Recursively searches through entire JSON structure.\n",
    "    \n",
    "    Args:\n",
    "        blueprint: Path to the blueprint JSON file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing extracted details for each Celonis module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get all Celonis modules (regardless of nesting)\n",
    "        celonis_modules = get_celonis_module_positions(blueprint)\n",
    "        \n",
    "        module_details = []\n",
    "        \n",
    "        for module_info in celonis_modules:\n",
    "            module_data = module_info['data']\n",
    "            module_name = module_info['module']\n",
    "            mapper = module_data.get('mapper', {})\n",
    "            metadata = module_data.get('metadata', {})  \n",
    "            \n",
    "            detail = {\n",
    "                'path': module_info['path'],\n",
    "                'module': module_name,\n",
    "                'id': module_info['id'],\n",
    "                'mapper': mapper,      \n",
    "                'metadata': metadata   \n",
    "            }\n",
    "            \n",
    "            # Rule 1: celonis:getKPIs\n",
    "            if module_name == 'celonis:getKPIs':\n",
    "                detail['kpis'] = mapper.get('kpis', [])\n",
    "                detail['knowledgeModelKey'] = mapper.get('knowledgeModelKey', '')\n",
    "            \n",
    "            # Rule 2: celonis:getRows\n",
    "            elif module_name == 'celonis:getRows':\n",
    "                km_columns = mapper.get('kmColumns', [])\n",
    "                column_names = [col.get('columnName', '') for col in km_columns]\n",
    "                detail['column_names'] = column_names\n",
    "                detail['dataOrKnowledgeModel'] = mapper.get('dataOrKnowledgeModel', '')\n",
    "                detail['knowledgeModelKey'] = mapper.get('knowledgeModelKey', '')  \n",
    "            \n",
    "            # Rule 3: celonis:updateAugmentedAttribute\n",
    "            elif module_name in ['celonis:updateAugmentedAttribute', 'celonis:updateAugmentedAttributeV2']:\n",
    "                detail['record'] = mapper.get('record', '')\n",
    "                detail['augmentedAttributeId'] = mapper.get('augmentedAttributeId', '')                \n",
    "                detail['knowledgeModelKey'] = mapper.get('knowledgeModelKey', '')\n",
    "            \n",
    "            \n",
    "            # Rule 4: celonis:queryData\n",
    "            elif module_name == 'celonis:queryData':\n",
    "                columns = mapper.get('columns', [])\n",
    "                column_details = []\n",
    "                for col in columns:\n",
    "                    column_name = col.get('columnName', '')\n",
    "                    column_formula = col.get('formula', '')\n",
    "                    column_details.append({\n",
    "                        'columnName': column_name,\n",
    "                        'formula': column_formula\n",
    "                    })\n",
    "                \n",
    "                detail['columns'] = column_details\n",
    "                detail['dataPool'] = mapper.get('dataPool', '')\n",
    "                detail['dataModel'] = mapper.get('dataModel', '')\n",
    "                \n",
    "                # Extract only filterExpression from each filter\n",
    "                filters = mapper.get('filter', [])\n",
    "                filter_expressions = [f.get('filterExpression', '') for f in filters]\n",
    "                detail['filterExpression'] = filter_expressions\n",
    "            \n",
    "            module_details.append(detail)\n",
    "        \n",
    "        return module_details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting module details: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf32971",
   "metadata": {},
   "source": [
    "### Extract pql (e.g. if you have a celonis:extractQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b830aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_and_columns_from_pql(pql: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts table and column references from a PQL query.\n",
    "    \n",
    "    Args:\n",
    "        pql: PQL query string\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'table_column_pairs': List of tuples (table, column)\n",
    "        - 'tables_only': List of table names without column references\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pattern for \"TABLE\".\"COLUMN\"\n",
    "    table_column_pattern = r'\"([^\"]+)\"\\s*\\.\\s*\"([^\"]+)\"'\n",
    "    \n",
    "    # Find all table.column pairs\n",
    "    table_column_matches = re.findall(table_column_pattern, pql)\n",
    "    \n",
    "    # Pattern for standalone \"TABLE\" \n",
    "    table_only_pattern = r'(?<!\\.\\s)(?<!\\.)\"([A-Z_][A-Z0-9_]*)\"(?!\\s*\\.\\s*\")'\n",
    "    \n",
    "    # Find all standalone tables\n",
    "    table_only_matches = re.findall(table_only_pattern, pql)\n",
    "    \n",
    "    # Remove tables that are already in table.column pairs\n",
    "    tables_in_pairs = {table for table, _ in table_column_matches}\n",
    "    # Also remove columns that appear in table.column pairs\n",
    "    columns_in_pairs = {column for _, column in table_column_matches}\n",
    "    \n",
    "    tables_only = [table for table in table_only_matches \n",
    "                   if table not in tables_in_pairs and table not in columns_in_pairs]\n",
    "    \n",
    "    return {\n",
    "        'table_column_pairs': list(set(table_column_matches)),  # Remove duplicates\n",
    "        'tables_only': list(set(tables_only))  # Remove duplicates\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1299c",
   "metadata": {},
   "source": [
    "### Execution Block for the Action Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ea6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Knowledge Models in lookup.\n",
      "Processing 3 blueprint files...\n",
      "\n",
      "Extracted 16 lineage records from Action Flows\n",
      "Saved to action_flow_lineage.csv (16 records)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. PRE-FETCH: Build lookup dictionary\n",
    "# =============================================================================\n",
    "km_lookup = {item['knowledge_model_key']: item for item in all_metadata}\n",
    "print(f\"Found {len(km_lookup)} Knowledge Models in lookup.\")\n",
    "\n",
    "# List to store all lineage relationships\n",
    "af_lineage = []\n",
    "\n",
    "# Get all blueprint JSON files\n",
    "blueprint_files = glob.glob('blueprints/*.json')\n",
    "print(f\"Processing {len(blueprint_files)} blueprint files...\")\n",
    "\n",
    "for blueprint_path in blueprint_files:\n",
    "    try:\n",
    "        # --- 1. FILENAME PARSING ---\n",
    "        filename = os.path.basename(blueprint_path)\n",
    "        clean_filename = filename.replace('.json', '')\n",
    "        \n",
    "        # Regex to find a UUID (Action Flow ID)\n",
    "        uuid_match = re.search(r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}', clean_filename)\n",
    "        af_id = uuid_match.group(0) if uuid_match else clean_filename\n",
    "\n",
    "        # --- 2. GET NAME FROM JSON ---\n",
    "        try:\n",
    "            with open(blueprint_path, 'r') as f:\n",
    "                blueprint_data = json.load(f)\n",
    "                af_name = blueprint_data.get('name', 'Unknown_Action_Flow')\n",
    "        except Exception:\n",
    "            af_name = 'Unknown_Action_Flow'\n",
    "            blueprint_data = {}\n",
    "\n",
    "        # Call your existing function\n",
    "        module_details = extract_celonis_module_details(blueprint_path)\n",
    "        \n",
    "        if not module_details:\n",
    "            continue\n",
    "        \n",
    "        relationships = []\n",
    "        \n",
    "        for detail in module_details:\n",
    "            module_name = detail['module']\n",
    "            module_number = detail.get('id', 'unknown') \n",
    "            \n",
    "            # --- CLEAN MODULE TYPE NAME ---\n",
    "            # Converts \"celonis:getRows\" -> \"getRows\"\n",
    "            clean_type = module_name.split(':')[-1]\n",
    "            \n",
    "            # Rule 1: celonis:getKPIs\n",
    "            if module_name == 'celonis:getKPIs':\n",
    "                km_key = detail.get('knowledgeModelKey', '')\n",
    "                if km_key.startswith('KNOWLEDGE_MODEL-'):\n",
    "                    km_key = km_key.replace('KNOWLEDGE_MODEL-', '', 1)\n",
    "                    \n",
    "                for kpi in detail.get('kpis', []):\n",
    "                    relationships.append({\n",
    "                        'source_id': f'{kpi}'.lower(),\n",
    "                        'source_name': kpi,\n",
    "                        'source_attr': None,\n",
    "                        'source_type': 'KPI',\n",
    "                        'source_node_type': '',\n",
    "                        'source_asset_id': km_key,\n",
    "                        'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                        'target_module_id': module_number,\n",
    "                        'target_module_type': clean_type, # Pass clean type\n",
    "                        'pql': '',\n",
    "                        'km_key': km_key,\n",
    "                        'dm_id': '',\n",
    "                        'dp_id': ''\n",
    "                    })\n",
    "            \n",
    "            # Rule 2: celonis:getRows\n",
    "            elif module_name == 'celonis:getRows':\n",
    "                km_key = detail.get('knowledgeModelKey', '') or detail.get('dataOrKnowledgeModel', '')\n",
    "                if km_key.startswith('KNOWLEDGE_MODEL-'):\n",
    "                    km_key = km_key.replace('KNOWLEDGE_MODEL-', '', 1)\n",
    "\n",
    "                for column in detail.get('column_names', []):\n",
    "                    if '.' in column:\n",
    "                        parts = column.split('.', 1)\n",
    "                        source_name = parts[0]\n",
    "                        source_attr = parts[1] if len(parts) > 1 else None\n",
    "                    else:\n",
    "                        source_name = column\n",
    "                        source_attr = None\n",
    "                    \n",
    "                    relationships.append({\n",
    "                        'source_id': f'{column}'.lower(),\n",
    "                        'source_name': source_name.lower(),\n",
    "                        'source_attr': source_attr.lower() if source_attr else None,\n",
    "                        'source_type': 'KM_COLUMN',\n",
    "                        'source_node_type': 'ATTRIBUTES',\n",
    "                        'source_asset_id': km_key,\n",
    "                        'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                        'target_module_id': module_number,\n",
    "                        'target_module_type': clean_type,\n",
    "                        'pql': '',\n",
    "                        'km_key': km_key,\n",
    "                        'dm_id': '',\n",
    "                        'dp_id': ''\n",
    "                    })\n",
    "            \n",
    "            # Rule 3: celonis:updateAugmentedAttribute (V1 & V2)\n",
    "            elif 'updateAugmentedAttribute' in module_name:\n",
    "                mapper = detail.get('mapper', {})\n",
    "                metadata = detail.get('metadata', {})\n",
    "                \n",
    "                km_key = mapper.get('knowledgeModelKey', '')\n",
    "                aug_attr_id_full = mapper.get('augmentedAttributeId', '')  \n",
    "                \n",
    "                # --- EXTRACT LABEL FROM METADATA ---\n",
    "                aug_attr_label = ''\n",
    "                try:\n",
    "                    restore_data = metadata.get('restore', {})\n",
    "                    aug_attr_metadata = restore_data.get('expect', {}).get('augmentedAttributeId', {})\n",
    "                    aug_attr_label = aug_attr_metadata.get('label', '')  # \"Aug Atts Test\"\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # --- PARSE THE AUGMENTED ATTRIBUTE ID ---\n",
    "                record = ''\n",
    "                attr_name = ''\n",
    "                \n",
    "                if aug_attr_id_full and '.' in aug_attr_id_full:\n",
    "                    parts = aug_attr_id_full.split('.')\n",
    "                    if len(parts) >= 3:\n",
    "                        record = parts[1].lower()  # \"vbak\"\n",
    "                \n",
    "                # --- BUILD THE ATTRIBUTE NAME ---\n",
    "                if aug_attr_label:\n",
    "                    # Convert \"Aug Atts Test\" -> \"aug_atts_test\"\n",
    "                    attr_name_base = aug_attr_label.lower().replace(' ', '_')\n",
    "                    # Add \"aug_\" prefix -> \"aug_aug_atts_test\"\n",
    "                    attr_name = f\"aug_{attr_name_base}\"\n",
    "                \n",
    "                # --- BUILD THE FINAL SOURCE ID ---\n",
    "                # Format: \"record_attr_name.value\" (with .value suffix!)\n",
    "                # Example: \"vbak_aug_aug_atts_test.value\"\n",
    "                if record and attr_name:\n",
    "                    source_id = f\"{record}_{attr_name}.value\"  \n",
    "                    full_aug_attr = f\"{record}_{attr_name}\"    # Keep without .value for UNIQUE_SOURCE_ID\n",
    "                else:\n",
    "                    # Fallback to the full ID if parsing fails\n",
    "                    source_id = aug_attr_id_full.lower()\n",
    "                    full_aug_attr = aug_attr_id_full.lower()\n",
    "\n",
    "                if km_key.startswith('KNOWLEDGE_MODEL-'):\n",
    "                    km_key = km_key.replace('KNOWLEDGE_MODEL-', '', 1)\n",
    "                if '.' in km_key:\n",
    "                    km_key = km_key.split('.')[-1]\n",
    "\n",
    "                # 1. READ Flow: KM Attribute -> Action Flow\n",
    "                relationships.append({\n",
    "                    'source_id': source_id,           \n",
    "                    'source_name': f\"{record}_{attr_name}\",           \n",
    "                    'source_attr': \"value\",  \n",
    "                    'source_type': 'AUGMENTED_ATTRIBUTE',\n",
    "                    'source_node_type': 'ATTRIBUTES',\n",
    "                    'source_asset_id': km_key,\n",
    "                    'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                    'target_module_id': module_number,\n",
    "                    'target_module_type': clean_type,\n",
    "                    'pql': '',\n",
    "                    'km_key': km_key,\n",
    "                    'dm_id': '',\n",
    "                    'dp_id': '',\n",
    "                    'reverse': False\n",
    "                })\n",
    "\n",
    "                # 2. WRITE Flow: Action Flow -> KM Attribute\n",
    "                # FIX: Keep 'target_module_id' as the Module Number so ID generation works\n",
    "                relationships.append({\n",
    "                    'source_id': source_id,                # Same Attribute ID\n",
    "                    'source_name': f\"{record}_{attr_name}\",# Same Attribute Name\n",
    "                    'source_attr': \"value\",                # Same Attribute Value\n",
    "                    'source_type': 'AUGMENTED_ATTRIBUTE',\n",
    "                    'source_node_type': 'ATTRIBUTES',\n",
    "                    'source_asset_id': km_key,\n",
    "                    'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                    'target_module_id': module_number,     # <--- KEEP THIS AS AF MODULE ID\n",
    "                    'target_module_type': clean_type,\n",
    "                    'pql': '',\n",
    "                    'km_key': km_key,\n",
    "                    'dm_id': '',\n",
    "                    'dp_id': '',\n",
    "                    'reverse': True\n",
    "                })\n",
    "            \n",
    "            \n",
    "            # Rule 4: celonis:queryData\n",
    "            elif module_name == 'celonis:queryData':\n",
    "                dp_id = detail.get('dataPool', '')\n",
    "                dm_id = detail.get('dataModel', '')\n",
    "                \n",
    "                def add_dm_rel(src_id, src_name, src_attr, node_type, pql_str):\n",
    "                    relationships.append({\n",
    "                        'source_id': src_id.lower(),\n",
    "                        'source_name': src_name.lower(),\n",
    "                        'source_attr': src_attr.lower() if src_attr else None,\n",
    "                        'source_type': 'DATA_MODEL',\n",
    "                        'source_node_type': node_type,\n",
    "                        'source_asset_id': dm_id,\n",
    "                        'source_asset_type': 'DATA_MODEL',\n",
    "                        'target_module_id': module_number,\n",
    "                        'target_module_type': clean_type,\n",
    "                        'pql': pql_str,\n",
    "                        'km_key': '',\n",
    "                        'dm_id': dm_id,\n",
    "                        'dp_id': dp_id\n",
    "                    })\n",
    "\n",
    "                for col in detail.get('columns', []):\n",
    "                    formula = col.get('formula', '')\n",
    "                    if formula:\n",
    "                        pql_refs = extract_tables_and_columns_from_pql(formula)\n",
    "                        for table, column in pql_refs['table_column_pairs']:\n",
    "                            add_dm_rel(f'{table}.{column}', table, column, 'TABLE_COLUMN', formula)\n",
    "                        for table in pql_refs['tables_only']:\n",
    "                            add_dm_rel(f'{table}', table, None, 'TABLE', formula)\n",
    "                \n",
    "                for filter_expr in detail.get('filterExpression', []):\n",
    "                    if filter_expr:\n",
    "                        pql_refs = extract_tables_and_columns_from_pql(filter_expr)\n",
    "                        for table, column in pql_refs['table_column_pairs']:\n",
    "                            add_dm_rel(f'{table}.{column}', table, column, 'TABLE_COLUMN', filter_expr)\n",
    "                        for table in pql_refs['tables_only']:\n",
    "                            add_dm_rel(f'{table}', table, None, 'TABLE', filter_expr)\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 3. CONVERT TO FINAL FORMAT\n",
    "        # -------------------------------------------------------------\n",
    "        for rel in relationships:\n",
    "            is_reversed = rel.get('reverse', False)\n",
    "            km_key = rel.get('km_key', '')\n",
    "            \n",
    "            meta = km_lookup.get(km_key, {})\n",
    "            km_id = meta.get('knowledge_model_id', '') if km_key else ''\n",
    "            dm_id = rel['dm_id'] if rel['dm_id'] else meta.get('data_model_id', '')\n",
    "            dp_id = rel['dp_id'] if rel['dp_id'] else meta.get('data_pool_id', '')\n",
    "            \n",
    "            module_id = rel['target_module_id']\n",
    "            module_type = rel['target_module_type']\n",
    "\n",
    "            # --- ID CONSTRUCTION (Updated with Source Info) ---\n",
    "            # Format: ActionFlowID.ActionFlowName.ModuleID.SourceName\n",
    "            source_part = rel['source_name'].replace(' ', '_') if rel['source_name'] else 'unknown'\n",
    "            af_unique_node_id = f\"{af_id}.{af_name}.{module_id}\".lower()\n",
    "            \n",
    "            # --- TARGET NAME CONSTRUCTION ---\n",
    "            # Format: \"Module 1 (getRows)\"\n",
    "            target_node_name = f\"Module {module_id} ({module_type})\"\n",
    "\n",
    "            if is_reversed:\n",
    "                # AF -> KM (Augmented Attribute Update)\n",
    "                # SOURCE is now the Action Flow (calculated correctly above)\n",
    "                unique_source_id = af_unique_node_id\n",
    "                \n",
    "                # TARGET is now the KM Attribute (using the stored source info)\n",
    "                unique_target_id = f'{km_id}.{rel[\"source_id\"]}'.lower() if km_id else f'{km_key}.{rel[\"source_id\"]}'.lower()\n",
    "                \n",
    "                af_lineage.append({\n",
    "                    'UNIQUE_SOURCE_ID': unique_source_id,\n",
    "                    'UNIQUE_TARGET_ID': unique_target_id,\n",
    "                    'SOURCE_ID': af_id,\n",
    "                    'SOURCE_NAME': f\"{af_name}\",\n",
    "                    'SOURCE_ATTRIBUTE': target_node_name,\n",
    "                    'SOURCE_NODE_TYPE': 'ACTION_FLOW',\n",
    "                    'SOURCE_TYPE': 'ACTION_FLOW',\n",
    "                    'SOURCE_PQL': rel['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': af_id,\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': 'ACTION_FLOW',\n",
    "                    \n",
    "                    # FIX: Map TARGET columns to the original Source details\n",
    "                    'TARGET_ID': rel['source_id'],      # Attribute ID\n",
    "                    'TARGET_NAME': rel['source_name'],  # Attribute Name\n",
    "                    'TARGET_ATTRIBUTE': rel['source_attr'], # \"value\"\n",
    "                    \n",
    "                    'TARGET_NODE_TYPE': 'AUGMENTED_ATTRIBUTE',\n",
    "                    'TARGET_STUDIO_ASSET_ID': km_id or km_key,\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': km_key,\n",
    "                    'KNOWLEDGE_MODEL_ID': km_id,\n",
    "                    'DATA_MODEL_ID': dm_id,\n",
    "                    'DATA_POOL_ID': dp_id\n",
    "                })\n",
    "            else:\n",
    "                # KM -> AF\n",
    "                if km_id:\n",
    "                    unique_source_prefix = km_id\n",
    "                elif dm_id:\n",
    "                    unique_source_prefix = dm_id\n",
    "                elif km_key:\n",
    "                    unique_source_prefix = km_key\n",
    "                else:\n",
    "                    unique_source_prefix = rel['source_asset_id']\n",
    "                \n",
    "                unique_source_id = f'{unique_source_prefix}.{rel[\"source_id\"]}'.lower() if unique_source_prefix else rel['source_id'].lower()\n",
    "                unique_target_id = af_unique_node_id\n",
    "                \n",
    "                af_lineage.append({\n",
    "                    'UNIQUE_SOURCE_ID': unique_source_id,\n",
    "                    'UNIQUE_TARGET_ID': unique_target_id,\n",
    "                    'SOURCE_ID': rel['source_id'],\n",
    "                    'SOURCE_NAME': rel['source_name'],\n",
    "                    'SOURCE_ATTRIBUTE': rel['source_attr'],\n",
    "                    'SOURCE_NODE_TYPE': rel['source_node_type'],\n",
    "                    'SOURCE_TYPE': rel['source_type'],\n",
    "                    'SOURCE_PQL': rel['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': km_id or rel['source_asset_id'],\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': rel['source_asset_type'],\n",
    "                    'TARGET_ID': str(module_id),\n",
    "                    'TARGET_NAME': f'{af_name} ({module_id})', \n",
    "                    'TARGET_ATTRIBUTE': target_node_name,\n",
    "                    'TARGET_NODE_TYPE': 'ACTION_FLOW',\n",
    "                    'TARGET_STUDIO_ASSET_ID': af_id,\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'ACTION_FLOW',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': km_key,\n",
    "                    'KNOWLEDGE_MODEL_ID': km_id,\n",
    "                    'DATA_MODEL_ID': dm_id,\n",
    "                    'DATA_POOL_ID': dp_id\n",
    "                })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nExtracted {len(af_lineage)} lineage records from Action Flows\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_af_lineage = pd.DataFrame(af_lineage)\n",
    "\n",
    "if not df_af_lineage.empty:\n",
    "    df_af_lineage['UNIQUE_KEY'] = (\n",
    "        df_af_lineage['DATA_MODEL_ID'].fillna('') + '.' + \n",
    "        df_af_lineage['SOURCE_NAME'].fillna('') + '.' + \n",
    "        df_af_lineage['SOURCE_ATTRIBUTE'].fillna('')\n",
    "    ).str.lower()\n",
    "\n",
    "df_af_lineage.to_csv('action_flow_lineage.csv', index=False)\n",
    "print(f\"Saved to action_flow_lineage.csv ({len(df_af_lineage)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c062dd3",
   "metadata": {},
   "source": [
    "## Append KM + Views + Action Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a2b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lineage_studio.csv\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([df_studio, df_af_lineage], ignore_index=True)\n",
    "\n",
    "# 3. Save the complete lineage table\n",
    "df_combined.to_csv('lineage_studio.csv', index=False)\n",
    "\n",
    "print(\"Saved lineage_studio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ead8b",
   "metadata": {},
   "source": [
    "## JOIN Data Models data with Studio Lineage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_models rows (Unique Inventory): 1869\n",
      "df_studio_unique rows (Unique Inventory): 480\n",
      "Table_usage_report rows: 1869\n",
      "\n",
      " Final complete lineage report saved to 'tables_usage_report.csv'\n"
     ]
    }
   ],
   "source": [
    "## JOIN Data Models data with Studio Lineage Data\n",
    "\n",
    "# --- 1. DEDUPLICATE INVENTORY (Essential Primary Key Fix) ---\n",
    "# Ensure the left table has a unique key (it should be 1:1)\n",
    "df_data_models = df_data_models.drop_duplicates(subset=['unique_id'], keep='first')\n",
    "print(f\"df_data_models rows (Unique Inventory): {len(df_data_models)}\")\n",
    "\n",
    "\n",
    "# --- 2. PREPARE STUDIO USAGE (Group usage data before merge) ---\n",
    "df_studio['JOIN_KEY'] = (\n",
    "    df_studio['DATA_MODEL_ID'] + '.' + \n",
    "    df_studio['SOURCE_NAME'] + '.' + \n",
    "    df_studio['SOURCE_ATTRIBUTE']\n",
    ").str.lower()\n",
    "\n",
    "# Create a clean list of ONLY the unique keys that were used (deduplicating multiple uses)\n",
    "df_studio_unique_usage = df_studio.drop_duplicates(subset=['JOIN_KEY'])\n",
    "print(f\"df_studio_unique rows (Unique Inventory): {len(df_studio_unique_usage)}\")\n",
    "\n",
    "# --- 3. PERFORM LEFT JOIN (Inventory size guaranteed) ---\n",
    "# Join the unique inventory (left) with the unique list of used keys (right)\n",
    "df_final = pd.merge(\n",
    "    df_data_models,\n",
    "    df_studio_unique_usage[['JOIN_KEY']], # Only need the key from the right side\n",
    "    left_on='unique_id',\n",
    "    right_on='JOIN_KEY',\n",
    "    how='left',  # This guarantees len(df_final) == len(df_data_models)\n",
    "    indicator=True,\n",
    "    suffixes=('_dm', '_studio')  # Add suffixes\n",
    ")\n",
    "\n",
    "# --- 4. CREATE USED/NOT_USED FLAG ---\n",
    "df_final['USED_NOT_USED'] = df_final['_merge'].map({\n",
    "    'both': 'USED',              # Column exists in inventory AND was found in studio usage\n",
    "    'left_only': 'NOT_USED',     # Column exists in inventory but was NOT found in studio usage\n",
    "    # 'right_only' is impossible with a Left Join on unique keys\n",
    "})\n",
    "\n",
    "# Drop the merge indicator and JOIN_KEY columns (keep unique_id)\n",
    "df_final = df_final.drop(columns=['_merge', 'JOIN_KEY'])\n",
    "\n",
    "\n",
    "# --- FINAL COLUMN SELECTION AND CLEANUP ---\n",
    "\n",
    "# Define the columns we need, using the '_dm' suffix where necessary \n",
    "columns_to_keep = [\n",
    "    'd_pool_id', \n",
    "    'd_pool_name', \n",
    "    'd_model_id', \n",
    "    'd_model_name', \n",
    "    'table_name', \n",
    "    'column_name', \n",
    "    'USED_NOT_USED'\n",
    "]\n",
    "\n",
    "# Select and rename columns for the final report\n",
    "final_output_cols = []\n",
    "for col in columns_to_keep:\n",
    "    if col + '_dm' in df_final.columns:\n",
    "         final_output_cols.append(col + '_dm')\n",
    "    else:\n",
    "         final_output_cols.append(col)\n",
    "\n",
    "# Reorder columns and select only the required ones\n",
    "df_final = df_final[final_output_cols]\n",
    "\n",
    "# Save the final result\n",
    "df_final.to_csv('tables_usage_report.csv', index=False)\n",
    "print(f\"Table_usage_report rows: {len(df_final)}\")\n",
    "print(\"\\n Final complete lineage report saved to 'tables_usage_report.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['NODE_TYPE_COMBINED'] = (\n",
    "    df_combined['SOURCE_NODE_TYPE'].astype(str) + '-' + \n",
    "    df_combined['SOURCE_TYPE'].astype(str)\n",
    ")\n",
    "\n",
    "# Create complete source nodes list with all columns from studio\n",
    "df_source_nodes = df_combined[[\n",
    "    'UNIQUE_SOURCE_ID', \n",
    "    'SOURCE_ID', \n",
    "    'NODE_TYPE_COMBINED',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE',\n",
    "    'SOURCE_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME',\n",
    "    'KNOWLEDGE_MODEL_ID',\n",
    "    'KNOWLEDGE_MODEL_KEY'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_SOURCE_ID': 'node',\n",
    "    'SOURCE_ID': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'KNOWLEDGE_MODEL_ID': 'knowledge_model_id',\n",
    "    'KNOWLEDGE_MODEL_KEY': 'knowledge_model_key',\n",
    "    'NODE_TYPE_COMBINED': 'category',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'SOURCE_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Create complete target nodes list\n",
    "df_target_nodes = df_combined[[\n",
    "    'UNIQUE_TARGET_ID', \n",
    "    'TARGET_NAME',\n",
    "    'TARGET_NODE_TYPE', \n",
    "    'TARGET_STUDIO_ASSET_TYPE',\n",
    "    'TARGET_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME',\n",
    "    'KNOWLEDGE_MODEL_ID',\n",
    "    'KNOWLEDGE_MODEL_KEY'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_TARGET_ID': 'node',\n",
    "    'TARGET_NAME': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'KNOWLEDGE_MODEL_ID': 'knowledge_model_id',\n",
    "    'KNOWLEDGE_MODEL_KEY': 'knowledge_model_key',\n",
    "    'TARGET_NODE_TYPE': 'category',\n",
    "    'TARGET_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'TARGET_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Combine ALL nodes from lineage\n",
    "df_mapping_nodes = pd.concat([df_source_nodes, df_target_nodes]).drop_duplicates(subset=['node']).reset_index(drop=True)\n",
    "\n",
    "# Save and show stats\n",
    "df_mapping_nodes.to_csv('mapping_nodes_studio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30f513",
   "metadata": {},
   "source": [
    "## Creating Table in Celonis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108573da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "\tDataPool(id='50d46c0a-39c5-4fe5-aa16-6b248fb2ac4f', name='test-pool-2'),\n",
       "\tDataPool(id='c840c791-823a-4d34-9571-f39dae182078', name='Sarvesh Monitoring Pool'),\n",
       "\tDataPool(id='e41f9df7-8011-4408-b63f-0d61ac6b8331', name='MaxPool'),\n",
       "\tDataPool(id='1e5bd13f-8b92-4ec3-8519-b356378840ec', name='LineageTest.dataPool 2025-05-04 13:17:49 [Hop Rod Rye]'),\n",
       "\tDataPool(id='ebdcfc35-b1c0-4a8c-bf5c-ae3c5f995bab', name='test pool - data lineage app [PC]'),\n",
       "\tDataPool(id='f6497957-5eac-49e9-9d44-270417bedaab', name='test-pool'),\n",
       "\tDataPool(id='6cd287bb-c8da-419d-9faa-2b42e8a08ec4', name='OCPM Data Pool'),\n",
       "\tDataPool(id='f5ab5019-fd07-4626-8a32-034b08df6f15', name='test-team-copy-for-lineage'),\n",
       "\tDataPool(id='8e16eac2-d9db-4f55-b061-f28e1110e58b', name='LineageTest.dataPool 2025-05-04 14:04:03 [St. Bernardus Abt 12]'),\n",
       "\tDataPool(id='9fed379e-98b9-4855-8127-21634200f675', name='LineageTest.dataPool 2025-05-04 14:31:56 [PÃ©chÃ© Mortel]'),\n",
       "\tDataPool(id='0847069a-7fdc-49a5-9580-1baf0e92fde2', name='LineageTest.dataPool 2025-05-05 13:18:47 [Chocolate St]'),\n",
       "\tDataPool(id='0b68c284-5147-403d-acc4-e95dba84b4dc', name='Test Data Consumption Fluctuations'),\n",
       "\tDataPool(id='f3b74a2d-06c5-4a6b-9fa5-731a2cd251ed', name='Test-Data-Pool-Copy-Imported-Connections'),\n",
       "\tDataPool(id='831db6ab-da5e-4d57-9967-c97f4320d2d6', name='Monitoring Pool'),\n",
       "\tDataPool(id='21a3ed98-f1f1-452d-9735-e58b34bac57e', name='lineage_tomas')\n",
       "]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pools = c.data_integration.get_data_pools()\n",
    "data_pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14725777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "\tDataModel(id='4ef2ec4b-aa72-4896-b53c-3d678c9683b5', name='test_tomas', pool_id='c840c791-823a-4d34-9571-f39dae182078'),\n",
       "\tDataModel(id='088283c6-9918-4b59-9a4b-96d9cb38bb15', name='full lineage', pool_id='c840c791-823a-4d34-9571-f39dae182078'),\n",
       "\tDataModel(id='81ce4087-c580-4da2-89f0-a00c1650b792', name='test_lineage', pool_id='c840c791-823a-4d34-9571-f39dae182078'),\n",
       "\tDataModel(id='13483c35-4b6a-4e90-b06c-9230b25e7b02', name='dummy_peopleDM', pool_id='c840c791-823a-4d34-9571-f39dae182078'),\n",
       "\tDataModel(id='80fea3cc-e1ba-4bbf-a29a-da1d2e50557f', name='Test Data Model', pool_id='c840c791-823a-4d34-9571-f39dae182078'),\n",
       "\tDataModel(id='24562740-9c61-4566-93ac-21f23f95a157', name='Data Lineage Minotring', pool_id='c840c791-823a-4d34-9571-f39dae182078')\n",
       "]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = c.data_integration.get_data_pool('c840c791-823a-4d34-9571-f39dae182078')\n",
    "dp.get_data_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb674a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' monitoring = data_pools.find(\"Sarvesh Monitoring Pool\")\\n\\n\\n# 1. LINEAGE TABLE (STUDIO)\\ncolumn_config_studio = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_combined.columns\\n]\\nmonitoring.create_table(\\n    df=df_combined, \\n    table_name=\\'lineage_frontend\\', \\n    column_config=column_config_studio, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded frontend lineage table\")\\n\\n# 2. MAPPING NODES\\ncolumn_config_nodes = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_mapping_nodes.columns\\n]\\nmonitoring.create_table(\\n    df=df_mapping_nodes, \\n    table_name=\\'mapping_nodes_frontend\\', \\n    column_config=column_config_nodes, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded frontend mapping nodes table\")\\n\\n# 3. BRIDGE TABLE\\ncolumn_config_bridge = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_bridge.columns\\n]\\nmonitoring.create_table(\\n    df=df_bridge, \\n    table_name=\\'bridge_lineage_mapping\\', \\n    column_config=column_config_bridge, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded bridge table\")\\n\\n# 4. USED REPORT TABLE\\ncolumn_config_report = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_final.columns\\n]\\nmonitoring.create_table(\\n    df=df_final, \\n    table_name=\\'usedtables_report_frontend\\', \\n    column_config=column_config_report, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded used tables report\") '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" monitoring = data_pools.find(\"Sarvesh Monitoring Pool\")\n",
    "\n",
    "\n",
    "# 1. LINEAGE TABLE (STUDIO)\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_combined.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_combined, \n",
    "    table_name='lineage_frontend', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend lineage table\")\n",
    "\n",
    "# 2. MAPPING NODES\n",
    "column_config_nodes = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_mapping_nodes.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_mapping_nodes, \n",
    "    table_name='mapping_nodes_frontend', \n",
    "    column_config=column_config_nodes, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend mapping nodes table\")\n",
    "\n",
    "# 3. BRIDGE TABLE\n",
    "column_config_bridge = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_bridge.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_bridge, \n",
    "    table_name='bridge_lineage_mapping', \n",
    "    column_config=column_config_bridge, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded bridge table\")\n",
    "\n",
    "# 4. USED REPORT TABLE\n",
    "column_config_report = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_final.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_final, \n",
    "    table_name='usedtables_report_frontend', \n",
    "    column_config=column_config_report, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded used tables report\") \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
