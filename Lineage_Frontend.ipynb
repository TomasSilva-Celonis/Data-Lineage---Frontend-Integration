{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0be7e7",
   "metadata": {},
   "source": [
    "## Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8eb48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pycelonis import get_celonis\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pycelonis.ems import ColumnTransport, ColumnType\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a32424",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.environ['CELONIS_URL'] = 'https://lineage.develop.celonis.cloud'\n",
    "#API Key for this specific url\n",
    "api_token = os.environ['CELONIS_API_TOKEN'] = 'MDE5YTE2M2UtZmU5YS03NTFkLWFjYmYtZGQ0NWQxODJjZmYzOmc2ZTY2UUkyU3R2RFkxVTA2L3VNc0tiZUxaVmZneHR0RVRuVS9ETFJWS3or'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5e5cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyType is not set. Defaulted to 'USER_KEY'.\n"
     ]
    }
   ],
   "source": [
    "#Initializing Celonis object\n",
    "c = get_celonis(base_url = url, api_token = api_token) # adjust base_url and api_token accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa002f1",
   "metadata": {},
   "source": [
    "## Studio Lineage (Views + Knowledge Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1969e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge_model_key(view_node):\n",
    "    km_key = json.loads(view_node.serialized_content)['metadata'].get('knowledgeModelKey')\n",
    "    return km_key\n",
    "\n",
    "def get_data_model_variable(knowledge_model):\n",
    "    try:\n",
    "        serialized = json.loads(knowledge_model.serialized_content)\n",
    "        data_model_expr = serialized.get(\"dataModelId\")\n",
    "        match = None\n",
    "        if isinstance(data_model_expr, str):\n",
    "            match = re.match(r'\\${{([a-zA-Z0-9_]+)}}', data_model_expr)\n",
    "        if not match:\n",
    "            print('ERROR: No valid Data Model Variable')\n",
    "            return None\n",
    "        return match.group(1)\n",
    "    except (KeyError, json.JSONDecodeError, AttributeError) as e:\n",
    "        print('ERROR: No Data Model Assigned')\n",
    "        return None\n",
    "\n",
    "data_pools = c.data_integration.get_data_pools()\n",
    "\n",
    "def find_data_pool_id(data_model_id):\n",
    "    data_pool_id = None\n",
    "    for data_pool in data_pools:\n",
    "        data_models = data_pool.get_data_models()\n",
    "        for data_model in data_models:\n",
    "            if data_model.id == data_model_id:\n",
    "                data_pool_id = data_pool.id\n",
    "                break\n",
    "    return data_pool_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7356550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineage API call\n",
    "def get_lineage(celonis, knowledge_model_key):\n",
    "    try:\n",
    "        lineage = celonis.client.request(\n",
    "            method='get',\n",
    "            url=f'/semantic-layer/api/usage/by-semantic-model/{knowledge_model_key}',\n",
    "            parse_json =True\n",
    "        )\n",
    "        return lineage\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d78533d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def get_km_attribute_details(knowledge_model):\\n\\n    Extracts attribute and KPI details from KM serialized_content.\\n    Returns two dicts: attribute_details and kpi_details\\n\\n    attribute_details = {}\\n    kpi_details = {}\\n\\n    try:\\n        km_content = json.loads(knowledge_model.serialized_content)\\n\\n        # Process record attributes\\n        if \\'records\\' in km_content:\\n            for record in km_content[\\'records\\']:\\n                record_id = record.get(\\'id\\')\\n\\n                if \\'attributes\\' not in record:\\n                    continue\\n\\n                for attr in record[\\'attributes\\']:\\n                    attr_id = attr.get(\\'id\\')\\n                    pql = attr.get(\\'pql\\')\\n\\n                    # Create unique key\\n                    key = f\"{record_id}.{attr_id}\".lower()\\n\\n                    # Determine source type\\n                    if pql and pql.strip():\\n                        source_type = \\'CUSTOMIZED\\'\\n                    else:\\n                        source_type = \\'AUTO_GENERATED\\'\\n\\n                    attribute_details[key] = {\\n                        \\'source_type\\': source_type,\\n                        \\'pql\\': pql if pql else \\'\\'\\n                    }\\n\\n        # Process KPIs \\n\\n        if \\'kpis\\' in km_content:\\n\\n            for i, kpi in enumerate(km_content[\\'kpis\\']):\\n                kpi_id = kpi.get(\\'id\\')\\n                kpi_pql = kpi.get(\\'pql\\', \\'\\')\\n                kpi_name = kpi.get(\\'displayName\\', kpi_id)\\n\\n                if kpi_id:\\n                    kpi_details[kpi_id] = {\\n                        \\'name\\': kpi_name,\\n                        \\'pql\\': kpi_pql,\\n                        \\'format\\': kpi.get(\\'format\\', \\'\\')\\n                    }\\n        else:\\n            print(\"No KPIs found in knowledge model content\")\\n\\n\\n    except Exception as e:\\n        print(f\"Warning: Could not extract details: {e}\")\\n        import traceback\\n        traceback.print_exc()\\n\\n    return attribute_details, kpi_details '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def get_km_attribute_details(knowledge_model):\n",
    "\n",
    "    Extracts attribute and KPI details from KM serialized_content.\n",
    "    Returns two dicts: attribute_details and kpi_details\n",
    "\n",
    "    attribute_details = {}\n",
    "    kpi_details = {}\n",
    "    \n",
    "    try:\n",
    "        km_content = json.loads(knowledge_model.serialized_content)\n",
    "        \n",
    "        # Process record attributes\n",
    "        if 'records' in km_content:\n",
    "            for record in km_content['records']:\n",
    "                record_id = record.get('id')\n",
    "                \n",
    "                if 'attributes' not in record:\n",
    "                    continue\n",
    "                \n",
    "                for attr in record['attributes']:\n",
    "                    attr_id = attr.get('id')\n",
    "                    pql = attr.get('pql')\n",
    "                    \n",
    "                    # Create unique key\n",
    "                    key = f\"{record_id}.{attr_id}\".lower()\n",
    "                    \n",
    "                    # Determine source type\n",
    "                    if pql and pql.strip():\n",
    "                        source_type = 'CUSTOMIZED'\n",
    "                    else:\n",
    "                        source_type = 'AUTO_GENERATED'\n",
    "                    \n",
    "                    attribute_details[key] = {\n",
    "                        'source_type': source_type,\n",
    "                        'pql': pql if pql else ''\n",
    "                    }\n",
    "        \n",
    "        # Process KPIs \n",
    "\n",
    "        if 'kpis' in km_content:\n",
    "            \n",
    "            for i, kpi in enumerate(km_content['kpis']):\n",
    "                kpi_id = kpi.get('id')\n",
    "                kpi_pql = kpi.get('pql', '')\n",
    "                kpi_name = kpi.get('displayName', kpi_id)\n",
    "                \n",
    "                if kpi_id:\n",
    "                    kpi_details[kpi_id] = {\n",
    "                        'name': kpi_name,\n",
    "                        'pql': kpi_pql,\n",
    "                        'format': kpi.get('format', '')\n",
    "                    }\n",
    "        else:\n",
    "            print(\"No KPIs found in knowledge model content\")\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract details: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return attribute_details, kpi_details \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c02b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_km_attribute_details(knowledge_model, data_model): \n",
    "    \"\"\"\n",
    "    Extracts attribute details and finds the 'table_schema' for each table.\n",
    "    FIXED: Manually constructs schema string to avoid 'bound method' errors.\n",
    "    \"\"\"\n",
    "    attribute_details = {}\n",
    "    kpi_details = {}\n",
    "    \n",
    "    try:\n",
    "        km_content = json.loads(knowledge_model.serialized_content)\n",
    "        \n",
    "        # --- Create a lookup map for table properties ---\n",
    "        table_properties = {}\n",
    "        if data_model:\n",
    "            pool_id = find_data_pool_id(data_model.id)\n",
    "            for table in data_model.get_tables():\n",
    "                # SAFELY construct the schema string\n",
    "                # This matches your Backend logic: pool_id + \"_\" + data_source_id\n",
    "                if table.data_source_id:\n",
    "                    safe_schema = f\"{pool_id}_{table.data_source_id}\"\n",
    "                else:\n",
    "                    safe_schema = pool_id\n",
    "\n",
    "                # Store schema by lowercase table alias\n",
    "                table_properties[table.alias_or_name.lower()] = {\n",
    "                    'table_schema': safe_schema\n",
    "                }\n",
    "        \n",
    "        # Process record attributes\n",
    "        if 'records' in km_content:\n",
    "            for record in km_content['records']:\n",
    "                record_id = record.get('id') # This is the table alias\n",
    "                table_info = table_properties.get(record_id.lower(), {})\n",
    "                \n",
    "                if 'attributes' not in record: continue\n",
    "                \n",
    "                for attr in record['attributes']:\n",
    "                    attr_id = attr.get('id')\n",
    "                    pql = attr.get('pql')\n",
    "                    key = f\"{record_id}.{attr_id}\".lower()\n",
    "                    \n",
    "                    if pql and pql.strip(): source_type = 'CUSTOMIZED'\n",
    "                    else: source_type = 'AUTO_GENERATED'\n",
    "                    \n",
    "                    attribute_details[key] = {\n",
    "                        'source_type': source_type,\n",
    "                        'pql': pql if pql else '',\n",
    "                        'table_schema': table_info.get('table_schema', '') # Defaults to empty string, not None\n",
    "                    }\n",
    "        \n",
    "        # Process KPIs \n",
    "        if 'kpis' in km_content:\n",
    "            for i, kpi in enumerate(km_content['kpis']):\n",
    "                kpi_id = kpi.get('id')\n",
    "                kpi_pql = kpi.get('pql', '')\n",
    "                kpi_name = kpi.get('displayName', kpi_id)\n",
    "                if kpi_id:\n",
    "                    kpi_details[kpi_id] = {'name': kpi_name, 'pql': kpi_pql, 'format': kpi.get('format', '')}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract details: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "        \n",
    "    return attribute_details, kpi_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd68fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_view_usages(lineage, usage: list, metadata: dict, attribute_details: dict, kpi_details: dict):\n",
    "    ## View usages\n",
    "    \n",
    "    # Record --> View\n",
    "    parent_key = 'viewUsages'\n",
    "    key = \"recordAttributeReferences\"\n",
    "    for record, attributes in lineage[parent_key][key].items():\n",
    "        for attribute, views in attributes.items():\n",
    "            # Get attribute details\n",
    "            attr_key = f\"{record}.{attribute}\".lower()\n",
    "            attr_info = attribute_details.get(attr_key, {'source_type': 'UNKNOWN', 'pql': ''})\n",
    "            \n",
    "            for view in views:\n",
    "                usage.append({\n",
    "                    'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                    'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                    'SOURCE_ID': f'{record}.{attribute}', \n",
    "                    'SOURCE_NAME': record,\n",
    "                    'SOURCE_ATTRIBUTE': attribute,\n",
    "                    'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                    'SOURCE_TYPE': attr_info['source_type'],\n",
    "                    'SOURCE_PQL': attr_info['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                    'TARGET_ID': view.get('nodeId'),\n",
    "                    'TARGET_NAME': view.get('assetName'),\n",
    "                    'TARGET_ATTRIBUTE': None,\n",
    "                    'TARGET_NODE_TYPE': 'VIEW',\n",
    "                    'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                    'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                    'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                    'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                    'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                })\n",
    "\n",
    "    # KPI --> View\n",
    "    key = 'kpiReferences'\n",
    "    \n",
    "    for kpi, views in lineage[parent_key][key].items():\n",
    "        kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "        \n",
    "        for view in views:\n",
    "            usage.append({\n",
    "                'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                'SOURCE_ID': f'{kpi}', \n",
    "                'SOURCE_NAME': kpi,\n",
    "                'SOURCE_ATTRIBUTE': None,\n",
    "                'SOURCE_NODE_TYPE': 'KPIS',\n",
    "                'SOURCE_TYPE': 'KPI',\n",
    "                'SOURCE_PQL': kpi_info['pql'],\n",
    "                'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                'TARGET_ID': view.get('nodeId'),\n",
    "                'TARGET_NAME': view.get('assetName'),\n",
    "                'TARGET_NODE_TYPE': 'VIEW',\n",
    "                'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                'DATA_SOURCE_ID': '',\n",
    "                'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "            })\n",
    "    \n",
    "    return usage\n",
    "\n",
    "def update_with_km_usages(lineage, usage, metadata, attribute_details: dict, kpi_details: dict):\n",
    "    ## Knowledge Model usages\n",
    "    \n",
    "    # Record --> KM property\n",
    "    parent_key = 'knowledgeModelUsages'\n",
    "    key = \"recordAttributeUsages\"\n",
    "    properties = ['kpis', 'filters', 'attributes', 'flags']\n",
    "    map_id = {'kpis': 'id', 'filters': 'id', 'attributes': 'recordId', 'flags': 'id'}\n",
    "\n",
    "    for property in properties:\n",
    "        for record, attributes in lineage[parent_key][key][property].items():\n",
    "            for attribute, props in attributes.items():\n",
    "                # Get attribute details\n",
    "                attr_key = f\"{record}.{attribute}\".lower()\n",
    "                attr_info = attribute_details.get(attr_key, {'source_type': 'UNKNOWN', 'pql': ''})\n",
    "                \n",
    "                for prop in props:\n",
    "                    prop_id = prop.get(map_id.get(property), 'id')\n",
    "                    prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                    usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{record}.{attribute}'.lower(), \n",
    "                        'SOURCE_NAME': record,\n",
    "                        'SOURCE_ATTRIBUTE': attribute,\n",
    "                        'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                        'SOURCE_TYPE': attr_info['source_type'],\n",
    "                        'SOURCE_PQL': attr_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "    \n",
    "    # KPI --> KM property\n",
    "    key = 'kpiUsages'\n",
    "    for property in properties:\n",
    "        for kpi, props in lineage[parent_key][key][property].items():\n",
    "            kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "            \n",
    "            for prop in props:\n",
    "                prop_id = prop.get(map_id.get(property), 'id')\n",
    "                prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{kpi}'.lower(), \n",
    "                        'SOURCE_NAME': kpi,\n",
    "                        'SOURCE_ATTRIBUTE': None,\n",
    "                        'SOURCE_NODE_TYPE': 'KPIS',\n",
    "                        'SOURCE_TYPE': 'KPI',\n",
    "                        'SOURCE_PQL': kpi_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "                \n",
    "    return usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c08ce",
   "metadata": {},
   "source": [
    "## Bridge Table - Integration between backend and frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5c3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bridge_links(data_model, knowledge_model_id, pool_table_lookup):\n",
    "    \"\"\"\n",
    "    Creates bridge links.\n",
    "    1. Raw Node: Lowercase (to match Backend physical tables)\n",
    "    2. Prefixed Node: Case-sensitive (to preserve Alias for Frontend)\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        dm_id = data_model.id\n",
    "        data_pool_id = find_data_pool_id(dm_id)\n",
    "        data_pool = c.data_integration.get_data_pool(data_pool_id)\n",
    "        \n",
    "        for table in data_model.get_tables():\n",
    "            # 1. Get the Logical Name (Alias)\n",
    "            dm_table_alias = table.alias_or_name\n",
    "            \n",
    "            # 2. Look up the PHYSICAL info\n",
    "            # We search using lowercase because your dictionary keys are lowercase\n",
    "            physical_info = pool_table_lookup.get(table.name.lower())\n",
    "            \n",
    "            if physical_info:\n",
    "                # Get the schema and name from the dictionary\n",
    "                dict_schema = physical_info['schema'].lower()\n",
    "                dict_name = physical_info['name'].lower()\n",
    "                \n",
    "                # --- BUILD SOURCE NODES ---\n",
    "                \n",
    "                # Variant 1: Raw Physical -> FORCE LOWERCASE\n",
    "                # This ensures 'BUSINESS_GRAPH_SCHEMA' becomes 'business_graph_schema'\n",
    "                # to match your Backend CSV.\n",
    "                source_node_raw = f\"{dict_schema}_{dict_name}\"\n",
    "                \n",
    "                # Variant 2: Prefixed with ID -> PRESERVE CASE\n",
    "                # We do NOT use .lower() here.\n",
    "                # It uses the schema from the dict (likely Upper) and the Alias (Mixed)\n",
    "                source_node_prefixed = f\"DATA_MODEL_TABLE_{dict_schema}_{dm_id}_{dm_table_alias}\"\n",
    "                \n",
    "            else:\n",
    "                # Fallback if physical table not found\n",
    "                continue\n",
    "\n",
    "            # 3. Create Links for Every Column\n",
    "            for column in table.get_columns():\n",
    "                \n",
    "                # Build Target (Frontend) Node ID\n",
    "                # This matches your frontend script logic (typically lowercased)\n",
    "                target_node = f\"{knowledge_model_id}.{dm_table_alias}.{column.name}\".lower()\n",
    "                \n",
    "                link_data = {\n",
    "                    \"target_node\": target_node,\n",
    "                    \"task_target\": \"DATA_MODEL_COLUMN\",\n",
    "                    \"data_pool_id\": data_pool_id,\n",
    "                    \"data_pool_name\": data_pool.name,\n",
    "                    \"data_schema_id\": dm_id, \n",
    "                    \"data_schema_name\": data_model.name\n",
    "                }\n",
    "                \n",
    "                # Add BOTH links\n",
    "                links.append({\"source_node\": source_node_raw, **link_data})\n",
    "                links.append({\"source_node\": source_node_prefixed, **link_data})\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not create bridge links for DM {data_model.name}: {e}\")\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa3aac",
   "metadata": {},
   "source": [
    "## Execution Block - Scan the environment and generate lineage and link table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c11a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space ahilmer - lineage test\n",
      "  Package Monitoring\n",
      "\tKnowledge Model Replication Cockpit Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Data Pipeline Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Data Consumption Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "Space Flo\n",
      "  Package Monitoring\n",
      "\tKnowledge Model Data Consumption Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Data Pipeline Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Replication Cockpit Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "Space FS\n",
      "  Package Data Lineage\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "Space Paula's Space\n",
      "  Package Data Lineage - Test\n",
      "\tKnowledge Model KM\n",
      "\t\t Successfully processed\n",
      "  Package Data Lineage - Monitoring Pool version\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Monitoring - Data Lineage\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Monitoring - Data Lineage Sarvesh Test\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "\tKnowledge Model Studio Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Demo\n",
      "\tKnowledge Model test:perspective_celonis_AccountsReceivable KM\n",
      "\t\t Successfully processed\n",
      "  Package Monitoring - Data Lineage Copy\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Data Lineage - New Version\n",
      "  Package Celonis Data Lineage Latest\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package full lineage\n",
      "\tKnowledge Model lineage KM\n",
      "\t\t Successfully processed\n",
      "  Package test_tomas\n",
      "\tKnowledge Model test_tomas KM\n",
      "\t\t Successfully processed\n",
      "\n",
      "Processing complete!\n",
      "Successfully processed 11 knowledge models\n",
      "Skipped 6 knowledge models due to errors\n",
      "Saved lineage_studio.csv\n",
      "Saved bridge_lineage_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "# Execution block - scanning all environment KMs\n",
    "usage = []\n",
    "all_bridge_links = []\n",
    "all_metadata = []\n",
    "error_km_count = 0\n",
    "processed_km_count = 0\n",
    "\n",
    "for space in c.studio.get_spaces():\n",
    "    print(\"Space\", space.name)\n",
    "    for package in space.get_packages():\n",
    "        print(\"  Package\", package.name)\n",
    "\n",
    "        # Safely get Knowledge Models\n",
    "        try:\n",
    "\n",
    "            knowledge_models = package.get_knowledge_models()\n",
    "        except Exception as e:\n",
    "            print(f\"    !!! CRITICAL ERROR: Could not fetch KMs for package '{package.name}'. Skipping package.\")\n",
    "            print(f\"    Error details: {e}\")\n",
    "            error_km_count += 1\n",
    "            continue # Skip to the next package\n",
    "        \n",
    "        for knowledge_model in package.get_knowledge_models():\n",
    "            try:\n",
    "                print(\"\\tKnowledge Model\", knowledge_model.name)\n",
    "                knowledge_model_key = knowledge_model.key\n",
    "                knowledge_model_id = knowledge_model.id\n",
    "                \n",
    "                # 1. Find Data Model ID\n",
    "                dm_variable = get_data_model_variable(knowledge_model)\n",
    "                if not dm_variable:\n",
    "                    print(f\"\\t\\t No data model variable found, skipping\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    package_variables = package.get_variables()\n",
    "                    variable_obj = package_variables.find(dm_variable, \"key\")\n",
    "                    data_model_id = variable_obj.value\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t\\t Error finding variable '{dm_variable}': {str(e)}\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                    \n",
    "                # 2. Find Data Pool & Data Model\n",
    "                data_pool_id = find_data_pool_id(data_model_id)\n",
    "                if data_pool_id:\n",
    "                    data_pool = c.data_integration.get_data_pool(data_pool_id)\n",
    "                    data_pool_name = data_pool.name\n",
    "                    data_model = data_pool.get_data_model(data_model_id)\n",
    "                    data_model_name = data_model.name\n",
    "                    \n",
    "                    # --- 3. Build Physical Table Lookup for this Pool (NEW) ---\n",
    "                    pool_table_lookup = {}\n",
    "                    try:\n",
    "                        for pool_table in data_pool.get_tables():\n",
    "                            # Robust Schema Logic\n",
    "                            if hasattr(pool_table, 'schema_name') and pool_table.schema_name:\n",
    "                                phys_schema = pool_table.schema_name\n",
    "                            else:\n",
    "                                phys_schema = f\"{data_pool.id}_{pool_table.data_source_id}\"\n",
    "                            \n",
    "                            # Store Lowercase Name -> Schema/Name info\n",
    "                            pool_table_lookup[pool_table.name.lower()] = {\n",
    "                                'schema': phys_schema,\n",
    "                                'name': pool_table.name\n",
    "                            }\n",
    "                    except Exception:\n",
    "                        pass \n",
    "                    # ----------------------------------------------------\n",
    "\n",
    "                    # --- 4. CREATE BRIDGE LINKS (With Lookup) ---\n",
    "                    # This creates the backend-matching links\n",
    "                    bridge_links = create_bridge_links(data_model, knowledge_model_id, pool_table_lookup)\n",
    "                    all_bridge_links.extend(bridge_links)\n",
    "\n",
    "                else:\n",
    "                    print(f\"\\t\\t Could not find data pool for data model ID: {data_model_id}\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "\n",
    "                # 5. Create Metadata Dictionary\n",
    "                metadata = {\n",
    "                    'data_model_id': data_model_id, \n",
    "                    'data_model_name': data_model_name,\n",
    "                    'data_pool_id': data_pool_id,\n",
    "                    'data_pool_name': data_pool_name,\n",
    "                    'root_id': package.id, \n",
    "                    'space_id': space.id,\n",
    "                    'knowledge_model_key': knowledge_model_key,\n",
    "                    'knowledge_model_id': knowledge_model_id\n",
    "                }\n",
    "\n",
    "                # Extract attribute and KPI details from KM\n",
    "                attribute_details, kpi_details = get_km_attribute_details(knowledge_model, data_model)\n",
    "                \n",
    "                # Get lineage\n",
    "                lineage = get_lineage(c, knowledge_model.root_with_key)\n",
    "                \n",
    "                # Check if lineage has required keys\n",
    "                if not lineage or 'viewUsages' not in lineage or 'knowledgeModelUsages' not in lineage:\n",
    "                    print(f\"\\t\\t Missing lineage data, skipping\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                update_with_view_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "                update_with_km_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "\n",
    "                all_metadata.append(metadata)\n",
    "                processed_km_count += 1\n",
    "                print(f\"\\t\\t Successfully processed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\t\\tError processing knowledge model: {str(e)}\")\n",
    "                error_km_count += 1\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Successfully processed {processed_km_count} knowledge models\")\n",
    "print(f\"Skipped {error_km_count} knowledge models due to errors\")\n",
    "            \n",
    "df_studio = pd.DataFrame(usage)\n",
    "df_studio.to_csv('lineage_studio.csv')\n",
    "print(\"Saved lineage_studio.csv\")\n",
    "\n",
    "# Create the bridge lineage CSV\n",
    "df_bridge = pd.DataFrame(all_bridge_links).drop_duplicates()\n",
    "df_bridge.to_csv('bridge_lineage_mapping.csv', index=False)\n",
    "print(\"Saved bridge_lineage_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d92f8",
   "metadata": {},
   "source": [
    "## Data Models Loads - Extract all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695b0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '34ca0a04-a73c-4ea8-ba73-f1863cf8471d',\n",
       "  'space_id': '24b90297-f60c-4ad2-b700-87ad401b9961',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'dd35ad1d-5ba1-44dc-b9a0-533587643b35'},\n",
       " {'data_model_id': '1e846159-f3a9-42a3-9d91-b3c7c0c7f904',\n",
       "  'data_model_name': 'data lineage - test',\n",
       "  'data_pool_id': 'ebdcfc35-b1c0-4a8c-bf5c-ae3c5f995bab',\n",
       "  'data_pool_name': 'test pool - data lineage app [PC]',\n",
       "  'root_id': 'c9b98018-21f1-4f3b-86b0-e1ba393125a5',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_data_lineage_test-km',\n",
       "  'knowledge_model_id': '59b95b69-c4b2-4696-b4a7-b1a4cf1235b5'},\n",
       " {'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '75324784-1854-4a68-8713-82ea390a2a79',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'b2afef54-36f0-438c-9074-f92929c0b606'},\n",
       " {'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '7fc4fd1b-6407-4cf3-8b9d-50b21397ac1b',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'dba5e290-fa7e-463e-8d1f-267762c01a41'},\n",
       " {'data_model_id': '24562740-9c61-4566-93ac-21f23f95a157',\n",
       "  'data_model_name': 'Data Lineage Minotring',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '680a5c61-f067-4b56-8dcd-e94fc0854a7c',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '75b1bc36-70b5-4f57-91d7-bbdef6ed410c'},\n",
       " {'data_model_id': '80fea3cc-e1ba-4bbf-a29a-da1d2e50557f',\n",
       "  'data_model_name': 'Test Data Model',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '680a5c61-f067-4b56-8dcd-e94fc0854a7c',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'studio-lineage',\n",
       "  'knowledge_model_id': '75fa6a46-61c2-455b-b04b-c4e42762ce5a'},\n",
       " {'data_model_id': '0ad397d5-ca2a-4a57-a0ab-76dc042117a0',\n",
       "  'data_model_name': 'test:perspective_celonis_AccountsReceivable',\n",
       "  'data_pool_id': '6cd287bb-c8da-419d-9faa-2b42e8a08ec4',\n",
       "  'data_pool_name': 'OCPM Data Pool',\n",
       "  'root_id': '8fc8ff44-1947-4cf0-8ef8-d0e4d19782fe',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_test_perspective_celonis_accountsreceivable-km',\n",
       "  'knowledge_model_id': '7de213a8-ba42-4b92-8d16-ab35f98a8b38'},\n",
       " {'data_model_id': '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '575272bd-cfd6-4849-8bab-078506b77b00',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '4d63f4e9-1e92-454e-a128-f161f43f18d6'},\n",
       " {'data_model_id': '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': 'dd496349-6fe5-47d9-b3d6-2e17906a8513',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '6fb03d9f-d533-4eb9-bcd3-3560729240f5'},\n",
       " {'data_model_id': '088283c6-9918-4b59-9a4b-96d9cb38bb15',\n",
       "  'data_model_name': 'full lineage',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '12c91533-5e38-4933-adc5-32785200bd97',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_lineage-km',\n",
       "  'knowledge_model_id': '423975bc-05cb-45fe-b0e3-58cb7cd1c4ed'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2796b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 unique Data Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'088283c6-9918-4b59-9a4b-96d9cb38bb15',\n",
       " '0ad397d5-ca2a-4a57-a0ab-76dc042117a0',\n",
       " '1e846159-f3a9-42a3-9d91-b3c7c0c7f904',\n",
       " '24562740-9c61-4566-93ac-21f23f95a157',\n",
       " '4ef2ec4b-aa72-4896-b53c-3d678c9683b5',\n",
       " '80fea3cc-e1ba-4bbf-a29a-da1d2e50557f',\n",
       " '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       " 'da1f57a6-3485-4eb3-944d-12825f56687e'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_data_model_ids = {item['data_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_data_model_ids)} unique Data Models to scan for.\")\n",
    "unique_data_model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04eea8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 unique Knowledge Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'423975bc-05cb-45fe-b0e3-58cb7cd1c4ed',\n",
       " '4d63f4e9-1e92-454e-a128-f161f43f18d6',\n",
       " '59b95b69-c4b2-4696-b4a7-b1a4cf1235b5',\n",
       " '6fb03d9f-d533-4eb9-bcd3-3560729240f5',\n",
       " '75b1bc36-70b5-4f57-91d7-bbdef6ed410c',\n",
       " '75fa6a46-61c2-455b-b04b-c4e42762ce5a',\n",
       " '7de213a8-ba42-4b92-8d16-ab35f98a8b38',\n",
       " 'b2afef54-36f0-438c-9074-f92929c0b606',\n",
       " 'bc107876-ed37-4c2c-be70-a2bfea3c36c3',\n",
       " 'dba5e290-fa7e-463e-8d1f-267762c01a41',\n",
       " 'dd35ad1d-5ba1-44dc-b9a0-533587643b35'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_KM_ids = {item['knowledge_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_KM_ids)} unique Knowledge Models to scan for.\")\n",
    "unique_KM_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d4f0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datamodel_metadata(celonis_connection):\n",
    "    \"\"\"\n",
    "    Scans for the required Data Models and returns a DataFrame of all their\n",
    "    tables and columns. Iterates through all metadata but only\n",
    "    processes each unique Data Model once.\n",
    "    \"\"\"\n",
    "    print(\"Starting Data Model Scan\")\n",
    "    data_model_data = []\n",
    "    \n",
    "    # A set to keep track of the Data Model IDs we have already processed\n",
    "    processed_data_model_ids = set()\n",
    "\n",
    "    print(\"\\nExtracting tables and columns from Data Models\")\n",
    "    # Loop through all the metadata\n",
    "    for metadata in all_metadata:\n",
    "        try:\n",
    "            current_dm_id = metadata['data_model_id']\n",
    "            \n",
    "            # If we have already processed this Data Model ID, skip to the next item\n",
    "            if current_dm_id in processed_data_model_ids:\n",
    "                continue\n",
    "\n",
    "            # If it's a new ID, process it\n",
    "            data_pool = celonis_connection.data_integration.get_data_pool(metadata['data_pool_id'])\n",
    "            data_model = data_pool.get_data_model(current_dm_id)\n",
    "            \n",
    "            for table in data_model.get_tables():\n",
    "                for column in table.get_columns():\n",
    "                    # Create unique_id from d_model_id.table_name.column_name (lowercase)\n",
    "                    unique_id = f\"{data_model.id}.{table.name}.{column.name}\".lower() \n",
    "\n",
    "                    data_model_data.append({\n",
    "                        \"unique_id\": unique_id,\n",
    "                        \"d_pool_id\": data_pool.id,\n",
    "                        \"d_pool_name\": data_pool.name,\n",
    "                        \"d_model_id\": data_model.id,\n",
    "                        \"d_model_name\": data_model.name,\n",
    "                        \"table_name\": table.name,\n",
    "                        \"column_name\": column.name\n",
    "                })\n",
    "            \n",
    "            # After successfully processing, add the ID to our set of processed IDs\n",
    "            processed_data_model_ids.add(current_dm_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Using a more specific variable for the error message\n",
    "            dm_id_for_error = metadata.get('data_model_id', 'unknown')\n",
    "            print(f\"Warning: Could not process tables for Data Model ID {dm_id_for_error}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"\\nData Model scan complete. Found {len(data_model_data)} columns in total.\")\n",
    "    \n",
    "    return pd.DataFrame(data_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fff86394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Model Scan\n",
      "\n",
      "Extracting tables and columns from Data Models\n",
      "\n",
      "Data Model scan complete. Found 1630 columns in total.\n"
     ]
    }
   ],
   "source": [
    "df_data_models = get_datamodel_metadata(c)\n",
    "df_data_models.to_csv('data_models.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ead8b",
   "metadata": {},
   "source": [
    "## JOIN Data Models data with Studio Lineage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "480eef8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_studio rows: 1873 rows\n",
      "\n",
      "df_data_models rows: 1630\n",
      "Unique IDs in df_data_models: 1566\n",
      "\n",
      "=== Verification ===\n",
      "Original df_data_models rows: 1630\n",
      "Original df_studio rows: 1873\n",
      "Final df_final rows: 3271\n",
      "\n",
      "=== Final Data Model Usage Report ===\n",
      "Total rows: 3271\n",
      "\n",
      "Usage breakdown:\n",
      "USED_NOT_USED\n",
      "USED        1873\n",
      "NOT_USED    1398\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Usage Statistics:\n",
      "- Columns/Rows USED: 1873\n",
      "- Data Model columns NOT_USED: 1398\n",
      "- Total rows: 3271\n",
      "\n",
      "Data Model Coverage:\n",
      "- Rows with Data Model info: 2499\n",
      "- Rows without Data Model info (studio only): 772\n",
      "\n",
      " Final complete lineage report saved to 'tables_usage_report.csv'\n"
     ]
    }
   ],
   "source": [
    "## JOIN Data Models data with Studio Lineage Data\n",
    "\n",
    "# Add JOIN_KEY to df_studio\n",
    "df_studio['JOIN_KEY'] = (\n",
    "    df_studio['DATA_MODEL_ID'] + '.' + \n",
    "    df_studio['SOURCE_NAME'] + '.' + \n",
    "    df_studio['SOURCE_ATTRIBUTE']\n",
    ").str.lower()\n",
    "\n",
    "print(f\"df_studio rows: {len(df_studio)} rows\")\n",
    "\n",
    "# Use existing unique_id from df_data_models (no need to create JOIN_KEY)\n",
    "print(f\"\\ndf_data_models rows: {len(df_data_models)}\")\n",
    "print(f\"Unique IDs in df_data_models: {df_data_models['unique_id'].nunique()}\")\n",
    "\n",
    "# OUTER JOIN using unique_id from df_data_models and JOIN_KEY from df_studio\n",
    "df_final = pd.merge(\n",
    "    df_data_models,\n",
    "    df_studio,\n",
    "    left_on='unique_id',\n",
    "    right_on='JOIN_KEY',\n",
    "    how='outer',  # OUTER JOIN keeps all rows from BOTH dataframes\n",
    "    indicator=True,\n",
    "    suffixes=('_dm', '_studio')  # Add suffixes to handle duplicate column names\n",
    ")\n",
    "\n",
    "# Create USED/NOT_USED column based on merge indicator\n",
    "df_final['USED_NOT_USED'] = df_final['_merge'].map({\n",
    "    'both': 'USED',              # Data model column is used in studio\n",
    "    'left_only': 'NOT_USED',     # Data model column not used\n",
    "    'right_only': 'USED'         # Studio row without data model match (still used in lineage)\n",
    "})\n",
    "\n",
    "# Drop the merge indicator and JOIN_KEY columns (keep unique_id)\n",
    "df_final = df_final.drop(columns=['_merge', 'JOIN_KEY'])\n",
    "\n",
    "# Reorder columns: \n",
    "# 1. All original data_models columns (including unique_id) - with _dm suffix if duplicated\n",
    "# 2. USED_NOT_USED column\n",
    "# 3. ALL studio columns (keeping everything, no exclusions)\n",
    "\n",
    "# Get all data model columns (unique_id is already included)\n",
    "data_model_cols = list(df_data_models.columns)\n",
    "# Update to use _dm suffix for columns that exist in both dataframes\n",
    "data_model_cols_final = []\n",
    "for col in data_model_cols:\n",
    "    if col + '_dm' in df_final.columns:\n",
    "        data_model_cols_final.append(col + '_dm')\n",
    "    else:\n",
    "        data_model_cols_final.append(col)\n",
    "\n",
    "# Get ALL studio columns (excluding only JOIN_KEY which we dropped)\n",
    "studio_cols = [col for col in df_studio.columns if col != 'JOIN_KEY']\n",
    "# Update to use _studio suffix for duplicated columns\n",
    "studio_cols_final = []\n",
    "for col in studio_cols:\n",
    "    if col + '_studio' in df_final.columns:\n",
    "        studio_cols_final.append(col + '_studio')\n",
    "    else:\n",
    "        studio_cols_final.append(col)\n",
    "\n",
    "# Final column order: data_models columns + USED_NOT_USED + ALL studio columns\n",
    "final_columns = data_model_cols_final + ['USED_NOT_USED'] + studio_cols_final\n",
    "\n",
    "# Reorder columns\n",
    "df_final = df_final[final_columns]\n",
    "\n",
    "# Verify row counts\n",
    "print(f\"\\n=== Verification ===\")\n",
    "print(f\"Original df_data_models rows: {len(df_data_models)}\")\n",
    "print(f\"Original df_studio rows: {len(df_studio)}\")\n",
    "print(f\"Final df_final rows: {len(df_final)}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== Final Data Model Usage Report ===\")\n",
    "print(f\"Total rows: {len(df_final)}\")\n",
    "print(f\"\\nUsage breakdown:\")\n",
    "print(df_final['USED_NOT_USED'].value_counts())\n",
    "\n",
    "# Calculate usage percentage\n",
    "used_count = (df_final['USED_NOT_USED'] == 'USED').sum()\n",
    "not_used_count = (df_final['USED_NOT_USED'] == 'NOT_USED').sum()\n",
    "total_count = len(df_final)\n",
    "\n",
    "print(f\"\\nUsage Statistics:\")\n",
    "print(f\"- Columns/Rows USED: {used_count}\")\n",
    "print(f\"- Data Model columns NOT_USED: {not_used_count}\")\n",
    "print(f\"- Total rows: {total_count}\")\n",
    "\n",
    "# Show breakdown by data model presence\n",
    "# Check which column name was used (with or without suffix)\n",
    "dm_id_col = 'd_model_id_dm' if 'd_model_id_dm' in df_final.columns else 'd_model_id'\n",
    "data_model_present = df_final[dm_id_col].notna().sum()\n",
    "data_model_missing = df_final[dm_id_col].isna().sum()\n",
    "\n",
    "print(f\"\\nData Model Coverage:\")\n",
    "print(f\"- Rows with Data Model info: {data_model_present}\")\n",
    "print(f\"- Rows without Data Model info (studio only): {data_model_missing}\")\n",
    "\n",
    "# Save the final result\n",
    "df_final.to_csv('tables_usage_report.csv', index=False)\n",
    "print(\"\\n Final complete lineage report saved to 'tables_usage_report.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cbf936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studio['NODE_TYPE_COMBINED'] = (\n",
    "    df_studio['SOURCE_NODE_TYPE'].astype(str) + '-' + \n",
    "    df_studio['SOURCE_TYPE'].astype(str)\n",
    ")\n",
    "\n",
    "# Create complete source nodes list with all columns from studio\n",
    "df_source_nodes = df_studio[[\n",
    "    'UNIQUE_SOURCE_ID', \n",
    "    'SOURCE_ID', \n",
    "    'NODE_TYPE_COMBINED',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE',\n",
    "    'SOURCE_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_SOURCE_ID': 'node',\n",
    "    'SOURCE_ID': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'NODE_TYPE_COMBINED': 'category',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'SOURCE_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Create complete target nodes list\n",
    "df_target_nodes = df_studio[[\n",
    "    'UNIQUE_TARGET_ID', \n",
    "    'TARGET_NAME',\n",
    "    'TARGET_NODE_TYPE', \n",
    "    'TARGET_STUDIO_ASSET_TYPE',\n",
    "    'TARGET_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_TARGET_ID': 'node',\n",
    "    'TARGET_NAME': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'TARGET_NODE_TYPE': 'category',\n",
    "    'TARGET_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'TARGET_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Combine ALL nodes from lineage\n",
    "df_mapping_nodes = pd.concat([df_source_nodes, df_target_nodes]).drop_duplicates(subset=['node']).reset_index(drop=True)\n",
    "\n",
    "\"\"\" # Now do outer join - this will keep ALL lineage nodes AND all data model columns\n",
    "df_network_nodes = pd.merge(\n",
    "    df_lineage_nodes,\n",
    "    df_data_models,\n",
    "    left_on='NODE_ID',\n",
    "    right_on='unique_id',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Fill in missing values\n",
    "df_network_nodes['NODE_ID'] = df_network_nodes['NODE_ID'].fillna(df_network_nodes['unique_id'])\n",
    "df_network_nodes['NODE_NAME'] = df_network_nodes['NODE_NAME'].fillna(df_network_nodes['column_name'])\n",
    "df_network_nodes['NODE_TYPE'] = df_network_nodes['NODE_TYPE'].fillna('DATA_COLUMN')\n",
    "\n",
    "# Use UNIQUE_KEY instead of JOIN_KEY (which was dropped)\n",
    "df_network_nodes['IS_USED'] = np.where(\n",
    "    df_network_nodes['unique_id'].notna(),\n",
    "    df_network_nodes['unique_id'].isin(df_studio['unique_key']),  # Fix: uppercase 'K',  \n",
    "    True  # Non-data-model nodes are inherently \"used\" if they're in lineage\n",
    ") \"\"\"\n",
    "\n",
    "# Save and show stats\n",
    "df_mapping_nodes.to_csv('mapping_nodes_studio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93d360ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping_nodes.to_csv('lineage_mapping_nodes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30f513",
   "metadata": {},
   "source": [
    "## Creating Table in Celonis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fb674a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Uploaded frontend lineage table\n",
      " Uploaded frontend mapping nodes table\n",
      " Uploaded bridge table\n",
      " Uploaded used tables report\n"
     ]
    }
   ],
   "source": [
    "monitoring = data_pools.find(\"Sarvesh Monitoring Pool\")\n",
    "\n",
    "# 1. LINEAGE TABLE (STUDIO)\n",
    "column_config_data_models = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=100) \n",
    "    for c in df_studio.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_studio, \n",
    "    table_name='lineage_frontend', \n",
    "    column_config=column_config_data_models, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend lineage table\")\n",
    "\n",
    "# 2. MAPPING NODES\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=5000) \n",
    "    for c in df_studio.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_mapping_nodes, \n",
    "    table_name='mapping_nodes_frontend', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend mapping nodes table\")\n",
    "\n",
    "# 3. BRIDGE TABLE\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=5000) \n",
    "    for c in df_bridge.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_bridge, \n",
    "    table_name='bridge_lineage_mapping', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded bridge table\")\n",
    "\n",
    "# 4. USED REPORT TABLE\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=5000) \n",
    "    for c in df_final.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_final, \n",
    "    table_name='usedtables_report_frontend', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded used tables report\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
