{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0be7e7",
   "metadata": {},
   "source": [
    "## Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8eb48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pycelonis import get_celonis\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pycelonis.ems import ColumnTransport, ColumnType\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a32424",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.environ['CELONIS_URL'] = 'https://lineage.develop.celonis.cloud'\n",
    "#API Key for this specific url\n",
    "api_token = os.environ['CELONIS_API_TOKEN'] = 'MDE5YTE2M2UtZmU5YS03NTFkLWFjYmYtZGQ0NWQxODJjZmYzOmc2ZTY2UUkyU3R2RFkxVTA2L3VNc0tiZUxaVmZneHR0RVRuVS9ETFJWS3or'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5e5cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyType is not set. Defaulted to 'USER_KEY'.\n"
     ]
    }
   ],
   "source": [
    "#Initializing Celonis object\n",
    "c = get_celonis(base_url = url, api_token = api_token) # adjust base_url and api_token accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa002f1",
   "metadata": {},
   "source": [
    "## Studio Lineage (Views + Knowledge Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1969e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge_model_key(view_node):\n",
    "    km_key = json.loads(view_node.serialized_content)['metadata'].get('knowledgeModelKey')\n",
    "    return km_key\n",
    "\n",
    "def get_data_model_variable(knowledge_model):\n",
    "    try:\n",
    "        serialized = json.loads(knowledge_model.serialized_content)\n",
    "        data_model_expr = serialized.get(\"dataModelId\")\n",
    "        match = None\n",
    "        if isinstance(data_model_expr, str):\n",
    "            match = re.match(r'\\${{([a-zA-Z0-9_]+)}}', data_model_expr)\n",
    "        if not match:\n",
    "            print('ERROR: No valid Data Model Variable')\n",
    "            return None\n",
    "        return match.group(1)\n",
    "    except (KeyError, json.JSONDecodeError, AttributeError) as e:\n",
    "        print('ERROR: No Data Model Assigned')\n",
    "        return None\n",
    "\n",
    "data_pools = c.data_integration.get_data_pools()\n",
    "\n",
    "def find_data_pool_id(data_model_id):\n",
    "    data_pool_id = None\n",
    "    for data_pool in data_pools:\n",
    "        data_models = data_pool.get_data_models()\n",
    "        for data_model in data_models:\n",
    "            if data_model.id == data_model_id:\n",
    "                data_pool_id = data_pool.id\n",
    "                break\n",
    "    return data_pool_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7356550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineage API call\n",
    "def get_lineage(celonis, knowledge_model_key):\n",
    "    try:\n",
    "        lineage = celonis.client.request(\n",
    "            method='get',\n",
    "            url=f'/semantic-layer/api/usage/by-semantic-model/{knowledge_model_key}',\n",
    "            parse_json =True\n",
    "        )\n",
    "        return lineage\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d78533d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def get_km_attribute_details(knowledge_model):\\n\\n    Extracts attribute and KPI details from KM serialized_content.\\n    Returns two dicts: attribute_details and kpi_details\\n\\n    attribute_details = {}\\n    kpi_details = {}\\n\\n    try:\\n        km_content = json.loads(knowledge_model.serialized_content)\\n\\n        # Process record attributes\\n        if \\'records\\' in km_content:\\n            for record in km_content[\\'records\\']:\\n                record_id = record.get(\\'id\\')\\n\\n                if \\'attributes\\' not in record:\\n                    continue\\n\\n                for attr in record[\\'attributes\\']:\\n                    attr_id = attr.get(\\'id\\')\\n                    pql = attr.get(\\'pql\\')\\n\\n                    # Create unique key\\n                    key = f\"{record_id}.{attr_id}\".lower()\\n\\n                    # Determine source type\\n                    if pql and pql.strip():\\n                        source_type = \\'CUSTOMIZED\\'\\n                    else:\\n                        source_type = \\'AUTO_GENERATED\\'\\n\\n                    attribute_details[key] = {\\n                        \\'source_type\\': source_type,\\n                        \\'pql\\': pql if pql else \\'\\'\\n                    }\\n\\n        # Process KPIs \\n\\n        if \\'kpis\\' in km_content:\\n\\n            for i, kpi in enumerate(km_content[\\'kpis\\']):\\n                kpi_id = kpi.get(\\'id\\')\\n                kpi_pql = kpi.get(\\'pql\\', \\'\\')\\n                kpi_name = kpi.get(\\'displayName\\', kpi_id)\\n\\n                if kpi_id:\\n                    kpi_details[kpi_id] = {\\n                        \\'name\\': kpi_name,\\n                        \\'pql\\': kpi_pql,\\n                        \\'format\\': kpi.get(\\'format\\', \\'\\')\\n                    }\\n        else:\\n            print(\"No KPIs found in knowledge model content\")\\n\\n\\n    except Exception as e:\\n        print(f\"Warning: Could not extract details: {e}\")\\n        import traceback\\n        traceback.print_exc()\\n\\n    return attribute_details, kpi_details '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def get_km_attribute_details(knowledge_model):\n",
    "\n",
    "    Extracts attribute and KPI details from KM serialized_content.\n",
    "    Returns two dicts: attribute_details and kpi_details\n",
    "\n",
    "    attribute_details = {}\n",
    "    kpi_details = {}\n",
    "    \n",
    "    try:\n",
    "        km_content = json.loads(knowledge_model.serialized_content)\n",
    "        \n",
    "        # Process record attributes\n",
    "        if 'records' in km_content:\n",
    "            for record in km_content['records']:\n",
    "                record_id = record.get('id')\n",
    "                \n",
    "                if 'attributes' not in record:\n",
    "                    continue\n",
    "                \n",
    "                for attr in record['attributes']:\n",
    "                    attr_id = attr.get('id')\n",
    "                    pql = attr.get('pql')\n",
    "                    \n",
    "                    # Create unique key\n",
    "                    key = f\"{record_id}.{attr_id}\".lower()\n",
    "                    \n",
    "                    # Determine source type\n",
    "                    if pql and pql.strip():\n",
    "                        source_type = 'CUSTOMIZED'\n",
    "                    else:\n",
    "                        source_type = 'AUTO_GENERATED'\n",
    "                    \n",
    "                    attribute_details[key] = {\n",
    "                        'source_type': source_type,\n",
    "                        'pql': pql if pql else ''\n",
    "                    }\n",
    "        \n",
    "        # Process KPIs \n",
    "\n",
    "        if 'kpis' in km_content:\n",
    "            \n",
    "            for i, kpi in enumerate(km_content['kpis']):\n",
    "                kpi_id = kpi.get('id')\n",
    "                kpi_pql = kpi.get('pql', '')\n",
    "                kpi_name = kpi.get('displayName', kpi_id)\n",
    "                \n",
    "                if kpi_id:\n",
    "                    kpi_details[kpi_id] = {\n",
    "                        'name': kpi_name,\n",
    "                        'pql': kpi_pql,\n",
    "                        'format': kpi.get('format', '')\n",
    "                    }\n",
    "        else:\n",
    "            print(\"No KPIs found in knowledge model content\")\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract details: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return attribute_details, kpi_details \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c02b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_km_attribute_details(knowledge_model, data_model): \n",
    "    \"\"\"\n",
    "    Extracts attribute details and finds the 'table_schema' for each table.\n",
    "    FIXED: Manually constructs schema string to avoid 'bound method' errors.\n",
    "    \"\"\"\n",
    "    attribute_details = {}\n",
    "    kpi_details = {}\n",
    "    \n",
    "    try:\n",
    "        km_content = json.loads(knowledge_model.serialized_content)\n",
    "        \n",
    "        # --- Create a lookup map for table properties ---\n",
    "        table_properties = {}\n",
    "        if data_model:\n",
    "            pool_id = find_data_pool_id(data_model.id)\n",
    "            for table in data_model.get_tables():\n",
    "                # SAFELY construct the schema string\n",
    "                # This matches your Backend logic: pool_id + \"_\" + data_source_id\n",
    "                if table.data_source_id:\n",
    "                    safe_schema = f\"{pool_id}_{table.data_source_id}\"\n",
    "                else:\n",
    "                    safe_schema = pool_id\n",
    "\n",
    "                # Store schema by lowercase table alias\n",
    "                table_properties[table.alias_or_name.lower()] = {\n",
    "                    'table_schema': safe_schema\n",
    "                }\n",
    "        \n",
    "        # Process record attributes\n",
    "        if 'records' in km_content:\n",
    "            for record in km_content['records']:\n",
    "                record_id = record.get('id') # This is the table alias\n",
    "                table_info = table_properties.get(record_id.lower(), {})\n",
    "                \n",
    "                if 'attributes' not in record: continue\n",
    "                \n",
    "                for attr in record['attributes']:\n",
    "                    attr_id = attr.get('id')\n",
    "                    pql = attr.get('pql')\n",
    "                    key = f\"{record_id}.{attr_id}\".lower()\n",
    "                    \n",
    "                    if pql and pql.strip(): source_type = 'CUSTOMIZED'\n",
    "                    else: source_type = 'AUTO_GENERATED'\n",
    "                    \n",
    "                    attribute_details[key] = {\n",
    "                        'source_type': source_type,\n",
    "                        'pql': pql if pql else '',\n",
    "                        'table_schema': table_info.get('table_schema', '') # Defaults to empty string, not None\n",
    "                    }\n",
    "        \n",
    "        # Process KPIs \n",
    "        if 'kpis' in km_content:\n",
    "            for i, kpi in enumerate(km_content['kpis']):\n",
    "                kpi_id = kpi.get('id')\n",
    "                kpi_pql = kpi.get('pql', '')\n",
    "                kpi_name = kpi.get('displayName', kpi_id)\n",
    "                if kpi_id:\n",
    "                    kpi_details[kpi_id] = {'name': kpi_name, 'pql': kpi_pql, 'format': kpi.get('format', '')}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract details: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "        \n",
    "    return attribute_details, kpi_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd68fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_view_usages(lineage, usage: list, metadata: dict, attribute_details: dict, kpi_details: dict):\n",
    "    ## View usages\n",
    "    \n",
    "    # Record --> View\n",
    "    parent_key = 'viewUsages'\n",
    "    key = \"recordAttributeReferences\"\n",
    "    for record, attributes in lineage[parent_key][key].items():\n",
    "        for attribute, views in attributes.items():\n",
    "            # Get attribute details\n",
    "            attr_key = f\"{record}.{attribute}\".lower()\n",
    "            attr_info = attribute_details.get(attr_key, {'source_type': 'UNKNOWN', 'pql': ''})\n",
    "            \n",
    "            for view in views:\n",
    "                usage.append({\n",
    "                    'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                    'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                    'SOURCE_ID': f'{record}.{attribute}', \n",
    "                    'SOURCE_NAME': record,\n",
    "                    'SOURCE_ATTRIBUTE': attribute,\n",
    "                    'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                    'SOURCE_TYPE': attr_info['source_type'],\n",
    "                    'SOURCE_PQL': attr_info['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                    'TARGET_ID': view.get('nodeId'),\n",
    "                    'TARGET_NAME': view.get('assetName'),\n",
    "                    'TARGET_ATTRIBUTE': None,\n",
    "                    'TARGET_NODE_TYPE': 'VIEW',\n",
    "                    'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                    'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                    'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                    'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                    'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                })\n",
    "\n",
    "    # KPI --> View\n",
    "    key = 'kpiReferences'\n",
    "    \n",
    "    for kpi, views in lineage[parent_key][key].items():\n",
    "        kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "        \n",
    "        for view in views:\n",
    "            usage.append({\n",
    "                'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                'SOURCE_ID': f'{kpi}', \n",
    "                'SOURCE_NAME': kpi,\n",
    "                'SOURCE_ATTRIBUTE': None,\n",
    "                'SOURCE_NODE_TYPE': 'KPIS',\n",
    "                'SOURCE_TYPE': 'KPI',\n",
    "                'SOURCE_PQL': kpi_info['pql'],\n",
    "                'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                'TARGET_ID': view.get('nodeId'),\n",
    "                'TARGET_NAME': view.get('assetName'),\n",
    "                'TARGET_NODE_TYPE': 'VIEW',\n",
    "                'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                'DATA_SOURCE_ID': '',\n",
    "                'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "            })\n",
    "    \n",
    "    return usage\n",
    "\n",
    "def update_with_km_usages(lineage, usage, metadata, attribute_details: dict, kpi_details: dict):\n",
    "    ## Knowledge Model usages\n",
    "    \n",
    "    # Record --> KM property\n",
    "    parent_key = 'knowledgeModelUsages'\n",
    "    key = \"recordAttributeUsages\"\n",
    "    properties = ['kpis', 'filters', 'attributes', 'flags']\n",
    "    map_id = {'kpis': 'id', 'filters': 'id', 'attributes': 'recordId', 'flags': 'id'}\n",
    "\n",
    "    for property in properties:\n",
    "        for record, attributes in lineage[parent_key][key][property].items():\n",
    "            for attribute, props in attributes.items():\n",
    "                # Get attribute details\n",
    "                attr_key = f\"{record}.{attribute}\".lower()\n",
    "                attr_info = attribute_details.get(attr_key, {'source_type': 'UNKNOWN', 'pql': ''})\n",
    "                \n",
    "                for prop in props:\n",
    "                    prop_id = prop.get(map_id.get(property), 'id')\n",
    "                    prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                    usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{record}.{attribute}'.lower(), \n",
    "                        'SOURCE_NAME': record,\n",
    "                        'SOURCE_ATTRIBUTE': attribute,\n",
    "                        'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                        'SOURCE_TYPE': attr_info['source_type'],\n",
    "                        'SOURCE_PQL': attr_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "    \n",
    "    # KPI --> KM property\n",
    "    key = 'kpiUsages'\n",
    "    for property in properties:\n",
    "        for kpi, props in lineage[parent_key][key][property].items():\n",
    "            kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "            \n",
    "            for prop in props:\n",
    "                prop_id = prop.get(map_id.get(property), 'id')\n",
    "                prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{kpi}'.lower(), \n",
    "                        'SOURCE_NAME': kpi,\n",
    "                        'SOURCE_ATTRIBUTE': None,\n",
    "                        'SOURCE_NODE_TYPE': 'KPIS',\n",
    "                        'SOURCE_TYPE': 'KPI',\n",
    "                        'SOURCE_PQL': kpi_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "                \n",
    "    return usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c08ce",
   "metadata": {},
   "source": [
    "## Bridge Table - Integration between backend and frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bridge_links(data_model, knowledge_model_id, pool_table_lookup):\n",
    "    \"\"\"\n",
    "    Creates bridge links.\n",
    "    1. Raw Node: Lowercase (to match Backend physical tables)\n",
    "    2. Prefixed Node: Case-sensitive (to preserve Alias for Frontend)\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        dm_id = data_model.id\n",
    "        data_pool_id = find_data_pool_id(dm_id)\n",
    "        data_pool = c.data_integration.get_data_pool(data_pool_id)\n",
    "        \n",
    "        for table in data_model.get_tables():\n",
    "            # 1. Get the Logical Name (Alias)\n",
    "            dm_table_alias = table.alias_or_name\n",
    "            \n",
    "            # 2. Look up the PHYSICAL info\n",
    "            # We search using lowercase because your dictionary keys are lowercase\n",
    "            physical_info = pool_table_lookup.get(table.name.lower())\n",
    "            \n",
    "            if physical_info:\n",
    "                # Get the schema and name from the dictionary\n",
    "                dict_schema = physical_info['schema'].lower()\n",
    "                dict_name = physical_info['name'].lower()\n",
    "                \n",
    "                # --- BUILD SOURCE NODES ---\n",
    "                \n",
    "                source_node_raw = f\"{dict_schema}_{dict_name}\"\n",
    "                source_node_prefixed = f\"DATA_MODEL_TABLE_{dict_schema}_{dm_id}_{dm_table_alias}\"\n",
    "                \n",
    "            else:\n",
    "                # Fallback if physical table not found\n",
    "                continue\n",
    "\n",
    "            # 3. Create Links for Every Column\n",
    "            for column in table.get_columns():\n",
    "                \n",
    "                # Build Target (Frontend) Node ID\n",
    "                # This matches your frontend script logic (typically lowercased)\n",
    "                target_node = f\"{knowledge_model_id}.{dm_table_alias}.{column.name}\".lower()\n",
    "                \n",
    "                link_data = {\n",
    "                    \"target_node\": target_node,\n",
    "                    \"task_target\": \"DATA_MODEL_COLUMN\",\n",
    "                    \"data_pool_id\": data_pool_id,\n",
    "                    \"data_pool_name\": data_pool.name,\n",
    "                    \"data_schema_id\": dm_id, \n",
    "                    \"data_schema_name\": data_model.name\n",
    "                }\n",
    "                \n",
    "                # Add BOTH links\n",
    "                links.append({\"source_node\": source_node_raw, **link_data})\n",
    "                links.append({\"source_node\": source_node_prefixed, **link_data})\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not create bridge links for DM {data_model.name}: {e}\")\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa3aac",
   "metadata": {},
   "source": [
    "## Execution Block - Scan the environment and generate lineage and link table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c11a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space ahilmer - lineage test\n",
      "  Package Monitoring\n",
      "\tKnowledge Model Replication Cockpit Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Data Pipeline Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Data Consumption Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "Space Flo\n",
      "  Package Monitoring\n",
      "\tKnowledge Model Data Consumption Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Data Pipeline Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "\tKnowledge Model Replication Cockpit Monitoring KM\n",
      "ERROR: No valid Data Model Variable\n",
      "\t\t No data model variable found, skipping\n",
      "Space FS\n",
      "  Package Data Lineage\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "Space Paula's Space\n",
      "  Package Data Lineage - Test\n",
      "\tKnowledge Model KM\n",
      "\t\t Successfully processed\n",
      "  Package Data Lineage - Monitoring Pool version\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Monitoring - Data Lineage\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Monitoring - Data Lineage Sarvesh Test\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "\tKnowledge Model Studio Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Demo\n",
      "\tKnowledge Model test:perspective_celonis_AccountsReceivable KM\n",
      "\t\t Successfully processed\n",
      "  Package Monitoring - Data Lineage Copy\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package Data Lineage - New Version\n",
      "  Package Celonis Data Lineage Latest\n",
      "\tKnowledge Model KM - Data Lineage\n",
      "\t\t Successfully processed\n",
      "  Package full lineage\n",
      "\tKnowledge Model lineage KM\n",
      "\t\t Successfully processed\n",
      "  Package test_tomas\n",
      "\tKnowledge Model test_tomas KM\n",
      "\t\t Successfully processed\n",
      "Space TomÃ¡s\n",
      "  Package E2E Lineage\n",
      "\tKnowledge Model full_lineage KM\n",
      "\t\t Successfully processed\n",
      "  Package testtable\n",
      "\tKnowledge Model test_tomas KM\n",
      "\t\t Successfully processed\n",
      "\n",
      "Processing complete!\n",
      "Successfully processed 13 knowledge models\n",
      "Skipped 6 knowledge models due to errors\n",
      "Saved lineage_studio.csv\n",
      "Saved bridge_lineage_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "# Execution block - scanning all environment KMs\n",
    "usage = []\n",
    "all_bridge_links = []\n",
    "all_metadata = []\n",
    "error_km_count = 0\n",
    "processed_km_count = 0\n",
    "\n",
    "for space in c.studio.get_spaces():\n",
    "    print(\"Space\", space.name)\n",
    "    for package in space.get_packages():\n",
    "        print(\"  Package\", package.name)\n",
    "\n",
    "        # Safely get Knowledge Models\n",
    "        try:\n",
    "\n",
    "            knowledge_models = package.get_knowledge_models()\n",
    "        except Exception as e:\n",
    "            print(f\"    !!! CRITICAL ERROR: Could not fetch KMs for package '{package.name}'. Skipping package.\")\n",
    "            print(f\"    Error details: {e}\")\n",
    "            error_km_count += 1\n",
    "            continue # Skip to the next package\n",
    "        \n",
    "        for knowledge_model in package.get_knowledge_models():\n",
    "            try:\n",
    "                print(\"\\tKnowledge Model\", knowledge_model.name)\n",
    "                knowledge_model_key = knowledge_model.key\n",
    "                knowledge_model_id = knowledge_model.id\n",
    "                \n",
    "                # 1. Find Data Model ID\n",
    "                dm_variable = get_data_model_variable(knowledge_model)\n",
    "                if not dm_variable:\n",
    "                    print(f\"\\t\\t No data model variable found, skipping\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    package_variables = package.get_variables()\n",
    "                    variable_obj = package_variables.find(dm_variable, \"key\")\n",
    "                    data_model_id = variable_obj.value\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t\\t Error finding variable '{dm_variable}': {str(e)}\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                    \n",
    "                # 2. Find Data Pool & Data Model\n",
    "                data_pool_id = find_data_pool_id(data_model_id)\n",
    "                if data_pool_id:\n",
    "                    data_pool = c.data_integration.get_data_pool(data_pool_id)\n",
    "                    data_pool_name = data_pool.name\n",
    "                    data_model = data_pool.get_data_model(data_model_id)\n",
    "                    data_model_name = data_model.name\n",
    "                    \n",
    "                    # --- 3. Build Physical Table Lookup for this Pool (NEW) ---\n",
    "                    pool_table_lookup = {}\n",
    "                    try:\n",
    "                        for pool_table in data_pool.get_tables():\n",
    "                            # Robust Schema Logic\n",
    "                            if hasattr(pool_table, 'schema_name') and pool_table.schema_name:\n",
    "                                phys_schema = pool_table.schema_name\n",
    "                            else:\n",
    "                                phys_schema = f\"{data_pool.id}_{pool_table.data_source_id}\"\n",
    "                            \n",
    "                            # Store Lowercase Name -> Schema/Name info\n",
    "                            pool_table_lookup[pool_table.name.lower()] = {\n",
    "                                'schema': phys_schema,\n",
    "                                'name': pool_table.name\n",
    "                            }\n",
    "                    except Exception:\n",
    "                        pass \n",
    "                    # ----------------------------------------------------\n",
    "\n",
    "                    # --- 4. CREATE BRIDGE LINKS (With Lookup) ---\n",
    "                    # This creates the backend-matching links\n",
    "                    bridge_links = create_bridge_links(data_model, knowledge_model_id, pool_table_lookup)\n",
    "                    all_bridge_links.extend(bridge_links)\n",
    "\n",
    "                else:\n",
    "                    print(f\"\\t\\t Could not find data pool for data model ID: {data_model_id}\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "\n",
    "                # 5. Create Metadata Dictionary\n",
    "                metadata = {\n",
    "                    'data_model_id': data_model_id, \n",
    "                    'data_model_name': data_model_name,\n",
    "                    'data_pool_id': data_pool_id,\n",
    "                    'data_pool_name': data_pool_name,\n",
    "                    'root_id': package.id, \n",
    "                    'space_id': space.id,\n",
    "                    'knowledge_model_key': knowledge_model_key,\n",
    "                    'knowledge_model_id': knowledge_model_id\n",
    "                }\n",
    "\n",
    "                # Extract attribute and KPI details from KM\n",
    "                attribute_details, kpi_details = get_km_attribute_details(knowledge_model, data_model)\n",
    "                \n",
    "                # Get lineage\n",
    "                lineage = get_lineage(c, knowledge_model.root_with_key)\n",
    "                \n",
    "                # Check if lineage has required keys\n",
    "                if not lineage or 'viewUsages' not in lineage or 'knowledgeModelUsages' not in lineage:\n",
    "                    print(f\"\\t\\t Missing lineage data, skipping\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                update_with_view_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "                update_with_km_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "\n",
    "                all_metadata.append(metadata)\n",
    "                processed_km_count += 1\n",
    "                print(f\"\\t\\t Successfully processed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\t\\tError processing knowledge model: {str(e)}\")\n",
    "                error_km_count += 1\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Successfully processed {processed_km_count} knowledge models\")\n",
    "print(f\"Skipped {error_km_count} knowledge models due to errors\")\n",
    "            \n",
    "df_studio = pd.DataFrame(usage)\n",
    "df_studio.to_csv('lineage_studio.csv')\n",
    "print(\"Saved lineage_studio.csv\")\n",
    "\n",
    "# Create the bridge lineage CSV\n",
    "df_bridge = pd.DataFrame(all_bridge_links).drop_duplicates()\n",
    "df_bridge.to_csv('bridge_lineage_mapping.csv', index=False)\n",
    "print(\"Saved bridge_lineage_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d92f8",
   "metadata": {},
   "source": [
    "## Data Models Loads - Extract all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4695b0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '34ca0a04-a73c-4ea8-ba73-f1863cf8471d',\n",
       "  'space_id': '24b90297-f60c-4ad2-b700-87ad401b9961',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'dd35ad1d-5ba1-44dc-b9a0-533587643b35'},\n",
       " {'data_model_id': '1e846159-f3a9-42a3-9d91-b3c7c0c7f904',\n",
       "  'data_model_name': 'data lineage - test',\n",
       "  'data_pool_id': 'ebdcfc35-b1c0-4a8c-bf5c-ae3c5f995bab',\n",
       "  'data_pool_name': 'test pool - data lineage app [PC]',\n",
       "  'root_id': 'c9b98018-21f1-4f3b-86b0-e1ba393125a5',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_data_lineage_test-km',\n",
       "  'knowledge_model_id': '59b95b69-c4b2-4696-b4a7-b1a4cf1235b5'},\n",
       " {'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '75324784-1854-4a68-8713-82ea390a2a79',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'b2afef54-36f0-438c-9074-f92929c0b606'},\n",
       " {'data_model_id': 'da1f57a6-3485-4eb3-944d-12825f56687e',\n",
       "  'data_model_name': 'Data Lineage Monitoring',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '7fc4fd1b-6407-4cf3-8b9d-50b21397ac1b',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': 'dba5e290-fa7e-463e-8d1f-267762c01a41'},\n",
       " {'data_model_id': '24562740-9c61-4566-93ac-21f23f95a157',\n",
       "  'data_model_name': 'Data Lineage Minotring',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '680a5c61-f067-4b56-8dcd-e94fc0854a7c',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '75b1bc36-70b5-4f57-91d7-bbdef6ed410c'},\n",
       " {'data_model_id': '80fea3cc-e1ba-4bbf-a29a-da1d2e50557f',\n",
       "  'data_model_name': 'Test Data Model',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '680a5c61-f067-4b56-8dcd-e94fc0854a7c',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'studio-lineage',\n",
       "  'knowledge_model_id': '75fa6a46-61c2-455b-b04b-c4e42762ce5a'},\n",
       " {'data_model_id': '0ad397d5-ca2a-4a57-a0ab-76dc042117a0',\n",
       "  'data_model_name': 'test:perspective_celonis_AccountsReceivable',\n",
       "  'data_pool_id': '6cd287bb-c8da-419d-9faa-2b42e8a08ec4',\n",
       "  'data_pool_name': 'OCPM Data Pool',\n",
       "  'root_id': '8fc8ff44-1947-4cf0-8ef8-d0e4d19782fe',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_test_perspective_celonis_accountsreceivable-km',\n",
       "  'knowledge_model_id': '7de213a8-ba42-4b92-8d16-ab35f98a8b38'},\n",
       " {'data_model_id': '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': '575272bd-cfd6-4849-8bab-078506b77b00',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '4d63f4e9-1e92-454e-a128-f161f43f18d6'},\n",
       " {'data_model_id': '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       "  'data_model_name': 'Data Lineage Monitoring - Latest',\n",
       "  'data_pool_id': '831db6ab-da5e-4d57-9967-c97f4320d2d6',\n",
       "  'data_pool_name': 'Monitoring Pool',\n",
       "  'root_id': 'dd496349-6fe5-47d9-b3d6-2e17906a8513',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'km-data-lineage',\n",
       "  'knowledge_model_id': '6fb03d9f-d533-4eb9-bcd3-3560729240f5'},\n",
       " {'data_model_id': '088283c6-9918-4b59-9a4b-96d9cb38bb15',\n",
       "  'data_model_name': 'full lineage',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '12c91533-5e38-4933-adc5-32785200bd97',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_lineage-km',\n",
       "  'knowledge_model_id': '423975bc-05cb-45fe-b0e3-58cb7cd1c4ed'},\n",
       " {'data_model_id': '4ef2ec4b-aa72-4896-b53c-3d678c9683b5',\n",
       "  'data_model_name': 'test_tomas',\n",
       "  'data_pool_id': 'c840c791-823a-4d34-9571-f39dae182078',\n",
       "  'data_pool_name': 'Sarvesh Monitoring Pool',\n",
       "  'root_id': '93a2829f-90e9-4b8d-8702-cd37600c1a11',\n",
       "  'space_id': '4e7bf097-7bea-48e5-af3d-035576e47309',\n",
       "  'knowledge_model_key': 'dm_test_tomas-km',\n",
       "  'knowledge_model_id': 'bc107876-ed37-4c2c-be70-a2bfea3c36c3'},\n",
       " {'data_model_id': 'bbcfb903-9134-433a-9c91-08fb75ad3d3f',\n",
       "  'data_model_name': 'full_lineage',\n",
       "  'data_pool_id': '21a3ed98-f1f1-452d-9735-e58b34bac57e',\n",
       "  'data_pool_name': 'lineage_tomas',\n",
       "  'root_id': '46e5eb37-062d-4f53-9add-3d24876f0a94',\n",
       "  'space_id': 'a871e45c-16c5-4e85-9e00-38f3853d5149',\n",
       "  'knowledge_model_key': 'dm_full_lineage-km',\n",
       "  'knowledge_model_id': '97ac8f46-c6f4-4b60-ba05-e7bf59863ddf'},\n",
       " {'data_model_id': '951165b9-1905-4c81-ad3b-4f5f94099f39',\n",
       "  'data_model_name': 'test_tomas',\n",
       "  'data_pool_id': '21a3ed98-f1f1-452d-9735-e58b34bac57e',\n",
       "  'data_pool_name': 'lineage_tomas',\n",
       "  'root_id': '29339c3b-0d84-4ffb-a655-157b96eeedb6',\n",
       "  'space_id': 'a871e45c-16c5-4e85-9e00-38f3853d5149',\n",
       "  'knowledge_model_key': 'dm_test_tomas-km',\n",
       "  'knowledge_model_id': '47b9f27c-9b09-4660-9f19-756390716613'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2796b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 unique Data Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'088283c6-9918-4b59-9a4b-96d9cb38bb15',\n",
       " '0ad397d5-ca2a-4a57-a0ab-76dc042117a0',\n",
       " '1e846159-f3a9-42a3-9d91-b3c7c0c7f904',\n",
       " '24562740-9c61-4566-93ac-21f23f95a157',\n",
       " '4ef2ec4b-aa72-4896-b53c-3d678c9683b5',\n",
       " '80fea3cc-e1ba-4bbf-a29a-da1d2e50557f',\n",
       " '904789ed-fa95-4ee4-81fa-b1cb1a8987e2',\n",
       " '951165b9-1905-4c81-ad3b-4f5f94099f39',\n",
       " 'bbcfb903-9134-433a-9c91-08fb75ad3d3f',\n",
       " 'da1f57a6-3485-4eb3-944d-12825f56687e'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_data_model_ids = {item['data_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_data_model_ids)} unique Data Models to scan for.\")\n",
    "unique_data_model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04eea8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 unique Knowledge Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'423975bc-05cb-45fe-b0e3-58cb7cd1c4ed',\n",
       " '47b9f27c-9b09-4660-9f19-756390716613',\n",
       " '4d63f4e9-1e92-454e-a128-f161f43f18d6',\n",
       " '59b95b69-c4b2-4696-b4a7-b1a4cf1235b5',\n",
       " '6fb03d9f-d533-4eb9-bcd3-3560729240f5',\n",
       " '75b1bc36-70b5-4f57-91d7-bbdef6ed410c',\n",
       " '75fa6a46-61c2-455b-b04b-c4e42762ce5a',\n",
       " '7de213a8-ba42-4b92-8d16-ab35f98a8b38',\n",
       " '97ac8f46-c6f4-4b60-ba05-e7bf59863ddf',\n",
       " 'b2afef54-36f0-438c-9074-f92929c0b606',\n",
       " 'bc107876-ed37-4c2c-be70-a2bfea3c36c3',\n",
       " 'dba5e290-fa7e-463e-8d1f-267762c01a41',\n",
       " 'dd35ad1d-5ba1-44dc-b9a0-533587643b35'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_KM_ids = {item['knowledge_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_KM_ids)} unique Knowledge Models to scan for.\")\n",
    "unique_KM_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d4f0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datamodel_metadata(celonis_connection):\n",
    "    \"\"\"\n",
    "    Scans for the required Data Models and returns a DataFrame of all their\n",
    "    tables and columns. Iterates through all metadata but only\n",
    "    processes each unique Data Model once.\n",
    "    \"\"\"\n",
    "    print(\"Starting Data Model Scan\")\n",
    "    data_model_data = []\n",
    "    \n",
    "    # A set to keep track of the Data Model IDs we have already processed\n",
    "    processed_data_model_ids = set()\n",
    "\n",
    "    print(\"\\nExtracting tables and columns from Data Models\")\n",
    "    # Loop through all the metadata\n",
    "    for metadata in all_metadata:\n",
    "        try:\n",
    "            current_dm_id = metadata['data_model_id']\n",
    "            \n",
    "            # If we have already processed this Data Model ID, skip to the next item\n",
    "            if current_dm_id in processed_data_model_ids:\n",
    "                continue\n",
    "\n",
    "            # If it's a new ID, process it\n",
    "            data_pool = celonis_connection.data_integration.get_data_pool(metadata['data_pool_id'])\n",
    "            data_model = data_pool.get_data_model(current_dm_id)\n",
    "            \n",
    "            for table in data_model.get_tables():\n",
    "                for column in table.get_columns():\n",
    "                    # Create unique_id from d_model_id.table_name.column_name (lowercase)\n",
    "                    unique_id = f\"{data_model.id}.{table.name}.{column.name}\".lower() \n",
    "\n",
    "                    data_model_data.append({\n",
    "                        \"unique_id\": unique_id,\n",
    "                        \"d_pool_id\": data_pool.id,\n",
    "                        \"d_pool_name\": data_pool.name,\n",
    "                        \"d_model_id\": data_model.id,\n",
    "                        \"d_model_name\": data_model.name,\n",
    "                        \"table_name\": table.name,\n",
    "                        \"column_name\": column.name\n",
    "                })\n",
    "            \n",
    "            # After successfully processing, add the ID to our set of processed IDs\n",
    "            processed_data_model_ids.add(current_dm_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Using a more specific variable for the error message\n",
    "            dm_id_for_error = metadata.get('data_model_id', 'unknown')\n",
    "            print(f\"Warning: Could not process tables for Data Model ID {dm_id_for_error}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert the list to a DataFrame and drop duplicates based on the primary key\n",
    "    return pd.DataFrame(data_model_data).drop_duplicates(subset=['unique_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff86394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Model Scan\n",
      "\n",
      "Extracting tables and columns from Data Models\n",
      "\n",
      "Data Model scan complete. Found 1574 columns in total.\n"
     ]
    }
   ],
   "source": [
    "df_data_models = get_datamodel_metadata(c)\n",
    "print(f\"\\nData Model scan complete. Found {len(df_data_models)} columns in total.\")\n",
    "\n",
    "df_data_models.to_csv('data_models.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ead8b",
   "metadata": {},
   "source": [
    "## JOIN Data Models data with Studio Lineage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3f0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_models rows (Unique Inventory): 1574\n",
      "df_studio_unique rows (Unique Inventory): 466\n",
      "Table_usage_report rows: 1574\n",
      "\n",
      " Final complete lineage report saved to 'tables_usage_report.csv'\n"
     ]
    }
   ],
   "source": [
    "## JOIN Data Models data with Studio Lineage Data\n",
    "\n",
    "# --- 1. DEDUPLICATE INVENTORY (Essential Primary Key Fix) ---\n",
    "# Ensure the left table has a unique key (it should be 1:1)\n",
    "df_data_models = df_data_models.drop_duplicates(subset=['unique_id'], keep='first')\n",
    "print(f\"df_data_models rows (Unique Inventory): {len(df_data_models)}\")\n",
    "\n",
    "\n",
    "# --- 2. PREPARE STUDIO USAGE (Group usage data before merge) ---\n",
    "df_studio['JOIN_KEY'] = (\n",
    "    df_studio['DATA_MODEL_ID'] + '.' + \n",
    "    df_studio['SOURCE_NAME'] + '.' + \n",
    "    df_studio['SOURCE_ATTRIBUTE']\n",
    ").str.lower()\n",
    "\n",
    "# Create a clean list of ONLY the unique keys that were used (deduplicating multiple uses)\n",
    "df_studio_unique_usage = df_studio.drop_duplicates(subset=['JOIN_KEY'])\n",
    "print(f\"df_studio_unique rows (Unique Inventory): {len(df_studio_unique_usage)}\")\n",
    "\n",
    "# --- 3. PERFORM LEFT JOIN (Inventory size guaranteed) ---\n",
    "# Join the unique inventory (left) with the unique list of used keys (right)\n",
    "df_final = pd.merge(\n",
    "    df_data_models,\n",
    "    df_studio_unique_usage[['JOIN_KEY']], # Only need the key from the right side\n",
    "    left_on='unique_id',\n",
    "    right_on='JOIN_KEY',\n",
    "    how='left',  # This guarantees len(df_final) == len(df_data_models)\n",
    "    indicator=True,\n",
    "    suffixes=('_dm', '_studio')  # Add suffixes\n",
    ")\n",
    "\n",
    "# --- 4. CREATE USED/NOT_USED FLAG ---\n",
    "df_final['USED_NOT_USED'] = df_final['_merge'].map({\n",
    "    'both': 'USED',              # Column exists in inventory AND was found in studio usage\n",
    "    'left_only': 'NOT_USED',     # Column exists in inventory but was NOT found in studio usage\n",
    "    # 'right_only' is impossible with a Left Join on unique keys\n",
    "})\n",
    "\n",
    "# Drop the merge indicator and JOIN_KEY columns (keep unique_id)\n",
    "df_final = df_final.drop(columns=['_merge', 'JOIN_KEY'])\n",
    "\n",
    "\n",
    "# --- FINAL COLUMN SELECTION AND CLEANUP ---\n",
    "\n",
    "# Define the columns we need, using the '_dm' suffix where necessary \n",
    "columns_to_keep = [\n",
    "    'd_pool_id', \n",
    "    'd_pool_name', \n",
    "    'd_model_id', \n",
    "    'd_model_name', \n",
    "    'table_name', \n",
    "    'column_name', \n",
    "    'USED_NOT_USED'\n",
    "]\n",
    "\n",
    "# Select and rename columns for the final report\n",
    "final_output_cols = []\n",
    "for col in columns_to_keep:\n",
    "    if col + '_dm' in df_final.columns:\n",
    "         final_output_cols.append(col + '_dm')\n",
    "    else:\n",
    "         final_output_cols.append(col)\n",
    "\n",
    "# Reorder columns and select only the required ones\n",
    "df_final = df_final[final_output_cols]\n",
    "\n",
    "# Save the final result\n",
    "df_final.to_csv('tables_usage_report.csv', index=False)\n",
    "print(f\"Table_usage_report rows: {len(df_final)}\")\n",
    "print(\"\\n Final complete lineage report saved to 'tables_usage_report.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cbf936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studio['NODE_TYPE_COMBINED'] = (\n",
    "    df_studio['SOURCE_NODE_TYPE'].astype(str) + '-' + \n",
    "    df_studio['SOURCE_TYPE'].astype(str)\n",
    ")\n",
    "\n",
    "# Create complete source nodes list with all columns from studio\n",
    "df_source_nodes = df_studio[[\n",
    "    'UNIQUE_SOURCE_ID', \n",
    "    'SOURCE_ID', \n",
    "    'NODE_TYPE_COMBINED',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE',\n",
    "    'SOURCE_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_SOURCE_ID': 'node',\n",
    "    'SOURCE_ID': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'NODE_TYPE_COMBINED': 'category',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'SOURCE_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Create complete target nodes list\n",
    "df_target_nodes = df_studio[[\n",
    "    'UNIQUE_TARGET_ID', \n",
    "    'TARGET_NAME',\n",
    "    'TARGET_NODE_TYPE', \n",
    "    'TARGET_STUDIO_ASSET_TYPE',\n",
    "    'TARGET_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_TARGET_ID': 'node',\n",
    "    'TARGET_NAME': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'TARGET_NODE_TYPE': 'category',\n",
    "    'TARGET_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'TARGET_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Combine ALL nodes from lineage\n",
    "df_mapping_nodes = pd.concat([df_source_nodes, df_target_nodes]).drop_duplicates(subset=['node']).reset_index(drop=True)\n",
    "\n",
    "\"\"\" # Now do outer join - this will keep ALL lineage nodes AND all data model columns\n",
    "df_network_nodes = pd.merge(\n",
    "    df_lineage_nodes,\n",
    "    df_data_models,\n",
    "    left_on='NODE_ID',\n",
    "    right_on='unique_id',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Fill in missing values\n",
    "df_network_nodes['NODE_ID'] = df_network_nodes['NODE_ID'].fillna(df_network_nodes['unique_id'])\n",
    "df_network_nodes['NODE_NAME'] = df_network_nodes['NODE_NAME'].fillna(df_network_nodes['column_name'])\n",
    "df_network_nodes['NODE_TYPE'] = df_network_nodes['NODE_TYPE'].fillna('DATA_COLUMN')\n",
    "\n",
    "# Use UNIQUE_KEY instead of JOIN_KEY (which was dropped)\n",
    "df_network_nodes['IS_USED'] = np.where(\n",
    "    df_network_nodes['unique_id'].notna(),\n",
    "    df_network_nodes['unique_id'].isin(df_studio['unique_key']),  # Fix: uppercase 'K',  \n",
    "    True  # Non-data-model nodes are inherently \"used\" if they're in lineage\n",
    ") \"\"\"\n",
    "\n",
    "# Save and show stats\n",
    "df_mapping_nodes.to_csv('mapping_nodes_studio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d360ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping_nodes.to_csv('lineage_mapping_nodes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30f513",
   "metadata": {},
   "source": [
    "## Creating Table in Celonis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "108573da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "\tDataPool(id='50d46c0a-39c5-4fe5-aa16-6b248fb2ac4f', name='test-pool-2'),\n",
       "\tDataPool(id='c840c791-823a-4d34-9571-f39dae182078', name='Sarvesh Monitoring Pool'),\n",
       "\tDataPool(id='e41f9df7-8011-4408-b63f-0d61ac6b8331', name='MaxPool'),\n",
       "\tDataPool(id='1e5bd13f-8b92-4ec3-8519-b356378840ec', name='LineageTest.dataPool 2025-05-04 13:17:49 [Hop Rod Rye]'),\n",
       "\tDataPool(id='ebdcfc35-b1c0-4a8c-bf5c-ae3c5f995bab', name='test pool - data lineage app [PC]'),\n",
       "\tDataPool(id='f6497957-5eac-49e9-9d44-270417bedaab', name='test-pool'),\n",
       "\tDataPool(id='6cd287bb-c8da-419d-9faa-2b42e8a08ec4', name='OCPM Data Pool'),\n",
       "\tDataPool(id='f5ab5019-fd07-4626-8a32-034b08df6f15', name='test-team-copy-for-lineage'),\n",
       "\tDataPool(id='8e16eac2-d9db-4f55-b061-f28e1110e58b', name='LineageTest.dataPool 2025-05-04 14:04:03 [St. Bernardus Abt 12]'),\n",
       "\tDataPool(id='9fed379e-98b9-4855-8127-21634200f675', name='LineageTest.dataPool 2025-05-04 14:31:56 [PÃ©chÃ© Mortel]'),\n",
       "\tDataPool(id='0847069a-7fdc-49a5-9580-1baf0e92fde2', name='LineageTest.dataPool 2025-05-05 13:18:47 [Chocolate St]'),\n",
       "\tDataPool(id='0b68c284-5147-403d-acc4-e95dba84b4dc', name='Test Data Consumption Fluctuations'),\n",
       "\tDataPool(id='831db6ab-da5e-4d57-9967-c97f4320d2d6', name='Monitoring Pool'),\n",
       "\tDataPool(id='f3b74a2d-06c5-4a6b-9fa5-731a2cd251ed', name='Test-Data-Pool-Copy-Imported-Connections'),\n",
       "\tDataPool(id='21a3ed98-f1f1-452d-9735-e58b34bac57e', name='lineage_tomas')\n",
       "]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pools = c.data_integration.get_data_pools()\n",
    "data_pools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d45a7",
   "metadata": {},
   "source": [
    "APPEND THE DATA TO THE TABLE INSTEAD OF CREATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb674a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Uploaded frontend lineage table\n",
      " Uploaded frontend mapping nodes table\n",
      " Uploaded bridge table\n",
      " Uploaded used tables report\n"
     ]
    }
   ],
   "source": [
    "monitoring = data_pools.find(\"lineage_tomas\")\n",
    "\n",
    "\n",
    "# 1. LINEAGE TABLE (STUDIO)\n",
    "column_config_data_models = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=100) \n",
    "    for c in df_studio.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_studio, \n",
    "    table_name='lineage_frontend', \n",
    "    column_config=column_config_data_models, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend lineage table\")\n",
    "\n",
    "# 2. MAPPING NODES\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=5000) \n",
    "    for c in df_studio.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_mapping_nodes, \n",
    "    table_name='mapping_nodes_frontend', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend mapping nodes table\")\n",
    "\n",
    "# 3. BRIDGE TABLE\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=5000) \n",
    "    for c in df_bridge.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_bridge, \n",
    "    table_name='bridge_lineage_mapping', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded bridge table\")\n",
    "\n",
    "# 4. USED REPORT TABLE\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=5000) \n",
    "    for c in df_final.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_final, \n",
    "    table_name='usedtables_report_frontend', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded used tables report\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
