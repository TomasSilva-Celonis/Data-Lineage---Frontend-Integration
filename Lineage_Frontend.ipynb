{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0be7e7",
   "metadata": {},
   "source": [
    "## Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8eb48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pycelonis import get_celonis\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pycelonis.ems import ColumnTransport, ColumnType\n",
    "import traceback\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a32424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" url = os.environ['CELONIS_URL'] = 'https://lineage.develop.celonis.cloud'\\n#API Key for this specific url\\napi_token = os.environ['CELONIS_API_TOKEN'] = 'MDE5YTE2M2UtZmU5YS03NTFkLWFjYmYtZGQ0NWQxODJjZmYzOmc2ZTY2UUkyU3R2RFkxVTA2L3VNc0tiZUxaVmZneHR0RVRuVS9ETFJWS3or' \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = os.environ['CELONIS_URL'] = 'https://lineage.develop.celonis.cloud'\n",
    "#API Key for this specific url\n",
    "api_token = os.environ['CELONIS_API_TOKEN'] = 'MDE5YTE2M2UtZmU5YS03NTFkLWFjYmYtZGQ0NWQxODJjZmYzOmc2ZTY2UUkyU3R2RFkxVTA2L3VNc0tiZUxaVmZneHR0RVRuVS9ETFJWS3or'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" url = os.environ['CELONIS_URL'] = 'https://snap-tiger-team-celonis-com.eu-1.celonis.cloud'\n",
    "#API Key for this specific url\n",
    "api_token = os.environ['CELONIS_API_TOKEN'] = 'Nzk3MTliYWUtM2U4ZC00MmJmLWFlMGMtMzY5YmIxZWRhNDgxOjVMQ1lxaGwyeGZLbWxvbGVOeTkzYzcrZEQ1cTgzSnlDN29CUXpFNlRRMXZG' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5e5cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyType is not set. Defaulted to 'USER_KEY'.\n"
     ]
    }
   ],
   "source": [
    "#Initializing Celonis object\n",
    "c = get_celonis(base_url = url, api_token = api_token) # adjust base_url and api_token accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa002f1",
   "metadata": {},
   "source": [
    "## Studio Lineage (Views + Knowledge Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1969e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge_model_key(view_node):\n",
    "    km_key = json.loads(view_node.serialized_content)['metadata'].get('knowledgeModelKey')\n",
    "    return km_key\n",
    "\n",
    "def get_data_model_variable(knowledge_model):\n",
    "    try:\n",
    "        serialized = json.loads(knowledge_model.serialized_content)\n",
    "        data_model_expr = serialized.get(\"dataModelId\")\n",
    "        match = None\n",
    "        if isinstance(data_model_expr, str):\n",
    "            match = re.match(r'\\${{([a-zA-Z0-9_]+)}}', data_model_expr)\n",
    "        if not match:\n",
    "            print('ERROR: No valid Data Model Variable')\n",
    "            return None\n",
    "        return match.group(1)\n",
    "    except (KeyError, json.JSONDecodeError, AttributeError) as e:\n",
    "        print('ERROR: No Data Model Assigned')\n",
    "        return None\n",
    "\n",
    "data_pools = c.data_integration.get_data_pools()\n",
    "\n",
    "def find_data_pool_id(data_model_id):\n",
    "    data_pool_id = None\n",
    "    for data_pool in data_pools:\n",
    "        data_models = data_pool.get_data_models()\n",
    "        for data_model in data_models:\n",
    "            if data_model.id == data_model_id:\n",
    "                data_pool_id = data_pool.id\n",
    "                break\n",
    "    return data_pool_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7356550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineage API call\n",
    "def get_lineage(celonis, knowledge_model_key):\n",
    "    try:\n",
    "        lineage = celonis.client.request(\n",
    "            method='get',\n",
    "            url=f'/semantic-layer/api/usage/by-semantic-model/{knowledge_model_key}',\n",
    "            parse_json =True\n",
    "        )\n",
    "        return lineage\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c02b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_km_attribute_details(knowledge_model, data_model): \n",
    "    \"\"\"\n",
    "    Extracts attribute details and finds the 'table_schema' for each table.\n",
    "    FIXED: Manually constructs schema string to avoid 'bound method' errors.\n",
    "    \"\"\"\n",
    "    attribute_details = {}\n",
    "    kpi_details = {}\n",
    "    \n",
    "    try:\n",
    "        km_content = json.loads(knowledge_model.serialized_content)\n",
    "        \n",
    "        # --- Create a lookup map for table properties ---\n",
    "        table_properties = {}\n",
    "        if data_model:\n",
    "            pool_id = find_data_pool_id(data_model.id)\n",
    "            for table in data_model.get_tables():\n",
    "                # SAFELY construct the schema string\n",
    "                # This matches your Backend logic: pool_id + \"_\" + data_source_id\n",
    "                if table.data_source_id:\n",
    "                    safe_schema = f\"{pool_id}_{table.data_source_id}\"\n",
    "                else:\n",
    "                    safe_schema = pool_id\n",
    "\n",
    "                # Store schema by lowercase table alias\n",
    "                table_properties[table.alias_or_name.lower()] = {\n",
    "                    'table_schema': safe_schema\n",
    "                }\n",
    "        \n",
    "        # Process record attributes\n",
    "        if 'records' in km_content:\n",
    "            for record in km_content['records']:\n",
    "                record_id = record.get('id') # This is the table alias\n",
    "                table_info = table_properties.get(record_id.lower(), {})\n",
    "                \n",
    "                if 'attributes' not in record: continue\n",
    "                \n",
    "                for attr in record['attributes']:\n",
    "                    attr_id = attr.get('id')\n",
    "                    pql = attr.get('pql')\n",
    "                    key = f\"{record_id}.{attr_id}\".lower()\n",
    "                    \n",
    "                    if pql and pql.strip(): source_type = 'CALCULATED'\n",
    "                    else: source_type = 'AUTO_GENERATED'\n",
    "                    \n",
    "                    attribute_details[key] = {\n",
    "                        'source_type': source_type,\n",
    "                        'pql': pql if pql else '',\n",
    "                        'table_schema': table_info.get('table_schema', '') # Defaults to empty string, not None\n",
    "                    }\n",
    "        \n",
    "        # Process KPIs \n",
    "        if 'kpis' in km_content:\n",
    "            for i, kpi in enumerate(km_content['kpis']):\n",
    "                kpi_id = kpi.get('id')\n",
    "                kpi_pql = kpi.get('pql', '')\n",
    "                kpi_name = kpi.get('displayName', kpi_id)\n",
    "                if kpi_id:\n",
    "                    kpi_details[kpi_id] = {'name': kpi_name, 'pql': kpi_pql, 'format': kpi.get('format', '')}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract details: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "        \n",
    "    return attribute_details, kpi_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd68fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_view_usages(lineage, usage: list, metadata: dict, attribute_details: dict, kpi_details: dict):\n",
    "    ## View usages\n",
    "    \n",
    "    # Record --> View\n",
    "    parent_key = 'viewUsages'\n",
    "    key = \"recordAttributeReferences\"\n",
    "    for record, attributes in lineage[parent_key][key].items():\n",
    "        for attribute, views in attributes.items():\n",
    "            # Get attribute details\n",
    "            attr_key = f\"{record}.{attribute}\".lower()\n",
    "            attr_info = attribute_details.get(attr_key, {'source_type': 'AUTO_GENERATED', 'pql': ''})\n",
    "            \n",
    "            for view in views:\n",
    "                usage.append({\n",
    "                    'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                    'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                    'SOURCE_ID': f'{record}.{attribute}', \n",
    "                    'SOURCE_NAME': record,\n",
    "                    'SOURCE_ATTRIBUTE': attribute,\n",
    "                    'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                    'SOURCE_TYPE': attr_info['source_type'],\n",
    "                    'SOURCE_PQL': attr_info['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                    'TARGET_ID': view.get('nodeId'),\n",
    "                    'TARGET_NAME': view.get('assetName'),\n",
    "                    'TARGET_ATTRIBUTE': None,\n",
    "                    'TARGET_NODE_TYPE': 'VIEW',\n",
    "                    'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                    'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                    'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                    'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                    'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                    'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                })\n",
    "\n",
    "    # KPI --> View\n",
    "    key = 'kpiReferences'\n",
    "    \n",
    "    for kpi, views in lineage[parent_key][key].items():\n",
    "        kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "        \n",
    "        for view in views:\n",
    "            usage.append({\n",
    "                'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                'UNIQUE_TARGET_ID': f'{metadata.get(\"root_id\")}.{view.get(\"nodeId\")}'.lower(),\n",
    "                'SOURCE_ID': f'{kpi}', \n",
    "                'SOURCE_NAME': kpi,\n",
    "                'SOURCE_ATTRIBUTE': None,\n",
    "                'SOURCE_NODE_TYPE': 'KPI',\n",
    "                'SOURCE_TYPE': '',\n",
    "                'SOURCE_PQL': kpi_info['pql'],\n",
    "                'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                'TARGET_ID': view.get('nodeId'),\n",
    "                'TARGET_NAME': view.get('assetName'),\n",
    "                'TARGET_NODE_TYPE': 'VIEW',\n",
    "                'TARGET_STUDIO_ASSET_ID': view.get('nodeId'),\n",
    "                'TARGET_STUDIO_ASSET_TYPE': 'VIEW',\n",
    "                'DATA_SOURCE_ID': '',\n",
    "                'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "            })\n",
    "    \n",
    "    return usage\n",
    "\n",
    "def update_with_km_usages(lineage, usage, metadata, attribute_details: dict, kpi_details: dict):\n",
    "    ## Knowledge Model usages\n",
    "    \n",
    "    # Record --> KM property\n",
    "    parent_key = 'knowledgeModelUsages'\n",
    "    key = \"recordAttributeUsages\"\n",
    "    properties = ['kpis', 'filters', 'attributes', 'flags']\n",
    "    map_id = {'kpis': 'id', 'filters': 'id', 'attributes': 'recordId', 'flags': 'id'}\n",
    "\n",
    "    for property in properties:\n",
    "        for record, attributes in lineage[parent_key][key][property].items():\n",
    "            for attribute, props in attributes.items():\n",
    "                # Get attribute details\n",
    "                attr_key = f\"{record}.{attribute}\".lower()\n",
    "                attr_info = attribute_details.get(attr_key, {'source_type': 'AUTO_GENERATED', 'pql': ''})\n",
    "                \n",
    "                for prop in props:\n",
    "                    prop_id = prop.get(map_id.get(property), 'id')\n",
    "                    prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                    usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{record}.{attribute}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{record}.{attribute}'.lower(), \n",
    "                        'SOURCE_NAME': record,\n",
    "                        'SOURCE_ATTRIBUTE': attribute,\n",
    "                        'SOURCE_NODE_TYPE': 'ATTRIBUTES',\n",
    "                        'SOURCE_TYPE': attr_info['source_type'],\n",
    "                        'SOURCE_PQL': attr_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "    \n",
    "    # KPI --> KM property\n",
    "    key = 'kpiUsages'\n",
    "    for property in properties:\n",
    "        for kpi, props in lineage[parent_key][key][property].items():\n",
    "            kpi_info = kpi_details.get(kpi, {'name': kpi, 'pql': ''})\n",
    "            \n",
    "            for prop in props:\n",
    "                prop_id = prop.get(map_id.get(property), 'id')\n",
    "                prop_id = f'{prop_id}.{prop.get(\"attributeId\")}' if prop.get('attributeId') else prop_id\n",
    "                usage.append({\n",
    "                        'UNIQUE_SOURCE_ID': f'{metadata.get('knowledge_model_id')}.{kpi}'.lower(),\n",
    "                        'UNIQUE_TARGET_ID': f'{metadata.get(\"knowledge_model_id\")}.{prop_id}'.lower(),\n",
    "                        'SOURCE_ID': f'{kpi}'.lower(), \n",
    "                        'SOURCE_NAME': kpi,\n",
    "                        'SOURCE_ATTRIBUTE': None,\n",
    "                        'SOURCE_NODE_TYPE': 'KPI',\n",
    "                        'SOURCE_TYPE': '',\n",
    "                        'SOURCE_PQL': kpi_info['pql'],\n",
    "                        'SOURCE_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'SOURCE_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                        'TARGET_ID': f'{prop_id}'.lower(),\n",
    "                        'TARGET_NAME': prop.get('displayName'),\n",
    "                        'TARGET_ATTRIBUTE': prop.get('attributeId'),\n",
    "                        'TARGET_NODE_TYPE': property.upper(),\n",
    "                        'TARGET_STUDIO_ASSET_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'TARGET_STUDIO_ASSET_TYPE':'KNOWLEDGE_MODEL',\n",
    "                        'DATA_SOURCE_ID': '',\n",
    "                        'KNOWLEDGE_MODEL_KEY': metadata.get('knowledge_model_key'),\n",
    "                        'KNOWLEDGE_MODEL_ID': metadata.get(\"knowledge_model_id\"),\n",
    "                        'DATA_MODEL_ID': metadata.get('data_model_id'),\n",
    "                        'DATA_MODEL_NAME': metadata.get('data_model_name'),\n",
    "                        'DATA_POOL_ID': metadata.get('data_pool_id'),\n",
    "                        'DATA_POOL_NAME': metadata.get('data_pool_name')\n",
    "                    })\n",
    "                \n",
    "    return usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c08ce",
   "metadata": {},
   "source": [
    "## Bridge Table - Integration between backend and frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5c3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bridge_links(data_model, knowledge_model_id, pool_table_lookup):\n",
    "    \"\"\"\n",
    "    Creates bridge links.\n",
    "    1. Raw Node: Lowercase (to match Backend physical tables)\n",
    "    2. Prefixed Node: Case-sensitive (to preserve Alias for Frontend)\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        dm_id = data_model.id\n",
    "        data_pool_id = find_data_pool_id(dm_id)\n",
    "        data_pool = c.data_integration.get_data_pool(data_pool_id)\n",
    "        \n",
    "        for table in data_model.get_tables():\n",
    "            # 1. Get the Logical Name (Alias)\n",
    "            dm_table_alias = table.alias_or_name\n",
    "            \n",
    "            # 2. Look up the PHYSICAL info\n",
    "            # We search using lowercase because your dictionary keys are lowercase\n",
    "            physical_info = pool_table_lookup.get(table.name.lower())\n",
    "            \n",
    "            if physical_info:\n",
    "                # Get the schema and name from the dictionary\n",
    "                dict_schema = physical_info['schema'].lower()\n",
    "                dict_name = physical_info['name'].lower()\n",
    "                \n",
    "                # --- BUILD SOURCE NODES ---\n",
    "                \n",
    "                source_node_raw = f\"{dict_schema}_{dict_name}\"\n",
    "                source_node_prefixed = f\"DATA_MODEL_TABLE_{dict_schema}_{dm_id}_{dm_table_alias}\"\n",
    "                \n",
    "            else:\n",
    "                # Fallback if physical table not found\n",
    "                continue\n",
    "\n",
    "            # 3. Create Links for Every Column\n",
    "            for column in table.get_columns():\n",
    "                \n",
    "                # Build Target (Frontend) Node ID\n",
    "                # This matches your frontend script logic (typically lowercased)\n",
    "                target_node = f\"{knowledge_model_id}.{dm_table_alias}.{column.name}\".lower()\n",
    "                \n",
    "                link_data = {\n",
    "                    \"target_node\": target_node,\n",
    "                    \"task_target\": \"DATA_MODEL_COLUMN\",\n",
    "                    \"data_pool_id\": data_pool_id,\n",
    "                    \"data_pool_name\": data_pool.name,\n",
    "                    \"data_model_id\": dm_id, \n",
    "                    \"data_model_name\": data_model.name\n",
    "                }\n",
    "                \n",
    "                # Add BOTH links\n",
    "                links.append({\"source_node\": source_node_raw, **link_data})\n",
    "                links.append({\"source_node\": source_node_prefixed, **link_data})\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Could not create bridge links for DM {data_model.name}: {e}\")\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa3aac",
   "metadata": {},
   "source": [
    "## Execution Block - Scan the environment and generate lineage and link table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c11a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space Action Flows Monitoring (Working)\n",
      "  Package Action Flows Monitor (With Dependencies)\n",
      "\tKnowledge Model afdb-km-test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 2. Find Data Pool & Data Model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m data_pool_id = \u001b[43mfind_data_pool_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_model_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_pool_id:\n\u001b[32m     47\u001b[39m     data_pool = c.data_integration.get_data_pool(data_pool_id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mfind_data_pool_id\u001b[39m\u001b[34m(data_model_id)\u001b[39m\n\u001b[32m     23\u001b[39m data_pool_id = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data_pool \u001b[38;5;129;01min\u001b[39;00m data_pools:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     data_models = \u001b[43mdata_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m data_model \u001b[38;5;129;01min\u001b[39;00m data_models:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data_model.id == data_model_id:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pycelonis/ems/data_integration/data_pool.py:200\u001b[39m, in \u001b[36mDataPool.get_data_models\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_data_models\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CelonisCollection[\u001b[33m\"\u001b[39m\u001b[33mDataModel\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    195\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Gets all data models of given data pool.\u001b[39;00m\n\u001b[32m    196\u001b[39m \n\u001b[32m    197\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m        A list containing all data models.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     data_model_transports = \u001b[43mIntegrationService\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_api_pools_pool_id_data_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CelonisCollection(\n\u001b[32m    202\u001b[39m         DataModel.from_transport(\u001b[38;5;28mself\u001b[39m.client, data_model_transport)\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m data_model_transport \u001b[38;5;129;01min\u001b[39;00m data_model_transports\n\u001b[32m    204\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data_model_transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    205\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pycelonis/service/integration/service.py:2534\u001b[39m, in \u001b[36mIntegrationService.get_api_pools_pool_id_data_models\u001b[39m\u001b[34m(client, pool_id, limit, **kwargs)\u001b[39m\n\u001b[32m   2532\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2533\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33mlimit\u001b[39m\u001b[33m\"\u001b[39m] = limit\n\u001b[32m-> \u001b[39m\u001b[32m2534\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/integration/api/pools/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpool_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/data-models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_json\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDataModelTransport\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pycelonis_core/client/client.py:156\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, timeout, follow_redirects, type_, parse_json, request_body)\u001b[39m\n\u001b[32m    142\u001b[39m request_headers = \u001b[38;5;28mself\u001b[39m._get_request_headers(headers)\n\u001b[32m    143\u001b[39m request = \u001b[38;5;28mself\u001b[39m.client.build_request(\n\u001b[32m    144\u001b[39m     method=method,\n\u001b[32m    145\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     timeout=timeout,\n\u001b[32m    154\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_processor.process(response=response, type_=type_, parse_json=parse_json)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pycelonis_core/utils/httpx.py:51\u001b[39m, in \u001b[36mRetryTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_retryable(request.method):\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_request_with_retry(request)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Execution block - scanning all environment KMs\n",
    "usage = []\n",
    "all_bridge_links = []\n",
    "all_metadata = []\n",
    "error_km_count = 0\n",
    "processed_km_count = 0\n",
    "\n",
    "for space in c.studio.get_spaces():\n",
    "    print(\"Space\", space.name)\n",
    "    for package in space.get_packages():\n",
    "        print(\"  Package\", package.name)\n",
    "\n",
    "        # Safely get Knowledge Models\n",
    "        try:\n",
    "            knowledge_models = package.get_knowledge_models()\n",
    "        except Exception as e:\n",
    "            print(f\"    !!! CRITICAL ERROR: Could not fetch KMs for package '{package.name}'. Skipping package.\")\n",
    "            print(f\"    Error details: {e}\")\n",
    "            error_km_count += 1\n",
    "            continue # Skip to the next package\n",
    "        \n",
    "        for knowledge_model in package.get_knowledge_models():\n",
    "            try:\n",
    "                print(\"\\tKnowledge Model\", knowledge_model.name)\n",
    "                knowledge_model_key = knowledge_model.key\n",
    "                knowledge_model_id = knowledge_model.id\n",
    "                \n",
    "                # 1. Find Data Model ID\n",
    "                dm_variable = get_data_model_variable(knowledge_model)\n",
    "                if not dm_variable:\n",
    "                    print(f\"\\t\\t No data model variable found, skipping\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    package_variables = package.get_variables()\n",
    "                    variable_obj = package_variables.find(dm_variable, \"key\")\n",
    "                    data_model_id = variable_obj.value\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t\\t Error finding variable '{dm_variable}': {str(e)}\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                    \n",
    "                # 2. Find Data Pool & Data Model\n",
    "                data_pool_id = find_data_pool_id(data_model_id)\n",
    "                if data_pool_id:\n",
    "                    data_pool = c.data_integration.get_data_pool(data_pool_id)\n",
    "                    data_pool_name = data_pool.name\n",
    "                    data_model = data_pool.get_data_model(data_model_id)\n",
    "                    data_model_name = data_model.name\n",
    "                    \n",
    "                    # --- 3. Build Physical Table Lookup for this Pool (NEW) ---\n",
    "                    pool_table_lookup = {}\n",
    "                    try:\n",
    "                        for pool_table in data_pool.get_tables():\n",
    "                            # Robust Schema Logic\n",
    "                            if hasattr(pool_table, 'schema_name') and pool_table.schema_name:\n",
    "                                phys_schema = pool_table.schema_name\n",
    "                            else:\n",
    "                                phys_schema = f\"{data_pool.id}_{pool_table.data_source_id}\"\n",
    "                            \n",
    "                            # Store Lowercase Name -> Schema/Name info\n",
    "                            pool_table_lookup[pool_table.name.lower()] = {\n",
    "                                'schema': phys_schema,\n",
    "                                'name': pool_table.name\n",
    "                            }\n",
    "                    except Exception:\n",
    "                        pass \n",
    "                    # ----------------------------------------------------\n",
    "\n",
    "                    # --- 4. CREATE BRIDGE LINKS (With Lookup) ---\n",
    "                    # This creates the backend-matching links\n",
    "                    bridge_links = create_bridge_links(data_model, knowledge_model_id, pool_table_lookup)\n",
    "                    all_bridge_links.extend(bridge_links)\n",
    "\n",
    "                else:\n",
    "                    print(f\"\\t\\t Could not find data pool for data model ID: {data_model_id}\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "\n",
    "                # 5. Create Metadata Dictionary\n",
    "                metadata = {\n",
    "                    'data_model_id': data_model_id, \n",
    "                    'data_model_name': data_model_name,\n",
    "                    'data_pool_id': data_pool_id,\n",
    "                    'data_pool_name': data_pool_name,\n",
    "                    'root_id': package.id, \n",
    "                    'space_id': space.id,\n",
    "                    'knowledge_model_key': knowledge_model_key,\n",
    "                    'knowledge_model_id': knowledge_model_id\n",
    "                }\n",
    "\n",
    "                # Extract attribute and KPI details from KM\n",
    "                attribute_details, kpi_details = get_km_attribute_details(knowledge_model, data_model)\n",
    "                \n",
    "                # Get lineage\n",
    "                lineage = get_lineage(c, knowledge_model.root_with_key)\n",
    "                \n",
    "                # Check if lineage has required keys\n",
    "                if not lineage or 'viewUsages' not in lineage or 'knowledgeModelUsages' not in lineage:\n",
    "                    print(f\"\\t\\t Missing lineage data, skipping\")\n",
    "                    error_km_count += 1\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                update_with_view_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "                update_with_km_usages(lineage, usage, metadata, attribute_details, kpi_details)\n",
    "\n",
    "                all_metadata.append(metadata)\n",
    "                processed_km_count += 1\n",
    "                print(f\"\\t\\t Successfully processed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\t\\tError processing knowledge model: {str(e)}\")\n",
    "                error_km_count += 1\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Successfully processed {processed_km_count} knowledge models\")\n",
    "print(f\"Skipped {error_km_count} knowledge models due to errors\")\n",
    "            \n",
    "df_studio = pd.DataFrame(usage)\n",
    "\n",
    "\n",
    "# Create the bridge lineage CSV\n",
    "df_bridge = pd.DataFrame(all_bridge_links).drop_duplicates()\n",
    "df_bridge.to_csv('bridge_lineage_mapping.csv', index=False)\n",
    "print(\"Saved bridge_lineage_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d92f8",
   "metadata": {},
   "source": [
    "## Data Models Loads - Extract all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4695b0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data_model_id': '1c1e8492-6555-48a8-95b3-a9038659131f',\n",
       "  'data_model_name': 'SAP ECC - Order to Cash Data Model',\n",
       "  'data_pool_id': '0cca1931-70ef-49b4-a097-332895c99c21',\n",
       "  'data_pool_name': 'SAP ECC - Order to Cash (OM + AR)',\n",
       "  'root_id': 'a16b0ed3-bed8-43e2-9c9d-773b0b0dbe2b',\n",
       "  'space_id': '57d0b829-fcf2-4363-a5ce-dff77daaad14',\n",
       "  'knowledge_model_key': 'km-test',\n",
       "  'knowledge_model_id': '6dc41ceb-3d03-4f90-9500-4d6a1db05f67'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2796b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique Data Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1c1e8492-6555-48a8-95b3-a9038659131f'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_data_model_ids = {item['data_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_data_model_ids)} unique Data Models to scan for.\")\n",
    "unique_data_model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04eea8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique Knowledge Models to scan for.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'6dc41ceb-3d03-4f90-9500-4d6a1db05f67'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of data model IDs we need to find\n",
    "unique_KM_ids = {item['knowledge_model_id'] for item in all_metadata}\n",
    "print(f\"Found {len(unique_KM_ids)} unique Knowledge Models to scan for.\")\n",
    "unique_KM_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d4f0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datamodel_metadata(celonis_connection):\n",
    "    \"\"\"\n",
    "    Scans for the required Data Models and returns a DataFrame of all their\n",
    "    tables and columns. Iterates through all metadata but only\n",
    "    processes each unique Data Model once.\n",
    "    \"\"\"\n",
    "    print(\"Starting Data Model Scan\")\n",
    "    data_model_data = []\n",
    "    \n",
    "    # A set to keep track of the Data Model IDs we have already processed\n",
    "    processed_data_model_ids = set()\n",
    "\n",
    "    print(\"\\nExtracting tables and columns from Data Models\")\n",
    "    # Loop through all the metadata\n",
    "    for metadata in all_metadata:\n",
    "        try:\n",
    "            current_dm_id = metadata['data_model_id']\n",
    "            \n",
    "            # If we have already processed this Data Model ID, skip to the next item\n",
    "            if current_dm_id in processed_data_model_ids:\n",
    "                continue\n",
    "\n",
    "            # If it's a new ID, process it\n",
    "            data_pool = celonis_connection.data_integration.get_data_pool(metadata['data_pool_id'])\n",
    "            data_model = data_pool.get_data_model(current_dm_id)\n",
    "            \n",
    "            for table in data_model.get_tables():\n",
    "                for column in table.get_columns():\n",
    "                    # Create unique_id from d_model_id.table_name.column_name (lowercase)\n",
    "                    unique_id = f\"{data_model.id}.{table.name}.{column.name}\".lower() \n",
    "\n",
    "                    data_model_data.append({\n",
    "                        \"unique_id\": unique_id,\n",
    "                        \"d_pool_id\": data_pool.id,\n",
    "                        \"d_pool_name\": data_pool.name,\n",
    "                        \"d_model_id\": data_model.id,\n",
    "                        \"d_model_name\": data_model.name,\n",
    "                        \"table_name\": table.name,\n",
    "                        \"column_name\": column.name\n",
    "                })\n",
    "            \n",
    "            # After successfully processing, add the ID to our set of processed IDs\n",
    "            processed_data_model_ids.add(current_dm_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Using a more specific variable for the error message\n",
    "            dm_id_for_error = metadata.get('data_model_id', 'unknown')\n",
    "            print(f\"Warning: Could not process tables for Data Model ID {dm_id_for_error}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert the list to a DataFrame and drop duplicates based on the primary key\n",
    "    return pd.DataFrame(data_model_data).drop_duplicates(subset=['unique_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff86394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Model Scan\n",
      "\n",
      "Extracting tables and columns from Data Models\n",
      "\n",
      "Data Model scan complete. Found 2353 columns in total.\n"
     ]
    }
   ],
   "source": [
    "df_data_models = get_datamodel_metadata(c)\n",
    "print(f\"\\nData Model scan complete. Found {len(df_data_models)} columns in total.\")\n",
    "\n",
    "df_data_models.to_csv('data_models.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7ee5f",
   "metadata": {},
   "source": [
    "## Studio Lineage - Action Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc31c25",
   "metadata": {},
   "source": [
    "### Extract Blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d846cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blueprint_api(\n",
    "    source, # This should be a pycelonis connection object\n",
    "    package, # This should be a pycelonis package object\n",
    "    node_data: dict,\n",
    "    source_space: object, # This is your custom config object\n",
    "    published: bool,\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"Exports blueprint and stores it in blueprint folder with User key\n",
    "\n",
    "    Args:\n",
    "        package: package where blueprint is\n",
    "        node_data: dict containing serializedContent\n",
    "        source_space: config object with blueprint paths\n",
    "        published: whether to get published or draft version\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_blueprint_api(\n",
    "        source,\n",
    "        package,\n",
    "        parsed_node_data: dict,  # Already parsed JSON\n",
    "        published: bool,\n",
    "    ) -> dict:\n",
    "        \"\"\"Function extracts the blueprint given the Action Flow data\n",
    "\n",
    "        Args:\n",
    "            source: Celonis object for source Team\n",
    "            package: Celonis object for package where the Action Flow is stored\n",
    "            parsed_node_data: Already parsed node data from serializedContent\n",
    "\n",
    "        Returns:\n",
    "            a json with the blueprint of the Action Flow\n",
    "        \"\"\"\n",
    "\n",
    "        def get_blueprint_versions(source, package, parsed_node_data):\n",
    "            base_url = source.client.base_url\n",
    "            blueprint_url = f\"{base_url}/ems-automation/api/root/{parsed_node_data['rootNodeId']}/asset/{parsed_node_data['key']}/proxy/api/v2/scenarios/{parsed_node_data['scenarioId']}/blueprints\"\n",
    "            blueprints_version = source.client.request(\n",
    "                url=blueprint_url, method=\"get\", parse_json=True\n",
    "            )\n",
    "            return blueprints_version\n",
    "\n",
    "        def convert_to_timestamp(timestamp_string):\n",
    "            timestamp = datetime.strptime(timestamp_string, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            return int(timestamp.timestamp())\n",
    "\n",
    "        base_url = source.client.base_url\n",
    "\n",
    "        # Get the blueprint's versions\n",
    "        blueprints = get_blueprint_versions(source, package, parsed_node_data)\n",
    "        blueprints[\"scenariosBlueprints\"] = sorted(\n",
    "            blueprints[\"scenariosBlueprints\"],\n",
    "            key=lambda x: x[\"created\"],\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        blueprint_url = f\"{base_url}/ems-automation/api/root/{parsed_node_data['rootNodeId']}/asset/{parsed_node_data['key']}/proxy/api/v2/scenarios/{parsed_node_data['scenarioId']}/blueprint\"\n",
    "        \n",
    "        if published:\n",
    "            # try getting the published version\n",
    "            blueprint_info = [\n",
    "                b for b in blueprints[\"scenariosBlueprints\"] if not b[\"draft\"]\n",
    "            ]\n",
    "            # if not possible -> get the draft version\n",
    "            if not blueprint_info:\n",
    "                blueprint_info = [\n",
    "                    b for b in blueprints[\"scenariosBlueprints\"] if b[\"draft\"]\n",
    "                ]\n",
    "        else:\n",
    "            # try getting the draft version\n",
    "            blueprint_info = [\n",
    "                b for b in blueprints[\"scenariosBlueprints\"] if b[\"draft\"]\n",
    "            ]\n",
    "            # if not possible -> get the published version\n",
    "            if not blueprint_info:\n",
    "                blueprint_info = [\n",
    "                    b for b in blueprints[\"scenariosBlueprints\"] if not b[\"draft\"]\n",
    "                ]\n",
    "\n",
    "        if blueprint_info:\n",
    "            id_timestamp = convert_to_timestamp(blueprint_info[0].get(\"created\"))\n",
    "            query_url = (\n",
    "                f\"blueprintId={blueprint_info[0].get('version')}&_={id_timestamp}\"\n",
    "            )\n",
    "\n",
    "            blueprint = source.client.request(\n",
    "                url=f\"{blueprint_url}?{query_url}\", method=\"get\", parse_json=True\n",
    "            )\n",
    "\n",
    "            if \"response\" in blueprint:\n",
    "                return blueprint[\"response\"][\"blueprint\"]\n",
    "\n",
    "    # Parse the serialized content ONCE at the beginning\n",
    "    parsed_node_data = json.loads(node_data[\"serializedContent\"])\n",
    "    \n",
    "    # Extract the key from parsed data\n",
    "    AF_key = parsed_node_data[\"key\"]\n",
    "    \n",
    "    # Get the blueprint using the parsed data\n",
    "    blueprint = extract_blueprint_api(source, package, parsed_node_data, published)\n",
    "    \n",
    "    # All blueprints (both draft and published) go directly into \"blueprints\" folder\n",
    "    bp_path = \"blueprints\"\n",
    "        \n",
    "    if not blueprint:\n",
    "        # Create a blank blueprint\n",
    "        blueprint = json.loads(\n",
    "            \"\"\"\n",
    "            {\n",
    "                \"flow\": [\n",
    "                    {\n",
    "                        \"id\": null,\n",
    "                        \"module\": \"placeholder:Placeholder\",\n",
    "                        \"metadata\": {\n",
    "                            \"designer\": {\n",
    "                                \"x\": 0,\n",
    "                                \"y\": 0\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"metadata\": {\n",
    "                    \"instant\": false,\n",
    "                    \"version\": 1,\n",
    "                    \"scenario\": {\n",
    "                        \"roundtrips\": 1,\n",
    "                        \"maxErrors\": 3,\n",
    "                        \"autoCommit\": true,\n",
    "                        \"autoCommitTriggerLast\": true,\n",
    "                        \"sequential\": false,\n",
    "                        \"confidential\": false,\n",
    "                        \"dataloss\": false,\n",
    "                        \"dlq\": false\n",
    "                    },\n",
    "                    \"designer\": {\n",
    "                        \"orphans\": []\n",
    "                    },\n",
    "                    \"zone\": \"integromat.try.k8s.celonis.cloud\"\n",
    "                }\n",
    "            }\"\"\"\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        file_name = f\"{AF_key}&{package.key}.json\"\n",
    "        print(f\"Blueprint extracted. Saving to: {file_name}\")\n",
    "\n",
    "    with open(bp_path + f\"/{AF_key}&{package.key}.json\", \"w\") as out:\n",
    "        json.dump(blueprint, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb636271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_blueprints():\n",
    "    \"\"\"\n",
    "    Download all Action Flow blueprints from all spaces and packages.\n",
    "    First deletes the existing blueprints directory and then creates a fresh one.\n",
    "    \"\"\"\n",
    "    print(\"-\"*80)\n",
    "    print(\"DOWNLOADING ACTION FLOW BLUEPRINTS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Delete existing blueprints directory if it exists\n",
    "    if os.path.exists(\"blueprints\"):\n",
    "        print(\"Removing existing blueprints directory...\")\n",
    "        shutil.rmtree(\"blueprints\")\n",
    "        print(\"Existing blueprints directory removed.\")\n",
    "    \n",
    "    # Create fresh blueprints directory\n",
    "    print(\"Creating new blueprints directory...\")\n",
    "    os.makedirs(\"blueprints\")\n",
    "    \n",
    "    total_blueprints_downloaded = 0\n",
    "    \n",
    "    # Iterate through all spaces and packages\n",
    "    for space in c.studio.get_spaces():\n",
    "        print(f\"\\nSpace: {space.name}\")\n",
    "        \n",
    "        for package in space.get_packages():\n",
    "            print(f\"  Package: {package.name}\")\n",
    "            \n",
    "            try:\n",
    "                # Get all nodes in the package\n",
    "                nodes = package.get_content_nodes()\n",
    "                \n",
    "                # Filter for SCENARIO nodes (Action Flows)\n",
    "                scenario_nodes = [node for node in nodes if hasattr(node, 'asset_type') and node.asset_type == 'SCENARIO']\n",
    "                \n",
    "                if not scenario_nodes:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"    Found {len(scenario_nodes)} Action Flow(s)\")\n",
    "                \n",
    "                # Process each Action Flow\n",
    "                for node in scenario_nodes:\n",
    "                    try:\n",
    "                        print(f\"      Processing: {node.name}\")\n",
    "                        \n",
    "                        # Prepare node_data dict\n",
    "                        node_data = {\n",
    "                            'serializedContent': node.serialized_content\n",
    "                        }\n",
    "                        \n",
    "                        # Download blueprint\n",
    "                        download_blueprint_api(\n",
    "                            source=c,\n",
    "                            package=package,\n",
    "                            node_data=node_data,\n",
    "                            source_space=None,\n",
    "                            published=False\n",
    "                        )\n",
    "                        \n",
    "                        total_blueprints_downloaded += 1\n",
    "                        print(f\"        Blueprint downloaded\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"        Error: {str(e)[:100]}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error accessing package: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"SUMMARY: Downloaded {total_blueprints_downloaded} blueprints\")\n",
    "    print(f\"{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7ee951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DOWNLOADING ACTION FLOW BLUEPRINTS\n",
      "--------------------------------------------------------------------------------\n",
      "Removing existing blueprints directory...\n",
      "Existing blueprints directory removed.\n",
      "Creating new blueprints directory...\n",
      "\n",
      "Space: Action Flows Monitoring (Working)\n",
      "  Package: Action Flows Monitor (With Dependencies)\n",
      "  Package: AF Monitoring App (Marketplace Source)\n",
      "\n",
      "Space: AF Dashboard\n",
      "  Package: AF Dashboard (Obsolete)\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: test\n",
      "Blueprint extracted. Saving to: af-dashboard.test&af-dashboard.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: AF Monitoring App - Imported\n",
      "  Package: AF Monitoring App (Obsolete)\n",
      "\n",
      "Space: Antonio Test\n",
      "  Package: App Checker Test\n",
      "  Package: App Availability Checker\n",
      "  Package: KPI Testing\n",
      "\n",
      "Space: App Availability Checker \n",
      "  Package: App Availability Checker (Marketplace Source)\n",
      "\n",
      "Space: App Reference Testing \n",
      "  Package: Universal Starter Kit\n",
      "\n",
      "Space: Applicants Challenge\n",
      "  Package: Challenge Action Flow\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: Challenge Action Flow Automation\n",
      "Blueprint extracted. Saving to: 39e4a923_539f_489c_89c5_70c76a00fc92.39e4a923_539f_489c_89c5_70c76a00fc92-automation&39e4a923_539f_489c_89c5_70c76a00fc92.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: Applied AI\n",
      "  Package: Order Management Starter Kit\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: Credit Block Removal\n",
      "Blueprint extracted. Saving to: order-management-starter-kit-store-3.credit-block-removal&order-management-starter-kit-store-3.json\n",
      "        Blueprint downloaded\n",
      "  Package: Open Order Processing\n",
      "    Found 5 Action Flow(s)\n",
      "      Processing: Credit Block Removal Agent\n",
      "Blueprint extracted. Saving to: order-management-execution-app-store.credit-block-removal-agent&order-management-execution-app-store.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Text Parser Troubleshooting\n",
      "Blueprint extracted. Saving to: order-management-execution-app-store.ai-agent-teamplate-wip-&order-management-execution-app-store.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Credit Block Removal Agent [Record Based]\n",
      "Blueprint extracted. Saving to: order-management-execution-app-store.credit-block-removal-agent-record-based-&order-management-execution-app-store.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Credit Block [Expert Validation]\n",
      "Blueprint extracted. Saving to: order-management-execution-app-store.credit-block-source-system-update-&order-management-execution-app-store.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Credit Block [Expert Rejection]\n",
      "Blueprint extracted. Saving to: order-management-execution-app-store.credit-block-expert-rejection-&order-management-execution-app-store.json\n",
      "        Blueprint downloaded\n",
      "  Package: SAP ECC - Order-to-Cash - Credit Management\n",
      "  Package: Accounts Receivable Starter Kit\n",
      "  Package: Credit Management\n",
      "    Found 8 Action Flow(s)\n",
      "      Processing: Email Test CFO\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.email-test-cfo&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: [Trigger] Agent Credit Block Recommendation\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.-trigger-agent-credit-block-recommendation&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: [TEST] Agent Credit Block Recommendation\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.-manual-v2-agent-credit-block-recommendation&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Accept AI Recommendation\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.validate-ai-recommendation&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: [Manual] Agent Credit Block Recommendation\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.credit-block-removal&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: SAP Agent [Test]\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.sap-agent-test&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Reject AI Recommendation\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.reject-ai-recommendation&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: [Kiko] Copy\n",
      "Blueprint extracted. Saving to: 7feaf805_d237_4f4b_a289_27fa28d87567.-kiko-copy&7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "  Package: Credit Management\n",
      "    Found 7 Action Flow(s)\n",
      "      Processing: SAP Agent [Test]\n",
      "Blueprint extracted. Saving to: cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.sap-agent-test&cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Reject AI Recommendation\n",
      "Blueprint extracted. Saving to: cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.reject-ai-recommendation&cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: [Trigger] Agent Credit Block Recommendation\n",
      "Blueprint extracted. Saving to: cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.-trigger-agent-credit-block-recommendation&cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Email Test CFO\n",
      "Blueprint extracted. Saving to: cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.email-test-cfo&cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Accept AI Recommendation\n",
      "Blueprint extracted. Saving to: cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.validate-ai-recommendation&cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: [TEST] Agent Credit Block Recommendation\n",
      "Blueprint extracted. Saving to: cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.-manual-v2-agent-credit-block-recommendation&cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "      Processing: [Manual] Agent Credit Block Recommendation\n",
      "Blueprint extracted. Saving to: cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.credit-block-removal&cosentino-7feaf805_d237_4f4b_a289_27fa28d87567.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: asd\n",
      "  Package: Order Management Starter Kit\n",
      "  Package: Procurement Starter Kit\n",
      "  Package: Order Management Starter Kit\n",
      "  Package: Order Management Starter Kit\n",
      "  Package: SAP ECC - Accounts Payable - Maverick Buying\n",
      "  Package: Procurement Starter Kit (object-centric)\n",
      "  Package: Order Management Starter Kit (object-centric)\n",
      "  Package: On Time Delivery\n",
      "  Package: Tax Code Checker\n",
      "  Package: Test Paula\n",
      "    Found 8 Action Flow(s)\n",
      "      Processing: Test trigger\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.test-trigger&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "      Processing: test 2\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.test-2&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "      Processing: km test naol\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.km-test-naol&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "      Processing: test LangSearch\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.test-langsearch&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "      Processing: integration with python scrapper\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.integration-with-python-scrapper&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "      Processing: open AI API\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.open-ai-api&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "      Processing: asdasda\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.asdasda&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get_kpi_action_flow\n",
      "Blueprint extracted. Saving to: ebc7349b_8ccf_4623_9c7b_68e7b7023772.asdasdasda&ebc7349b_8ccf_4623_9c7b_68e7b7023772.json\n",
      "        Blueprint downloaded\n",
      "  Package: SAP S/4HANA - Accounts Payable - Payment Behavior - Payment Behavior\n",
      "\n",
      "Space: Backup\n",
      "  Package: action-flows\n",
      "    Found 12 Action Flow(s)\n",
      "      Processing: gsheet-to-json-database-backup\n",
      "Blueprint extracted. Saving to: action-flows.gsheet-to-json-database&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: post-response-backup-script\n",
      "Blueprint extracted. Saving to: action-flows.post-response-script&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: clean-up-gsheet\n",
      "Blueprint extracted. Saving to: action-flows.clean-up-gsheet&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: gform-to-gsheet\n",
      "Blueprint extracted. Saving to: action-flows.gform-to-gsheet&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: feedback-button\n",
      "Blueprint extracted. Saving to: action-flows.feedback-button&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: gsheet-to-json-database-restore\n",
      "Blueprint extracted. Saving to: action-flows.gsheet-to-json-database-restore&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: post-response-restore-script\n",
      "Blueprint extracted. Saving to: action-flows.post-response-restore-script&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: gsheet-to-json-database-migration\n",
      "Blueprint extracted. Saving to: action-flows.gsheet-to-json-database-migration&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: post-response-migration-script\n",
      "Blueprint extracted. Saving to: action-flows.post-response-migration-script&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: feedback-bad-result\n",
      "Blueprint extracted. Saving to: action-flows.feedback-bad-result&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: gsheet-to-script\n",
      "Blueprint extracted. Saving to: action-flows.gsheet-to-script&action-flows.json\n",
      "        Blueprint downloaded\n",
      "      Processing: post-response-regular-backups\n",
      "Blueprint extracted. Saving to: action-flows.post-response-regular-backups&action-flows.json\n",
      "        Blueprint downloaded\n",
      "  Package: innovations backup\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: innovations backups\n",
      "Blueprint extracted. Saving to: innovation-backup.innovations-backups&innovation-backup.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: CKC\n",
      "  Package: Procurement Starter Kit\n",
      "\n",
      "Space: Data Lineage Dev\n",
      "  Package: Data Model Optimizer - Case Centric\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: deselect column from extraction\n",
      "Blueprint extracted. Saving to: f14befe7_f585_4dea_9647_4c605e47d8a7.update-extraction-configuration-af&f14befe7_f585_4dea_9647_4c605e47d8a7.json\n",
      "        Blueprint downloaded\n",
      "  Package: Data Lineage - Case Centric\n",
      "  Package: Data Lineage - POC version\n",
      "  Package: Data Lineage (Task Level)\n",
      "  Package: Data Lineage - with Monitoring Pool\n",
      "\n",
      "Space: delete_me_demo_gen_user_token_test_space\n",
      "  Package: delete_me_demo_gen_user_token_test_package\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: asdewrpiongferio\n",
      "Blueprint extracted. Saving to: delete_me_demo_gen_user_token_test_package.asdewrpiongferio&delete_me_demo_gen_user_token_test_package.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: Dummy Space\n",
      "  Package: Dummy Package\n",
      "    Found 7 Action Flow(s)\n",
      "      Processing: Tiger Team Challenge Invite AF\n",
      "Blueprint extracted. Saving to: 372672c5_4093_443b_a359_3522a382513b.tiger-team-challenge-invite-af&372672c5_4093_443b_a359_3522a382513b.json\n",
      "        Blueprint downloaded\n",
      "      Processing: airtable\n",
      "Blueprint extracted. Saving to: 372672c5_4093_443b_a359_3522a382513b.airtable&372672c5_4093_443b_a359_3522a382513b.json\n",
      "        Blueprint downloaded\n",
      "      Processing: gform-to-gsheet\n",
      "Blueprint extracted. Saving to: 372672c5_4093_443b_a359_3522a382513b.gform-to-gsheet&372672c5_4093_443b_a359_3522a382513b.json\n",
      "        Blueprint downloaded\n",
      "      Processing: innovations backups\n",
      "Blueprint extracted. Saving to: 372672c5_4093_443b_a359_3522a382513b.innovations-backups&372672c5_4093_443b_a359_3522a382513b.json\n",
      "        Blueprint downloaded\n",
      "      Processing: notify results to user\n",
      "Blueprint extracted. Saving to: 372672c5_4093_443b_a359_3522a382513b.notify-results-to-user&372672c5_4093_443b_a359_3522a382513b.json\n",
      "        Blueprint downloaded\n",
      "      Processing: check changes on extraction packages\n",
      "Blueprint extracted. Saving to: 372672c5_4093_443b_a359_3522a382513b.check-changes-on-extraction-packages&372672c5_4093_443b_a359_3522a382513b.json\n",
      "        Blueprint downloaded\n",
      "      Processing: test push\n",
      "Blueprint extracted. Saving to: 372672c5_4093_443b_a359_3522a382513b.test-push&372672c5_4093_443b_a359_3522a382513b.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: G-Drive Template Cleanup\n",
      "  Package: Drive Cleanup\n",
      "    Found 14 Action Flow(s)\n",
      "      Processing: delete file\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.delete-file&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: reject deletion\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.reject-deletion&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get account folders\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.get-account-folders&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get opportunity name\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.testing-folders&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: add to gsheet\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.add-to-gsheet&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: search to delete unused templates\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.delete-unused-templates&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get folders\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.get-folders&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: file cleanup\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.file-cleanup&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: from gsheet to mlwb\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.new-testing-api&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: send email to opportunity owners\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.get-opportunity-templates&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: trigger action flow\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.testing-api&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get info opportunities\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.get-opportunities-folders&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: search pre sales members oppys\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.search-pre-sales-members-oppys&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "      Processing: process g-form response\n",
      "Blueprint extracted. Saving to: drive-cleanup-tool.process-g-form-response&drive-cleanup-tool.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: hackett_cc\n",
      "  Package: test_CC\n",
      "    Found 3 Action Flow(s)\n",
      "      Processing: hackett-request-activity-counts\n",
      "Blueprint extracted. Saving to: 47c3ce20_f8aa_4be7_9834_41fbbee8678e.hackett-request-activity-counts&47c3ce20_f8aa_4be7_9834_41fbbee8678e.json\n",
      "        Blueprint downloaded\n",
      "      Processing: json-ingestions\n",
      "Blueprint extracted. Saving to: 47c3ce20_f8aa_4be7_9834_41fbbee8678e.json-ingestions&47c3ce20_f8aa_4be7_9834_41fbbee8678e.json\n",
      "        Blueprint downloaded\n",
      "      Processing: test \n",
      "Blueprint extracted. Saving to: 47c3ce20_f8aa_4be7_9834_41fbbee8678e.test-&47c3ce20_f8aa_4be7_9834_41fbbee8678e.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: ISO 56K / Innovations\n",
      "  Package: Concur Invoice\n",
      "  Package: automation\n",
      "    Found 10 Action Flow(s)\n",
      "      Processing: faulty config notification\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.faulty-config-notification&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: airtable\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.airtable&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: form to airtable\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.form-to-airtable&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: set_to_synced\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.set-to-synced&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: set_to_faulty_AF\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.set-to-faulty-af&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: drop_api_key_info_AF\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.drop-api-key-info-af&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: download-file-test\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.download-file-test&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: delete me testing logs\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.delete-me-testing-logs&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: form to airtable backup\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.form-to-airtable-backup&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "      Processing: upload packages drive\n",
      "Blueprint extracted. Saving to: iso-56k-20-20-automation.download-file-test-copy&iso-56k-20-20-automation.json\n",
      "        Blueprint downloaded\n",
      "  Package: Innovation Process\n",
      "\n",
      "Space: JAB\n",
      "  Package: Operational App Template | Enhanced Views (JAB) | Case \n",
      "  Package: Open Credit Memo App (object-centric) | Enhanced Views (JAB)\n",
      "\n",
      "Space: KM Aanalysis\n",
      "  Package: Order Management Starter Kit (object-centric)\n",
      "\n",
      "Space: KPI Display\n",
      "  Package: Procurement KPI Display\n",
      "  Package: Order Management KPI Display\n",
      "  Package: Accounts Payable KPI Display\n",
      "  Package: KPI Display (Marketplace)\n",
      "\n",
      "Space: Monitoring\n",
      "  Package: Monitoring\n",
      "\n",
      "Space: OCPM Pool\n",
      "  Package: Accounts Receivable Starter Kit (object-centric) \n",
      "  Package: Universal Starter Kit | Case \n",
      "  Package: Inventory Management Starter Kit (object-centric)\n",
      "  Package: Order Management Starter Kit (object-centric)\n",
      "  Package: Payment Term Checker (object-centric)\n",
      "  Package: Procurement Starter Kit (object-centric)\n",
      "  Package: Accounts Payable Starter Kit (object-centric)\n",
      "  Package: Open Credit Memos (object-centric)\n",
      "  Package: Blocked Invoices (object-centric)\n",
      "  Package: Shipped Not Invoiced (object-centric)\n",
      "  Package: ITSM Incident Management Starter Kit (object-centric)\n",
      "  Package: Unshipped Orders (object-centric)\n",
      "  Package: Open Order Processing (object-centric)\n",
      "  Package: Open PO Management (object-centric) | Enhanced Views\n",
      "\n",
      "Space: PC Test\n",
      "  Package: DACHSER\n",
      "\n",
      "Space: Process Sphere dev\n",
      "  Package: Michel's package\n",
      "  Package: Expense Report\n",
      "\n",
      "Space: PVJ Testing\n",
      "  Package: PVJ Testing\n",
      "\n",
      "Space: RfX\n",
      "  Package: Decoding RfX\n",
      "\n",
      "Space: Snap\n",
      "  Package: P2P Supplier Reliability\n",
      "  Package: Demo - Pizza Order Process\n",
      "  Package: Integration Graph [TEST]\n",
      "\n",
      "Space: test hackett\n",
      "  Package: ocpm_proc\n",
      "    Found 2 Action Flow(s)\n",
      "      Processing: hackett-request-activity-counts\n",
      "Blueprint extracted. Saving to: d8410302_67f3_43bb_a632_3d2d35481d90.hackett-request-activity-counts-new1&d8410302_67f3_43bb_a632_3d2d35481d90.json\n",
      "        Blueprint downloaded\n",
      "      Processing: json-ingestions\n",
      "Blueprint extracted. Saving to: d8410302_67f3_43bb_a632_3d2d35481d90.json-ingestions-new1&d8410302_67f3_43bb_a632_3d2d35481d90.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: testad\n",
      "  Package: testade\n",
      "\n",
      "Space: Testing Space\n",
      "  Package: AF Testing\n",
      "    Found 5 Action Flow(s)\n",
      "      Processing: Test AF\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-af-testing.test-af&snap-tiger-team-celonis-com-af-testing.json\n",
      "        Blueprint downloaded\n",
      "      Processing: test km module\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-af-testing.test-km-module&snap-tiger-team-celonis-com-af-testing.json\n",
      "        Blueprint downloaded\n",
      "      Processing: azure open ai test\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-af-testing.azure-open-ai-test&snap-tiger-team-celonis-com-af-testing.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Test Demo Video upload\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-af-testing.test-demo-video-upload&snap-tiger-team-celonis-com-af-testing.json\n",
      "        Blueprint downloaded\n",
      "      Processing: webhook video download\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-af-testing.webhook-video-download&snap-tiger-team-celonis-com-af-testing.json\n",
      "        Blueprint downloaded\n",
      "  Package: dummy package\n",
      "    Found 7 Action Flow(s)\n",
      "      Processing: copy\n",
      "Blueprint extracted. Saving to: dummy-package.dummy-af&dummy-package.json\n",
      "        Blueprint downloaded\n",
      "      Processing: aggregate\n",
      "Blueprint extracted. Saving to: dummy-package.aggregate&dummy-package.json\n",
      "        Blueprint downloaded\n",
      "      Processing: s3testing\n",
      "Blueprint extracted. Saving to: dummy-package.s3testing&dummy-package.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Json-ingestions\n",
      "        Error: \n",
      "            Request : GET -> https://snap-tiger-team-celonis-com.eu-1.celonis.cloud/ems-automation/\n",
      "      Processing: Hackett Request - Activity Counts\n",
      "        Error: \n",
      "            Request : GET -> https://snap-tiger-team-celonis-com.eu-1.celonis.cloud/ems-automation/\n",
      "      Processing: af\n",
      "Blueprint extracted. Saving to: dummy-package.af&dummy-package.json\n",
      "        Blueprint downloaded\n",
      "      Processing: A\n",
      "Blueprint extracted. Saving to: dummy-package.a&dummy-package.json\n",
      "        Blueprint downloaded\n",
      "  Package: Drive Cleanup\n",
      "    Found 4 Action Flow(s)\n",
      "      Processing: file cleanup\n",
      "Blueprint extracted. Saving to: drive-cleanup.file-cleanup&drive-cleanup.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get folders\n",
      "Blueprint extracted. Saving to: drive-cleanup.get-folders&drive-cleanup.json\n",
      "        Blueprint downloaded\n",
      "      Processing: test\n",
      "Blueprint extracted. Saving to: drive-cleanup.test-two&drive-cleanup.json\n",
      "        Blueprint downloaded\n",
      "      Processing: test blueprint\n",
      "Blueprint extracted. Saving to: drive-cleanup.test-blueprint&drive-cleanup.json\n",
      "        Blueprint downloaded\n",
      "  Package: test dm optimization\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: testing for views\n",
      "Blueprint extracted. Saving to: test-dm-optimization.testing-for-views&test-dm-optimization.json\n",
      "        Blueprint downloaded\n",
      "  Package: ExportPurposes\n",
      "  Package: IM Control Center\n",
      "    Found 5 Action Flow(s)\n",
      "      Processing: Json-ingestions\n",
      "        Error: \n",
      "            Request : GET -> https://snap-tiger-team-celonis-com.eu-1.celonis.cloud/ems-automation/\n",
      "      Processing: Hackett Request - Activity Counts\n",
      "        Error: \n",
      "            Request : GET -> https://snap-tiger-team-celonis-com.eu-1.celonis.cloud/ems-automation/\n",
      "      Processing: Action Flow Name\n",
      "Blueprint extracted. Saving to: im-control-center-store.action-flow-key&im-control-center-store.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Action Flow Test - Paula\n",
      "Blueprint extracted. Saving to: im-control-center-store.action-flow-key-paula&im-control-center-store.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Action Flow Test - Paula v2\n",
      "Blueprint extracted. Saving to: im-control-center-store.action-flow-key-paula-v2&im-control-center-store.json\n",
      "        Blueprint downloaded\n",
      "  Package: Order Management Pre Sales Package (object-centric)\n",
      "  Package: Prediction Builder - Classification\n",
      "  Package: Prediction Builder - Classification - New\n",
      "  Package: Order Management Starter Kit (case-centric - ECC)\n",
      "\n",
      "Space: TestSpace\n",
      "  Package: Accounts Payable Starter Kit\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: asfdasdasddsfsf\n",
      "Blueprint extracted. Saving to: accounts-payable-starter-kit-store.asfdasdasddsfsf&accounts-payable-starter-kit-store.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: Tiger Space\n",
      "  Package: iso 56k 2020\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: Google Form to Air Table\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-iso-56k-2020.google-form-to-air-table&snap-tiger-team-celonis-com-iso-56k-2020.json\n",
      "        Blueprint downloaded\n",
      "  Package: ml-repo-github-signup\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: form to invite\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-ml-repo-github-signup.form-to-invite&snap-tiger-team-celonis-com-ml-repo-github-signup.json\n",
      "        Blueprint downloaded\n",
      "  Package: Tiger Team Challenge Invite\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: Tiger Team Challenge Invite AF\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-tiger-team-challenge-invite.tiger-team-challenge-invite-af&snap-tiger-team-celonis-com-tiger-team-challenge-invite.json\n",
      "        Blueprint downloaded\n",
      "  Package: asset tracking\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: inbound tracking\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-asset-tracking.inbound-tracking&snap-tiger-team-celonis-com-asset-tracking.json\n",
      "        Blueprint downloaded\n",
      "  Package: SolEng GDrive\n",
      "    Found 4 Action Flow(s)\n",
      "      Processing: GTM Opportunity Clean Up\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-soleng-gdrive.gtm-opportunity-clean-up&snap-tiger-team-celonis-com-soleng-gdrive.json\n",
      "        Blueprint downloaded\n",
      "      Processing: GTM New Opportunity Creation\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-soleng-gdrive.gtm-new-opportunity-creation&snap-tiger-team-celonis-com-soleng-gdrive.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Automatic Permissions by Folder Owner\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-soleng-gdrive.automatic-permissions-by-folder-owner&snap-tiger-team-celonis-com-soleng-gdrive.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Testing\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-soleng-gdrive.testing&snap-tiger-team-celonis-com-soleng-gdrive.json\n",
      "        Blueprint downloaded\n",
      "  Package: Script Usage Tracking\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: script tracking target\n",
      "Blueprint extracted. Saving to: snap-tiger-team-celonis-com-script-usage-tracking.script-tracking-target&snap-tiger-team-celonis-com-script-usage-tracking.json\n",
      "        Blueprint downloaded\n",
      "  Package: APC Calculator (Draft)\n",
      "  Package: CoE TT Request\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: template to request\n",
      "Blueprint extracted. Saving to: coe-tt-request.template-to-request&coe-tt-request.json\n",
      "        Blueprint downloaded\n",
      "  Package: OCPM Config Automation\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: download config file\n",
      "Blueprint extracted. Saving to: ocpm-config-automation.download-config-file&ocpm-config-automation.json\n",
      "        Blueprint downloaded\n",
      "  Package: Impact Day :) \n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: Automated Survey OCR\n",
      "Blueprint extracted. Saving to: impact-day-.automated-survey-ocr&impact-day-.json\n",
      "        Blueprint downloaded\n",
      "  Package: OCPM pre flight check\n",
      "    Found 3 Action Flow(s)\n",
      "      Processing: trigger script\n",
      "Blueprint extracted. Saving to: ocpm-pre-flight-check.trigger-script&ocpm-pre-flight-check.json\n",
      "        Blueprint downloaded\n",
      "      Processing: send result to user\n",
      "Blueprint extracted. Saving to: ocpm-pre-flight-check.send-result-to-user&ocpm-pre-flight-check.json\n",
      "        Blueprint downloaded\n",
      "      Processing: notify failed execution\n",
      "Blueprint extracted. Saving to: ocpm-pre-flight-check.notify-failed-execution&ocpm-pre-flight-check.json\n",
      "        Blueprint downloaded\n",
      "  Package: OCPM extraction job creator\n",
      "    Found 5 Action Flow(s)\n",
      "      Processing: trigger script\n",
      "Blueprint extracted. Saving to: ocpm-extraction-job-creator.trigger-script&ocpm-extraction-job-creator.json\n",
      "        Blueprint downloaded\n",
      "      Processing: notify results to user\n",
      "Blueprint extracted. Saving to: ocpm-extraction-job-creator.notify-results-to-user&ocpm-extraction-job-creator.json\n",
      "        Blueprint downloaded\n",
      "      Processing: notify failed execution\n",
      "Blueprint extracted. Saving to: ocpm-extraction-job-creator.notify-failed-execution&ocpm-extraction-job-creator.json\n",
      "        Blueprint downloaded\n",
      "      Processing: check changes on extraction packages\n",
      "Blueprint extracted. Saving to: ocpm-extraction-job-creator.check-changes-on-extraction-packages&ocpm-extraction-job-creator.json\n",
      "        Blueprint downloaded\n",
      "      Processing: trigger script webhook\n",
      "Blueprint extracted. Saving to: ocpm-extraction-job-creator.trigger-script-webhook&ocpm-extraction-job-creator.json\n",
      "        Blueprint downloaded\n",
      "  Package: Turnkey Training Version\n",
      "  Package: non ccdm content loading\n",
      "    Found 17 Action Flow(s)\n",
      "      Processing: trigger script\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.trigger-script&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: send results to user\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.send-results-to-user&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: upload table dependencies\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.upload-table-dependencies&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: submit business app\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.submit-business-app&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: submit innovation app\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.submit-innovation-app&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: update config_apps.toml\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.update-config-apps-toml&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get latest config_apps data\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.get-latest-config-apps-data&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get perspectives names per workspace\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.get-perspectives-names-per-workspace&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: update metadata innovation apps\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.update-metadata-innovation-apps&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: webhook trigger export script\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.webhook-trigger-export-script&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: trigger script - EXT\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.trigger-script-ext&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: share access to EXT google form\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.share-access-to-ext-google-form&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get celonis catalog developers\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.get-celonis-catalog-developers&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: check developer email\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.check-developer-email&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: get pre-ocpm-data-jobs data\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.get-pre-ocpm-data-jobs-data&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: trigger script - CCDM 3.0\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.trigger-script-ccdm-3-0&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "      Processing: ccdm 3.0 slack message\n",
      "Blueprint extracted. Saving to: eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.ccdm-3-0-slack-message&eb18ba26_66ff_4bb7_b24b_2279ac73f3bb.json\n",
      "        Blueprint downloaded\n",
      "  Package: Asset Tracking\n",
      "  Package: OCPM TT Assets\n",
      "    Found 6 Action Flow(s)\n",
      "      Processing: trigger script\n",
      "Blueprint extracted. Saving to: 24282566_5ac4_4bb8_909e_fd3f7ed67c72.24282566_5ac4_4bb8_909e_fd3f7ed67c72-automation&24282566_5ac4_4bb8_909e_fd3f7ed67c72.json\n",
      "        Blueprint downloaded\n",
      "      Processing: trigger extraction generator\n",
      "Blueprint extracted. Saving to: 24282566_5ac4_4bb8_909e_fd3f7ed67c72.trigger-script-webhook&24282566_5ac4_4bb8_909e_fd3f7ed67c72.json\n",
      "        Blueprint downloaded\n",
      "      Processing: generate new api token\n",
      "Blueprint extracted. Saving to: 24282566_5ac4_4bb8_909e_fd3f7ed67c72.generate-new-api-token&24282566_5ac4_4bb8_909e_fd3f7ed67c72.json\n",
      "        Blueprint downloaded\n",
      "      Processing: trigger pre-flight checker\n",
      "Blueprint extracted. Saving to: 24282566_5ac4_4bb8_909e_fd3f7ed67c72.trigger-pre-flight-checker-af&24282566_5ac4_4bb8_909e_fd3f7ed67c72.json\n",
      "        Blueprint downloaded\n",
      "      Processing: trigger mad script\n",
      "Blueprint extracted. Saving to: 24282566_5ac4_4bb8_909e_fd3f7ed67c72.trigger-mad-script&24282566_5ac4_4bb8_909e_fd3f7ed67c72.json\n",
      "        Blueprint downloaded\n",
      "      Processing: delete last used api token\n",
      "Blueprint extracted. Saving to: 24282566_5ac4_4bb8_909e_fd3f7ed67c72.delete-api-token&24282566_5ac4_4bb8_909e_fd3f7ed67c72.json\n",
      "        Blueprint downloaded\n",
      "  Package: PVJ\n",
      "  Package: Celonis Benchmarking\n",
      "    Found 6 Action Flow(s)\n",
      "      Processing: Update VEs emails from Accounts\n",
      "Blueprint extracted. Saving to: fc1f6aad_efb6_4283_834e_1a3b1d06b691.update-ves-emails-from-accounts&fc1f6aad_efb6_4283_834e_1a3b1d06b691.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Give Access to Benchmarking\n",
      "Blueprint extracted. Saving to: fc1f6aad_efb6_4283_834e_1a3b1d06b691.give-access-to-benchmarking-sheet-copy&fc1f6aad_efb6_4283_834e_1a3b1d06b691.json\n",
      "        Blueprint downloaded\n",
      "      Processing: OPT IN Report Generator\n",
      "Blueprint extracted. Saving to: fc1f6aad_efb6_4283_834e_1a3b1d06b691.opt-in-report-generator-official&fc1f6aad_efb6_4283_834e_1a3b1d06b691.json\n",
      "        Blueprint downloaded\n",
      "      Processing: NOT OPT IN Report Generator\n",
      "Blueprint extracted. Saving to: fc1f6aad_efb6_4283_834e_1a3b1d06b691.not-opt-in-report-generator2&fc1f6aad_efb6_4283_834e_1a3b1d06b691.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Benchmarking Pre-processing Report Data\n",
      "Blueprint extracted. Saving to: fc1f6aad_efb6_4283_834e_1a3b1d06b691.benchmarking-pre-processing-data&fc1f6aad_efb6_4283_834e_1a3b1d06b691.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Benchmarking Pre-processing Processes Sheets\n",
      "Blueprint extracted. Saving to: fc1f6aad_efb6_4283_834e_1a3b1d06b691.benchmarking-pre-processing-processes-sheets&fc1f6aad_efb6_4283_834e_1a3b1d06b691.json\n",
      "        Blueprint downloaded\n",
      "  Package: Control Tower\n",
      "  Package: Hackett Distribution\n",
      "    Found 5 Action Flow(s)\n",
      "      Processing: Send Notification\n",
      "Blueprint extracted. Saving to: 0a887516_4633_4881_8fb5_1b542fd195c6.send-notification&0a887516_4633_4881_8fb5_1b542fd195c6.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Hackett Deployment\n",
      "Blueprint extracted. Saving to: 0a887516_4633_4881_8fb5_1b542fd195c6.hackett-deployment&0a887516_4633_4881_8fb5_1b542fd195c6.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Notify Failed Execution\n",
      "Blueprint extracted. Saving to: 0a887516_4633_4881_8fb5_1b542fd195c6.notify-failed-execution&0a887516_4633_4881_8fb5_1b542fd195c6.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Backup Hackett Assets\n",
      "Blueprint extracted. Saving to: 0a887516_4633_4881_8fb5_1b542fd195c6.backup-hackett-assets&0a887516_4633_4881_8fb5_1b542fd195c6.json\n",
      "        Blueprint downloaded\n",
      "      Processing: Hackett Backup Zip File\n",
      "Blueprint extracted. Saving to: 0a887516_4633_4881_8fb5_1b542fd195c6.hackett-backup-zip-file&0a887516_4633_4881_8fb5_1b542fd195c6.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "Space: woifnfdonodfv\n",
      "  Package: Universal Starter Kit\n",
      "    Found 1 Action Flow(s)\n",
      "      Processing: asdasdasdad\n",
      "Blueprint extracted. Saving to: universal-starter-kit-store.asdasdasdad&universal-starter-kit-store.json\n",
      "        Blueprint downloaded\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SUMMARY: Downloaded 156 blueprints\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "download_all_blueprints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440a3d9",
   "metadata": {},
   "source": [
    "### Extract celonis modules from blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03781e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celonis_module_positions(blueprint: str) -> list:\n",
    "    \"\"\"\n",
    "    Recursively searches through entire JSON structure to find all modules \n",
    "    that start with 'celonis:', regardless of nesting depth.\n",
    "    \n",
    "    Args:\n",
    "        blueprint: Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing module data for each Celonis module\n",
    "    \"\"\"\n",
    "    def find_celonis_modules(obj, path=\"\"):\n",
    "        \"\"\"Recursively search for celonis modules in nested structures\"\"\"\n",
    "        modules = []\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            # Check if this dict has a 'module' key starting with 'celonis:'\n",
    "            if 'module' in obj and isinstance(obj['module'], str) and obj['module'].startswith('celonis:'):\n",
    "                # Found a Celonis module - store the entire object\n",
    "                modules.append({\n",
    "                    'data': obj,\n",
    "                    'path': path,\n",
    "                    'module': obj['module'],\n",
    "                    'id': obj.get('id')\n",
    "                })\n",
    "            \n",
    "            # Recursively search all values in this dict\n",
    "            for key, value in obj.items():\n",
    "                new_path = f\"{path}.{key}\" if path else key\n",
    "                modules.extend(find_celonis_modules(value, new_path))\n",
    "        \n",
    "        elif isinstance(obj, list):\n",
    "            # Recursively search all items in this list\n",
    "            for i, item in enumerate(obj):\n",
    "                new_path = f\"{path}[{i}]\"\n",
    "                modules.extend(find_celonis_modules(item, new_path))\n",
    "        \n",
    "        return modules\n",
    "    \n",
    "    try:\n",
    "        with open(blueprint, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Start recursive search from root\n",
    "        celonis_modules = find_celonis_modules(data)\n",
    "        \n",
    "        return celonis_modules\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9abd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_celonis_module_details(blueprint: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts details from Celonis modules in an Action Flow blueprint.\n",
    "    Recursively searches through entire JSON structure.\n",
    "    \n",
    "    Args:\n",
    "        blueprint: Path to the blueprint JSON file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing extracted details for each Celonis module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get all Celonis modules (regardless of nesting)\n",
    "        celonis_modules = get_celonis_module_positions(blueprint)\n",
    "        \n",
    "        module_details = []\n",
    "        \n",
    "        for module_info in celonis_modules:\n",
    "            module_data = module_info['data']\n",
    "            module_name = module_info['module']\n",
    "            mapper = module_data.get('mapper', {})\n",
    "            metadata = module_data.get('metadata', {})  \n",
    "            \n",
    "            detail = {\n",
    "                'path': module_info['path'],\n",
    "                'module': module_name,\n",
    "                'id': module_info['id'],\n",
    "                'mapper': mapper,      \n",
    "                'metadata': metadata   \n",
    "            }\n",
    "            \n",
    "            # Rule 1: celonis:getKPIs\n",
    "            if module_name == 'celonis:getKPIs':\n",
    "                detail['kpis'] = mapper.get('kpis', [])\n",
    "                detail['knowledgeModelKey'] = mapper.get('knowledgeModelKey', '')\n",
    "            \n",
    "            # Rule 2: celonis:getRows\n",
    "            elif module_name == 'celonis:getRows':\n",
    "                km_columns = mapper.get('kmColumns', [])\n",
    "                column_names = [col.get('columnName', '') for col in km_columns]\n",
    "                detail['column_names'] = column_names\n",
    "                detail['dataOrKnowledgeModel'] = mapper.get('dataOrKnowledgeModel', '')\n",
    "                detail['knowledgeModelKey'] = mapper.get('knowledgeModelKey', '')  \n",
    "            \n",
    "            # Rule 3: celonis:updateAugmentedAttribute\n",
    "            elif module_name in ['celonis:updateAugmentedAttribute', 'celonis:updateAugmentedAttributeV2']:\n",
    "                detail['record'] = mapper.get('record', '')\n",
    "                detail['augmentedAttributeId'] = mapper.get('augmentedAttributeId', '')                \n",
    "                detail['knowledgeModelKey'] = mapper.get('knowledgeModelKey', '')\n",
    "            \n",
    "            \n",
    "            # Rule 4: celonis:queryData\n",
    "            elif module_name == 'celonis:queryData':\n",
    "                columns = mapper.get('columns', [])\n",
    "                column_details = []\n",
    "                for col in columns:\n",
    "                    column_name = col.get('columnName', '')\n",
    "                    column_formula = col.get('formula', '')\n",
    "                    column_details.append({\n",
    "                        'columnName': column_name,\n",
    "                        'formula': column_formula\n",
    "                    })\n",
    "                \n",
    "                detail['columns'] = column_details\n",
    "                detail['dataPool'] = mapper.get('dataPool', '')\n",
    "                detail['dataModel'] = mapper.get('dataModel', '')\n",
    "                \n",
    "                # Extract only filterExpression from each filter\n",
    "                filters = mapper.get('filter', [])\n",
    "                filter_expressions = [f.get('filterExpression', '') for f in filters]\n",
    "                detail['filterExpression'] = filter_expressions\n",
    "            \n",
    "            module_details.append(detail)\n",
    "        \n",
    "        return module_details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting module details: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf32971",
   "metadata": {},
   "source": [
    "### Extract pql (e.g. if you have a celonis:extractQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24b830aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_and_columns_from_pql(pql: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts table and column references from a PQL query.\n",
    "    \n",
    "    Args:\n",
    "        pql: PQL query string\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'table_column_pairs': List of tuples (table, column)\n",
    "        - 'tables_only': List of table names without column references\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pattern for \"TABLE\".\"COLUMN\"\n",
    "    table_column_pattern = r'\"([^\"]+)\"\\s*\\.\\s*\"([^\"]+)\"'\n",
    "    \n",
    "    # Find all table.column pairs\n",
    "    table_column_matches = re.findall(table_column_pattern, pql)\n",
    "    \n",
    "    # Pattern for standalone \"TABLE\" \n",
    "    table_only_pattern = r'(?<!\\.\\s)(?<!\\.)\"([A-Z_][A-Z0-9_]*)\"(?!\\s*\\.\\s*\")'\n",
    "    \n",
    "    # Find all standalone tables\n",
    "    table_only_matches = re.findall(table_only_pattern, pql)\n",
    "    \n",
    "    # Remove tables that are already in table.column pairs\n",
    "    tables_in_pairs = {table for table, _ in table_column_matches}\n",
    "    # Also remove columns that appear in table.column pairs\n",
    "    columns_in_pairs = {column for _, column in table_column_matches}\n",
    "    \n",
    "    tables_only = [table for table in table_only_matches \n",
    "                   if table not in tables_in_pairs and table not in columns_in_pairs]\n",
    "    \n",
    "    return {\n",
    "        'table_column_pairs': list(set(table_column_matches)),  # Remove duplicates\n",
    "        'tables_only': list(set(tables_only))  # Remove duplicates\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1299c",
   "metadata": {},
   "source": [
    "### Execution Block for the Action Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ea6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 Knowledge Models in lookup.\n",
      "Processing 156 blueprint files...\n",
      "\n",
      "Extracted 574 lineage records from Action Flows\n",
      "Saved to action_flow_lineage.csv (574 records)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. PRE-FETCH: Build lookup dictionary\n",
    "# =============================================================================\n",
    "km_lookup = {item['knowledge_model_key']: item for item in all_metadata}\n",
    "print(f\"Found {len(km_lookup)} Knowledge Models in lookup.\")\n",
    "\n",
    "# List to store all lineage relationships\n",
    "af_lineage = []\n",
    "\n",
    "# Get all blueprint JSON files\n",
    "blueprint_files = glob.glob('blueprints/*.json')\n",
    "print(f\"Processing {len(blueprint_files)} blueprint files...\")\n",
    "\n",
    "for blueprint_path in blueprint_files:\n",
    "    try:\n",
    "        # --- 1. FILENAME PARSING ---\n",
    "        filename = os.path.basename(blueprint_path)\n",
    "        clean_filename = filename.replace('.json', '')\n",
    "        \n",
    "        # Regex to find a UUID (Action Flow ID)\n",
    "        uuid_match = re.search(r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}', clean_filename)\n",
    "        af_id = uuid_match.group(0) if uuid_match else clean_filename\n",
    "\n",
    "        # --- 2. GET NAME FROM JSON ---\n",
    "        try:\n",
    "            with open(blueprint_path, 'r') as f:\n",
    "                blueprint_data = json.load(f)\n",
    "                af_name = blueprint_data.get('name', 'Unknown_Action_Flow')\n",
    "        except Exception:\n",
    "            af_name = 'Unknown_Action_Flow'\n",
    "            blueprint_data = {}\n",
    "\n",
    "        # Call your existing function\n",
    "        module_details = extract_celonis_module_details(blueprint_path)\n",
    "        \n",
    "        if not module_details:\n",
    "            continue\n",
    "        \n",
    "        relationships = []\n",
    "        \n",
    "        for detail in module_details:\n",
    "            module_name = detail['module']\n",
    "            module_number = detail.get('id', 'unknown') \n",
    "            \n",
    "            # --- CLEAN MODULE TYPE NAME ---\n",
    "            # Converts \"celonis:getRows\" -> \"getRows\"\n",
    "            clean_type = module_name.split(':')[-1]\n",
    "            \n",
    "            # Rule 1: celonis:getKPIs\n",
    "            if module_name == 'celonis:getKPIs':\n",
    "                km_key = detail.get('knowledgeModelKey', '')\n",
    "                if km_key.startswith('KNOWLEDGE_MODEL-'):\n",
    "                    km_key = km_key.replace('KNOWLEDGE_MODEL-', '', 1)\n",
    "                    \n",
    "                for kpi in detail.get('kpis', []):\n",
    "                    relationships.append({\n",
    "                        'source_id': f'{kpi}'.lower(),\n",
    "                        'source_name': kpi,\n",
    "                        'source_attr': None,\n",
    "                        'source_type': 'KPI',\n",
    "                        'source_node_type': '',\n",
    "                        'source_asset_id': km_key,\n",
    "                        'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                        'target_module_id': module_number,\n",
    "                        'target_module_type': clean_type, # Pass clean type\n",
    "                        'pql': '',\n",
    "                        'km_key': km_key,\n",
    "                        'dm_id': '',\n",
    "                        'dp_id': ''\n",
    "                    })\n",
    "            \n",
    "            # Rule 2: celonis:getRows\n",
    "            elif module_name == 'celonis:getRows':\n",
    "                km_key = detail.get('knowledgeModelKey', '') or detail.get('dataOrKnowledgeModel', '')\n",
    "                if km_key.startswith('KNOWLEDGE_MODEL-'):\n",
    "                    km_key = km_key.replace('KNOWLEDGE_MODEL-', '', 1)\n",
    "\n",
    "                for column in detail.get('column_names', []):\n",
    "                    if '.' in column:\n",
    "                        parts = column.split('.', 1)\n",
    "                        source_name = parts[0]\n",
    "                        source_attr = parts[1] if len(parts) > 1 else None\n",
    "                    else:\n",
    "                        source_name = column\n",
    "                        source_attr = None\n",
    "                    \n",
    "                    relationships.append({\n",
    "                        'source_id': f'{column}'.lower(),\n",
    "                        'source_name': source_name.lower(),\n",
    "                        'source_attr': source_attr.lower() if source_attr else None,\n",
    "                        'source_type': 'KM_COLUMN',\n",
    "                        'source_node_type': 'ATTRIBUTES',\n",
    "                        'source_asset_id': km_key,\n",
    "                        'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                        'target_module_id': module_number,\n",
    "                        'target_module_type': clean_type,\n",
    "                        'pql': '',\n",
    "                        'km_key': km_key,\n",
    "                        'dm_id': '',\n",
    "                        'dp_id': ''\n",
    "                    })\n",
    "            \n",
    "            # Rule 3: celonis:updateAugmentedAttribute (V1 & V2)\n",
    "            elif 'updateAugmentedAttribute' in module_name:\n",
    "                mapper = detail.get('mapper', {})\n",
    "                metadata = detail.get('metadata', {})\n",
    "                \n",
    "                km_key = mapper.get('knowledgeModelKey', '')\n",
    "                aug_attr_id_full = mapper.get('augmentedAttributeId', '')  \n",
    "                \n",
    "                # --- EXTRACT LABEL FROM METADATA ---\n",
    "                aug_attr_label = ''\n",
    "                try:\n",
    "                    restore_data = metadata.get('restore', {})\n",
    "                    aug_attr_metadata = restore_data.get('expect', {}).get('augmentedAttributeId', {})\n",
    "                    aug_attr_label = aug_attr_metadata.get('label', '')  # \"Aug Atts Test\"\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # --- PARSE THE AUGMENTED ATTRIBUTE ID ---\n",
    "                record = ''\n",
    "                attr_name = ''\n",
    "                \n",
    "                if aug_attr_id_full and '.' in aug_attr_id_full:\n",
    "                    parts = aug_attr_id_full.split('.')\n",
    "                    if len(parts) >= 3:\n",
    "                        record = parts[1].lower()  # \"vbak\"\n",
    "                \n",
    "                # --- BUILD THE ATTRIBUTE NAME ---\n",
    "                if aug_attr_label:\n",
    "                    # Convert \"Aug Atts Test\" -> \"aug_atts_test\"\n",
    "                    attr_name_base = aug_attr_label.lower().replace(' ', '_')\n",
    "                    # Add \"aug_\" prefix -> \"aug_aug_atts_test\"\n",
    "                    attr_name = f\"aug_{attr_name_base}\"\n",
    "                \n",
    "                # --- BUILD THE FINAL SOURCE ID ---\n",
    "                # Format: \"record_attr_name.value\" (with .value suffix!)\n",
    "                # Example: \"vbak_aug_aug_atts_test.value\"\n",
    "                if record and attr_name:\n",
    "                    source_id = f\"{record}_{attr_name}.value\"  \n",
    "                    full_aug_attr = f\"{record}_{attr_name}\"    # Keep without .value for UNIQUE_SOURCE_ID\n",
    "                else:\n",
    "                    # Fallback to the full ID if parsing fails\n",
    "                    source_id = aug_attr_id_full.lower()\n",
    "                    full_aug_attr = aug_attr_id_full.lower()\n",
    "\n",
    "                if km_key.startswith('KNOWLEDGE_MODEL-'):\n",
    "                    km_key = km_key.replace('KNOWLEDGE_MODEL-', '', 1)\n",
    "                if '.' in km_key:\n",
    "                    km_key = km_key.split('.')[-1]\n",
    "\n",
    "                # 1. READ Flow: KM Attribute -> Action Flow\n",
    "                relationships.append({\n",
    "                    'source_id': source_id,           \n",
    "                    'source_name': f\"{record}_{attr_name}\",           \n",
    "                    'source_attr': \"value\",  \n",
    "                    'source_type': 'AUGMENTED_ATTRIBUTE',\n",
    "                    'source_node_type': 'ATTRIBUTES',\n",
    "                    'source_asset_id': km_key,\n",
    "                    'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                    'target_module_id': module_number,\n",
    "                    'target_module_type': clean_type,\n",
    "                    'pql': '',\n",
    "                    'km_key': km_key,\n",
    "                    'dm_id': '',\n",
    "                    'dp_id': '',\n",
    "                    'reverse': False\n",
    "                })\n",
    "\n",
    "                # 2. WRITE Flow: Action Flow -> KM Attribute\n",
    "                # FIX: Keep 'target_module_id' as the Module Number so ID generation works\n",
    "                relationships.append({\n",
    "                    'source_id': source_id,                # Same Attribute ID\n",
    "                    'source_name': f\"{record}_{attr_name}\",# Same Attribute Name\n",
    "                    'source_attr': \"value\",                # Same Attribute Value\n",
    "                    'source_type': 'AUGMENTED_ATTRIBUTE',\n",
    "                    'source_node_type': 'ATTRIBUTES',\n",
    "                    'source_asset_id': km_key,\n",
    "                    'source_asset_type': 'KNOWLEDGE_MODEL',\n",
    "                    'target_module_id': module_number,     # <--- KEEP THIS AS AF MODULE ID\n",
    "                    'target_module_type': clean_type,\n",
    "                    'pql': '',\n",
    "                    'km_key': km_key,\n",
    "                    'dm_id': '',\n",
    "                    'dp_id': '',\n",
    "                    'reverse': True\n",
    "                })\n",
    "            \n",
    "            \n",
    "            # Rule 4: celonis:queryData\n",
    "            elif module_name == 'celonis:queryData':\n",
    "                dp_id = detail.get('dataPool', '')\n",
    "                dm_id = detail.get('dataModel', '')\n",
    "                \n",
    "                def add_dm_rel(src_id, src_name, src_attr, node_type, pql_str):\n",
    "                    relationships.append({\n",
    "                        'source_id': src_id.lower(),\n",
    "                        'source_name': src_name.lower(),\n",
    "                        'source_attr': src_attr.lower() if src_attr else None,\n",
    "                        'source_type': 'DATA_MODEL',\n",
    "                        'source_node_type': node_type,\n",
    "                        'source_asset_id': dm_id,\n",
    "                        'source_asset_type': 'DATA_MODEL',\n",
    "                        'target_module_id': module_number,\n",
    "                        'target_module_type': clean_type,\n",
    "                        'pql': pql_str,\n",
    "                        'km_key': '',\n",
    "                        'dm_id': dm_id,\n",
    "                        'dp_id': dp_id\n",
    "                    })\n",
    "\n",
    "                for col in detail.get('columns', []):\n",
    "                    formula = col.get('formula', '')\n",
    "                    if formula:\n",
    "                        pql_refs = extract_tables_and_columns_from_pql(formula)\n",
    "                        for table, column in pql_refs['table_column_pairs']:\n",
    "                            add_dm_rel(f'{table}.{column}', table, column, 'TABLE_COLUMN', formula)\n",
    "                        for table in pql_refs['tables_only']:\n",
    "                            add_dm_rel(f'{table}', table, None, 'TABLE', formula)\n",
    "                \n",
    "                for filter_expr in detail.get('filterExpression', []):\n",
    "                    if filter_expr:\n",
    "                        pql_refs = extract_tables_and_columns_from_pql(filter_expr)\n",
    "                        for table, column in pql_refs['table_column_pairs']:\n",
    "                            add_dm_rel(f'{table}.{column}', table, column, 'TABLE_COLUMN', filter_expr)\n",
    "                        for table in pql_refs['tables_only']:\n",
    "                            add_dm_rel(f'{table}', table, None, 'TABLE', filter_expr)\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 3. CONVERT TO FINAL FORMAT\n",
    "        # -------------------------------------------------------------\n",
    "        for rel in relationships:\n",
    "            is_reversed = rel.get('reverse', False)\n",
    "            km_key = rel.get('km_key', '')\n",
    "            \n",
    "            meta = km_lookup.get(km_key, {})\n",
    "            km_id = meta.get('knowledge_model_id', '') if km_key else ''\n",
    "            dm_id = rel['dm_id'] if rel['dm_id'] else meta.get('data_model_id', '')\n",
    "            dp_id = rel['dp_id'] if rel['dp_id'] else meta.get('data_pool_id', '')\n",
    "            \n",
    "            module_id = rel['target_module_id']\n",
    "            module_type = rel['target_module_type']\n",
    "\n",
    "            # --- ID CONSTRUCTION (Updated with Source Info) ---\n",
    "            # Format: ActionFlowID.ActionFlowName.ModuleID.SourceName\n",
    "            source_part = rel['source_name'].replace(' ', '_') if rel['source_name'] else 'unknown'\n",
    "            af_unique_node_id = f\"{af_id}.{af_name}.{module_id}\".lower()\n",
    "            \n",
    "            # --- TARGET NAME CONSTRUCTION ---\n",
    "            # Format: \"Module 1 (getRows)\"\n",
    "            target_node_name = f\"Module {module_id} ({module_type})\"\n",
    "\n",
    "            if is_reversed:\n",
    "                # AF -> KM (Augmented Attribute Update)\n",
    "                # SOURCE is now the Action Flow (calculated correctly above)\n",
    "                unique_source_id = af_unique_node_id\n",
    "                \n",
    "                # TARGET is now the KM Attribute (using the stored source info)\n",
    "                unique_target_id = f'{km_id}.{rel[\"source_id\"]}'.lower() if km_id else f'{km_key}.{rel[\"source_id\"]}'.lower()\n",
    "                \n",
    "                af_lineage.append({\n",
    "                    'UNIQUE_SOURCE_ID': unique_source_id,\n",
    "                    'UNIQUE_TARGET_ID': unique_target_id,\n",
    "                    'SOURCE_ID': af_id,\n",
    "                    'SOURCE_NAME': f\"{af_name}\",\n",
    "                    'SOURCE_ATTRIBUTE': target_node_name,\n",
    "                    'SOURCE_NODE_TYPE': 'ACTION_FLOW',\n",
    "                    'SOURCE_TYPE': 'ACTION_FLOW',\n",
    "                    'SOURCE_PQL': rel['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': af_id,\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': 'ACTION_FLOW',\n",
    "                    \n",
    "                    # FIX: Map TARGET columns to the original Source details\n",
    "                    'TARGET_ID': rel['source_id'],      # Attribute ID\n",
    "                    'TARGET_NAME': rel['source_name'],  # Attribute Name\n",
    "                    'TARGET_ATTRIBUTE': rel['source_attr'], # \"value\"\n",
    "                    \n",
    "                    'TARGET_NODE_TYPE': 'AUGMENTED_ATTRIBUTE',\n",
    "                    'TARGET_STUDIO_ASSET_ID': km_id or km_key,\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'KNOWLEDGE_MODEL',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': km_key,\n",
    "                    'KNOWLEDGE_MODEL_ID': km_id,\n",
    "                    'DATA_MODEL_ID': dm_id,\n",
    "                    'DATA_POOL_ID': dp_id\n",
    "                })\n",
    "            else:\n",
    "                # KM -> AF\n",
    "                if km_id:\n",
    "                    unique_source_prefix = km_id\n",
    "                elif dm_id:\n",
    "                    unique_source_prefix = dm_id\n",
    "                elif km_key:\n",
    "                    unique_source_prefix = km_key\n",
    "                else:\n",
    "                    unique_source_prefix = rel['source_asset_id']\n",
    "                \n",
    "                unique_source_id = f'{unique_source_prefix}.{rel[\"source_id\"]}'.lower() if unique_source_prefix else rel['source_id'].lower()\n",
    "                unique_target_id = af_unique_node_id\n",
    "                \n",
    "                af_lineage.append({\n",
    "                    'UNIQUE_SOURCE_ID': unique_source_id,\n",
    "                    'UNIQUE_TARGET_ID': unique_target_id,\n",
    "                    'SOURCE_ID': rel['source_id'],\n",
    "                    'SOURCE_NAME': rel['source_name'],\n",
    "                    'SOURCE_ATTRIBUTE': rel['source_attr'],\n",
    "                    'SOURCE_NODE_TYPE': rel['source_node_type'],\n",
    "                    'SOURCE_TYPE': rel['source_type'],\n",
    "                    'SOURCE_PQL': rel['pql'],\n",
    "                    'SOURCE_STUDIO_ASSET_ID': km_id or rel['source_asset_id'],\n",
    "                    'SOURCE_STUDIO_ASSET_TYPE': rel['source_asset_type'],\n",
    "                    'TARGET_ID': str(module_id),\n",
    "                    'TARGET_NAME': f'{af_name} ({module_id})', \n",
    "                    'TARGET_ATTRIBUTE': target_node_name,\n",
    "                    'TARGET_NODE_TYPE': 'ACTION_FLOW',\n",
    "                    'TARGET_STUDIO_ASSET_ID': af_id,\n",
    "                    'TARGET_STUDIO_ASSET_TYPE': 'ACTION_FLOW',\n",
    "                    'DATA_SOURCE_ID': '',\n",
    "                    'KNOWLEDGE_MODEL_KEY': km_key,\n",
    "                    'KNOWLEDGE_MODEL_ID': km_id,\n",
    "                    'DATA_MODEL_ID': dm_id,\n",
    "                    'DATA_POOL_ID': dp_id\n",
    "                })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nExtracted {len(af_lineage)} lineage records from Action Flows\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_af_lineage = pd.DataFrame(af_lineage)\n",
    "\n",
    "if not df_af_lineage.empty:\n",
    "    df_af_lineage['UNIQUE_KEY'] = (\n",
    "        df_af_lineage['DATA_MODEL_ID'].fillna('') + '.' + \n",
    "        df_af_lineage['SOURCE_NAME'].fillna('') + '.' + \n",
    "        df_af_lineage['SOURCE_ATTRIBUTE'].fillna('')\n",
    "    ).str.lower()\n",
    "\n",
    "df_af_lineage.to_csv('action_flow_lineage.csv', index=False)\n",
    "print(f\"Saved to action_flow_lineage.csv ({len(df_af_lineage)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c062dd3",
   "metadata": {},
   "source": [
    "## Append KM + Views + Action Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed3a2b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lineage_studio.csv\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([df_studio, df_af_lineage], ignore_index=True)\n",
    "\n",
    "# 3. Save the complete lineage table\n",
    "df_combined.to_csv('lineage_studio.csv', index=False)\n",
    "\n",
    "print(\"Saved lineage_studio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ead8b",
   "metadata": {},
   "source": [
    "## JOIN Data Models data with Studio Lineage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b3f0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_models rows (Unique Inventory): 2353\n",
      "df_studio_unique rows (Unique Inventory): 9\n",
      "Table_usage_report rows: 2353\n",
      "\n",
      " Final complete lineage report saved to 'tables_usage_report.csv'\n"
     ]
    }
   ],
   "source": [
    "## JOIN Data Models data with Studio Lineage Data\n",
    "\n",
    "# --- 1. DEDUPLICATE INVENTORY (Essential Primary Key Fix) ---\n",
    "# Ensure the left table has a unique key (it should be 1:1)\n",
    "df_data_models = df_data_models.drop_duplicates(subset=['unique_id'], keep='first')\n",
    "print(f\"df_data_models rows (Unique Inventory): {len(df_data_models)}\")\n",
    "\n",
    "\n",
    "# --- 2. PREPARE STUDIO USAGE (Group usage data before merge) ---\n",
    "df_studio['JOIN_KEY'] = (\n",
    "    df_studio['DATA_MODEL_ID'] + '.' + \n",
    "    df_studio['SOURCE_NAME'] + '.' + \n",
    "    df_studio['SOURCE_ATTRIBUTE']\n",
    ").str.lower()\n",
    "\n",
    "# Create a clean list of ONLY the unique keys that were used (deduplicating multiple uses)\n",
    "df_studio_unique_usage = df_studio.drop_duplicates(subset=['JOIN_KEY'])\n",
    "print(f\"df_studio_unique rows (Unique Inventory): {len(df_studio_unique_usage)}\")\n",
    "\n",
    "# --- 3. PERFORM LEFT JOIN (Inventory size guaranteed) ---\n",
    "# Join the unique inventory (left) with the unique list of used keys (right)\n",
    "df_final = pd.merge(\n",
    "    df_data_models,\n",
    "    df_studio_unique_usage[['JOIN_KEY']], # Only need the key from the right side\n",
    "    left_on='unique_id',\n",
    "    right_on='JOIN_KEY',\n",
    "    how='left',  # This guarantees len(df_final) == len(df_data_models)\n",
    "    indicator=True,\n",
    "    suffixes=('_dm', '_studio')  # Add suffixes\n",
    ")\n",
    "\n",
    "# --- 4. CREATE USED/NOT_USED FLAG ---\n",
    "df_final['USED_NOT_USED'] = df_final['_merge'].map({\n",
    "    'both': 'USED',              # Column exists in inventory AND was found in studio usage\n",
    "    'left_only': 'NOT_USED',     # Column exists in inventory but was NOT found in studio usage\n",
    "    # 'right_only' is impossible with a Left Join on unique keys\n",
    "})\n",
    "\n",
    "# Drop the merge indicator and JOIN_KEY columns (keep unique_id)\n",
    "df_final = df_final.drop(columns=['_merge', 'JOIN_KEY'])\n",
    "\n",
    "\n",
    "# --- FINAL COLUMN SELECTION AND CLEANUP ---\n",
    "\n",
    "# Define the columns we need, using the '_dm' suffix where necessary \n",
    "columns_to_keep = [\n",
    "    'd_pool_id', \n",
    "    'd_pool_name', \n",
    "    'd_model_id', \n",
    "    'd_model_name', \n",
    "    'table_name', \n",
    "    'column_name', \n",
    "    'USED_NOT_USED'\n",
    "]\n",
    "\n",
    "# Select and rename columns for the final report\n",
    "final_output_cols = []\n",
    "for col in columns_to_keep:\n",
    "    if col + '_dm' in df_final.columns:\n",
    "         final_output_cols.append(col + '_dm')\n",
    "    else:\n",
    "         final_output_cols.append(col)\n",
    "\n",
    "# Reorder columns and select only the required ones\n",
    "df_final = df_final[final_output_cols]\n",
    "\n",
    "# Save the final result\n",
    "df_final.to_csv('tables_usage_report.csv', index=False)\n",
    "print(f\"Table_usage_report rows: {len(df_final)}\")\n",
    "print(\"\\n Final complete lineage report saved to 'tables_usage_report.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cbf936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['NODE_TYPE_COMBINED'] = (\n",
    "    df_combined['SOURCE_NODE_TYPE'].astype(str) + '-' + \n",
    "    df_combined['SOURCE_TYPE'].astype(str)\n",
    ")\n",
    "\n",
    "# Create complete source nodes list with all columns from studio\n",
    "df_source_nodes = df_combined[[\n",
    "    'UNIQUE_SOURCE_ID', \n",
    "    'SOURCE_ID', \n",
    "    'NODE_TYPE_COMBINED',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE',\n",
    "    'SOURCE_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME',\n",
    "    'KNOWLEDGE_MODEL_ID',\n",
    "    'KNOWLEDGE_MODEL_KEY'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_SOURCE_ID': 'node',\n",
    "    'SOURCE_ID': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'KNOWLEDGE_MODEL_ID': 'knowledge_model_id',\n",
    "    'KNOWLEDGE_MODEL_KEY': 'knowledge_model_key',\n",
    "    'NODE_TYPE_COMBINED': 'category',\n",
    "    'SOURCE_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'SOURCE_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Create complete target nodes list\n",
    "df_target_nodes = df_combined[[\n",
    "    'UNIQUE_TARGET_ID', \n",
    "    'TARGET_NAME',\n",
    "    'TARGET_NODE_TYPE', \n",
    "    'TARGET_STUDIO_ASSET_TYPE',\n",
    "    'TARGET_STUDIO_ASSET_ID',\n",
    "    'DATA_MODEL_ID',\n",
    "    'DATA_MODEL_NAME',\n",
    "    'DATA_POOL_ID',\n",
    "    'DATA_POOL_NAME',\n",
    "    'KNOWLEDGE_MODEL_ID',\n",
    "    'KNOWLEDGE_MODEL_KEY'\n",
    "]].rename(columns={\n",
    "    'UNIQUE_TARGET_ID': 'node',\n",
    "    'TARGET_NAME': 'node_name',\n",
    "    'DATA_POOL_ID': 'data_pool_id',\n",
    "    'DATA_POOL_NAME': 'data_pool_name',\n",
    "    'DATA_MODEL_ID': 'data_model_id',\n",
    "    'DATA_MODEL_NAME': 'data_model_name',\n",
    "    'KNOWLEDGE_MODEL_ID': 'knowledge_model_id',\n",
    "    'KNOWLEDGE_MODEL_KEY': 'knowledge_model_key',\n",
    "    'TARGET_NODE_TYPE': 'category',\n",
    "    'TARGET_STUDIO_ASSET_TYPE': 'asset_type',\n",
    "    'TARGET_STUDIO_ASSET_ID': 'asset_id',\n",
    "})\n",
    "\n",
    "# Combine ALL nodes from lineage\n",
    "df_mapping_nodes = pd.concat([df_source_nodes, df_target_nodes]).drop_duplicates(subset=['node']).reset_index(drop=True)\n",
    "\n",
    "# Save and show stats\n",
    "df_mapping_nodes.to_csv('mapping_nodes_studio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30f513",
   "metadata": {},
   "source": [
    "## Creating Table in Celonis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "108573da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "\tDataPool(id='f9b4965c-cfd2-4557-a38a-bd8b40e824b5', name='delete me - accounts payable extension test'),\n",
       "\tDataPool(id='2b8d2a7e-e9af-4f95-aa8c-921812c97c00', name='test_lineage'),\n",
       "\tDataPool(id='bab3cce6-2ad7-4b54-9938-e13e4f66b264', name='XML Test'),\n",
       "\tDataPool(id='34d1a636-42fa-4d33-ad7d-72b5a39711d7', name='delete me - tariff app (1)'),\n",
       "\tDataPool(id='5eb730ac-269a-41d4-93ec-ce5a420eb3d7', name='delete me - target bug'),\n",
       "\tDataPool(id='a68ab44c-41bd-415a-bbd5-7362e1a11a73', name='OCPM Pre-Flight Checker'),\n",
       "\tDataPool(id='524f9bcd-6dd5-441c-88d1-684bbb09861f', name='delete me - tariff app (1) (1)'),\n",
       "\tDataPool(id='30382fae-aa6c-4870-9f46-9206878b5731', name='Monitoring Pool'),\n",
       "\tDataPool(id='355b12f1-3e1a-495e-bb5d-67d213333fbc', name='Test Pool'),\n",
       "\tDataPool(id='2878e4cc-3e6a-4bab-a996-2f184001ec5d', name='OCPM Extractions SAP ECC Accounts Payable and Procurement'),\n",
       "\tDataPool(id='68c6aab9-c2c8-417e-b708-85e0a6677c58', name='Prediction Builder'),\n",
       "\tDataPool(id='1f220dca-a95e-4b88-9b83-232cc12c79be', name='SAP ECC - Accounts Receivable (1)'),\n",
       "\tDataPool(id='21e2c92e-9cb4-4513-9b6e-96e55cc2eba3', name='Sailfin test'),\n",
       "\tDataPool(id='38379fbb-c309-4cd4-ba3e-58b9a363f841', name='delete me - test mad deployment'),\n",
       "\tDataPool(id='cd711dc9-ac79-4999-b9dd-e040edbfa4ea', name='SAP ECC - Purchase to Pay (5)'),\n",
       "\tDataPool(id='19a99289-2e90-4b35-833a-9cbb1c13ff92', name='Asset Tracking'),\n",
       "\tDataPool(id='03431f3b-fadf-4a3c-85ad-fd2cdc92cecb', name='DACHSER Optimizer'),\n",
       "\tDataPool(id='9d73779d-43da-4c00-9622-87c4ff9c8baf', name='LZ'),\n",
       "\tDataPool(id='268cd39f-91b2-46db-a115-a234e4fdb17b', name='EMS Loader '),\n",
       "\tDataPool(id='dae5c830-20e6-4a11-89d1-648f47924ca4', name='OCPM Extractions SAP ECC Accounts Payable'),\n",
       "\tDataPool(id='66b2a6a6-90be-4cdc-a7bb-bca071fa09ca', name='asdasdad'),\n",
       "\tDataPool(id='74331939-baab-4df4-ac1c-b04c54dee496', name='OCPM Extractions (sap-ecc) (3)'),\n",
       "\tDataPool(id='2f569809-1ba9-4d89-8c32-7ee6f8650348', name='Orbit Day Demo'),\n",
       "\tDataPool(id='4bc8e9cf-493f-42da-9e35-46ca4c55adac', name='OCPM Extractions (sap-ecc) (5)'),\n",
       "\tDataPool(id='8bd669f0-3b53-4444-8b32-6e327e961de5', name='OCPM Extractions SAP ECC Order Management'),\n",
       "\tDataPool(id='c8bb977c-6cae-422c-8d7d-cf7eb8c2b26c', name='delete me - test L2O'),\n",
       "\tDataPool(id='7a2fc7cc-c257-4d9f-9bbc-fca4736f98aa', name='Module for Advanced Deployments - DEMO'),\n",
       "\tDataPool(id='0379bdda-7a46-4829-a74b-cd98bac5e2fd', name='josc ocpm dp'),\n",
       "\tDataPool(id='ae1a25df-b504-4fe8-be3e-94d42490b368', name='SAP ECC - Purchase to Pay (4)'),\n",
       "\tDataPool(id='a3120dff-342f-4fc6-966c-68b7790fc169', name='App Availability Checker Local'),\n",
       "\tDataPool(id='b74217a0-2c64-4343-94cd-9f7177e0ca20', name='Test Pool JS'),\n",
       "\tDataPool(id='659ee9b2-359f-45f0-95ce-6dcd01d38735', name='OCPM Extractions (oracle-ebs_all)'),\n",
       "\tDataPool(id='edb1c695-4d2c-4e4a-9b4c-c3b501e0509c', name='OCPM Extractions (sap-ecc_all)'),\n",
       "\tDataPool(id='93e4a8af-737d-4ff3-9bb7-70d6c270ab99', name='OCPM Extractions SAP ECC Procurement'),\n",
       "\tDataPool(id='0797768b-612a-4e0c-84d4-16567944c427', name='OCPM Extractions SAP ECC Inventory Management'),\n",
       "\tDataPool(id='d3afec86-3414-449e-b749-8b55b6115803', name='OCPM Extractions SAP ECC Accounts Receivable'),\n",
       "\tDataPool(id='088ba6a7-fe26-4ec1-a986-0506923565a5', name='Data Model Optimizer'),\n",
       "\tDataPool(id='a885626d-234f-4d52-865d-8ea2184c5707', name='MAD Extraction Jobs'),\n",
       "\tDataPool(id='5d1110d1-2d8f-43b8-9889-9a1b2ef36973', name='pushtest'),\n",
       "\tDataPool(id='5f5726a9-c7ad-450d-9bf8-cf1430fa6f52', name='System Transformation Readiness App Connector - DO NOT USE WITHOUT BA SUPPORT'),\n",
       "\tDataPool(id='4c5637fc-a46e-4bde-99dc-824d70f157d9', name='Concur Testing'),\n",
       "\tDataPool(id='036e813d-a2ff-47ec-966d-29db319818c9', name='OCPM Extractions ORACLE EBS Procurement'),\n",
       "\tDataPool(id='6fa0c809-c319-46f2-b4d8-08e061384c13', name='Oracle Cloud - Accounts Payable'),\n",
       "\tDataPool(id='947d250f-3e5b-4447-b80c-b7c5a6e91434', name='Oracle Cloud P2P'),\n",
       "\tDataPool(id='d9b7394a-3cc1-4345-815b-09fd1b664081', name='Action Flow Monitoring (working)'),\n",
       "\tDataPool(id='0725dc2c-5dde-42a6-8a31-bb903f7c4a12', name='OCPM Extractions ORACLE EBS Accounts Payable'),\n",
       "\tDataPool(id='7755b18c-4278-4764-8f84-85bd2da855c8', name='OCPM Extractions ORACLE EBS Accounts Receivable'),\n",
       "\tDataPool(id='e11f1f12-6a9e-4d1b-9221-e411ae003504', name='OCPM Extractions ORACLE EBS Order Management'),\n",
       "\tDataPool(id='b8b4c436-ed4e-4248-845f-0e3d415c8a0c', name='OCPM Extractions ORACLE EBS Inventory Management'),\n",
       "\tDataPool(id='03b6ac69-4c1f-4619-83b9-31397cca23d8', name='delete me - testing mad extensions'),\n",
       "\tDataPool(id='4b5855c3-2e13-4639-827e-42034221be51', name='Source OCPM'),\n",
       "\tDataPool(id='f3d33db4-5d2f-447a-b4c5-b923a338c215', name='delete me empty 1'),\n",
       "\tDataPool(id='9ee41b70-3484-4858-b444-59464a514cbb', name='ReadSoft - Accounts Payable'),\n",
       "\tDataPool(id='019e96d4-4c1b-46b3-99b8-d8ddc5b83f6a', name='SAP ECC - Production Planning'),\n",
       "\tDataPool(id='4d564593-4ce6-4ef2-a2c8-a2d9c8f94c97', name='Accounts Payable'),\n",
       "\tDataPool(id='51dbaf38-e198-4adc-90c8-9889bdc54efd', name='delete me - multiple data connections script test'),\n",
       "\tDataPool(id='efb847aa-0aef-4b65-a20d-1062950d3aa8', name='SAP ECC - Accounts Payable (1)'),\n",
       "\tDataPool(id='7040cd02-e75a-45e4-9d76-b002d9ddaea7', name='KM Replacer test'),\n",
       "\tDataPool(id='03b164d6-2fc8-4c62-af33-0ce7115159a1', name='SAP ECC - Accounts Payable'),\n",
       "\tDataPool(id='c564125e-f386-4e58-96fc-65d80dc7dd37', name='claude test'),\n",
       "\tDataPool(id='551d1182-d171-49d2-86db-b07929ff81cb', name='delete me init test'),\n",
       "\tDataPool(id='fdedf5c5-359f-4a57-98b4-d1df9038a98b', name='OCPM Testing Data Pool'),\n",
       "\tDataPool(id='d1c7cf68-68bd-43b0-83eb-92b619ae5620', name='claude test 2'),\n",
       "\tDataPool(id='c4bf51e0-ca47-4c2d-9722-8566fa75a3ad', name='delete me - A'),\n",
       "\tDataPool(id='15865e23-f7c6-43b9-ad67-ed51256cfe62', name='elena test'),\n",
       "\tDataPool(id='f34a2d68-7f88-4870-a790-3e26df6933ae', name='delete me - ccdm 3 test'),\n",
       "\tDataPool(id='f86e1bdd-3df2-4937-82d1-f158d5f22b2a', name='delete me - B'),\n",
       "\tDataPool(id='f408bc08-83ae-4f47-b826-f62ae5463a13', name='delete me catalog 3 extension'),\n",
       "\tDataPool(id='bea3eff2-138f-4069-9d8b-b13ad4b92b60', name='OCPM Extractions (SAP)'),\n",
       "\tDataPool(id='8a52b8af-bff9-4a6c-be4c-d768a36bccbb', name='OCPM Extractions (sap-ecc) (4)'),\n",
       "\tDataPool(id='c2db6aef-36d6-46c9-b53a-cec299acc969', name='delete me source'),\n",
       "\tDataPool(id='a51500ef-602f-48eb-b20d-7a09e3f1f3db', name='RfX Data Pool'),\n",
       "\tDataPool(id='e0581030-9a03-4497-822c-6c97c77c9b8e', name='delete me V3 test'),\n",
       "\tDataPool(id='a23925cd-2f5f-40cb-bdb4-619ba4f4b5ef', name='delete me - oracle test'),\n",
       "\tDataPool(id='f4c60d51-50f6-4107-8d03-66a74ff92c82', name='TEST'),\n",
       "\tDataPool(id='be97f0fc-758f-40ec-b719-20ee79c63886', name='delete me parameter test source'),\n",
       "\tDataPool(id='72c66e66-a673-4c99-92d4-adcbd4fb52d1', name='delete me parameter test target'),\n",
       "\tDataPool(id='42b5bda8-5308-4b1c-8f7b-408e0c92f404', name='delete me gr gi app'),\n",
       "\tDataPool(id='4493466f-15e0-4758-bd22-9cb9b0e631f9', name='delete me - zendesk test'),\n",
       "\tDataPool(id='064d779c-d5c8-4426-8de8-3bdf13608761', name='Salesforce Sales Cloud - Opportunity'),\n",
       "\tDataPool(id='2915c49f-36f4-42c9-8f61-d2f9a9d7c32a', name='OCPM Extractions (oracle-ebs)'),\n",
       "\tDataPool(id='6f99d6c7-5565-455b-829c-37e1dd406927', name='delete me empty 2'),\n",
       "\tDataPool(id='1a0edf4a-6397-417f-a86a-3713a964ef45', name='SAP Credit Management [AI Agent]'),\n",
       "\tDataPool(id='a378a13d-8da1-4971-89fc-6ce4c1068bc6', name='SAP Workload Monitor Data'),\n",
       "\tDataPool(id='c89a5f6e-25e6-4aa9-8fcd-255d080fe66b', name='Ingka Group [TEST]'),\n",
       "\tDataPool(id='4cec9f23-a84b-4aa4-b417-959665d3eda1', name='OCPM Extractions (sap-ecc) (2)'),\n",
       "\tDataPool(id='32287fc9-d4d7-40fb-b3ed-ebfba7282a8c', name='AF Monitoring App DP (outdated)'),\n",
       "\tDataPool(id='58d011f1-2105-4f79-b91f-fc580cfe8c98', name='SAP ECC - Purchase to Pay (3)'),\n",
       "\tDataPool(id='44658047-938c-4247-92d6-ff658faa1724', name='OCPM Data Pool'),\n",
       "\tDataPool(id='0cca1931-70ef-49b4-a097-332895c99c21', name='SAP ECC - Order to Cash (OM + AR)'),\n",
       "\tDataPool(id='22cbb399-5abd-4930-acea-0c40d09ebe01', name='SAP ECC - Accounts Receivable'),\n",
       "\tDataPool(id='67ee81f8-c1d5-476d-b00d-402a3947b3db', name='Accounts Receivable [Execution Apps]'),\n",
       "\tDataPool(id='c048a75d-6d7e-4b6a-9d25-821fd202a9d7', name='Delete Me Testing 2'),\n",
       "\tDataPool(id='85dd7564-e459-404e-803a-b669e8ecea6e', name='[DO NOT DELETE] - OCPM Extractions'),\n",
       "\tDataPool(id='d160202d-5ad8-47b3-8793-a9f995f3e0ef', name='delete me target'),\n",
       "\tDataPool(id='2d0b7496-e553-464c-a23a-ea0a0bbef59f', name='delete me empty'),\n",
       "\tDataPool(id='0ca99972-d4f3-4f94-96b6-aa7bad3c960d', name='OCPM Automation - Testing Pool'),\n",
       "\tDataPool(id='77fc7727-fe19-4f60-b032-c0c791b1adad', name='SAP ECC -  P2P-AP Multi-Event-Log'),\n",
       "\tDataPool(id='b492fe61-2cc4-45a2-81a4-457e0f8cb0ce', name='SAP ECC -  P2P-AP Multi-Event-Log (1)'),\n",
       "\tDataPool(id='51b19223-55c9-4036-bb5e-88f121ab1246', name='SAP ECC - Purchase to Pay (6)'),\n",
       "\tDataPool(id='3bfcdb81-de9c-43aa-8705-c492adc716ca', name='SAP ECC - Purchase to Pay (7)'),\n",
       "\tDataPool(id='63693cc0-d379-4d11-9027-bcf5844ba049', name='OCPM Extractions (sap-ecc) (6)'),\n",
       "\tDataPool(id='aa2e3419-e3b0-430f-9d0e-fb91e75aff06', name='OCPM Extractions (oracle-ebs) (1)'),\n",
       "\tDataPool(id='ab98ca5b-001a-409b-828d-37f1a8a14fc1', name='delete me - scnv app mad tool'),\n",
       "\tDataPool(id='c9a22b64-1758-458b-812a-8b6e83439323', name='delte me - mad 3 test js'),\n",
       "\tDataPool(id='09069a93-c9df-48dd-8707-b102cc583156', name='Action Flow Monitoring (outdated)'),\n",
       "\tDataPool(id='a0897e56-3d9a-448c-a896-89237f29cfca', name='Data Synthesis'),\n",
       "\tDataPool(id='01eddee0-88af-408d-95b1-e4ca4f12dd04', name='SAP ECC - Order to Cash'),\n",
       "\tDataPool(id='5dfeccc0-15c7-4a13-acf4-fcccf093fa3b', name='APC Estimator'),\n",
       "\tDataPool(id='587f57be-417b-45d4-9454-b1a9b533cda1', name='delete_me_demo_gen_user_token_test'),\n",
       "\tDataPool(id='25574cb6-e0f1-44b0-91ae-ef0cc75cbb4f', name='Delete Me - Testing Pool'),\n",
       "\tDataPool(id='c28e22cd-cad9-4310-853a-d5f67405c0bf', name='Rossum Testing'),\n",
       "\tDataPool(id='48895725-0e63-4d30-8e17-19f0b4ed41e6', name='ABAP'),\n",
       "\tDataPool(id='3860f3a1-9718-4554-8d3d-5d0810eadfbf', name='ABAP'),\n",
       "\tDataPool(id='2ec52c5d-52f5-4abc-a401-330c28e2f9e3', name='SAP ECC - Purchase to Pay'),\n",
       "\tDataPool(id='f4e3a705-35e2-4403-b4d3-f05e1b637986', name='SAP ECC - Purchase to Pay (1)'),\n",
       "\tDataPool(id='f467a500-d818-4070-8fc2-c7bb2e3aa90b', name='SAP ECC - Purchase to Pay (2)'),\n",
       "\tDataPool(id='c51cf8fa-56c9-4a35-88b8-be9426807c7c', name='Innovations_automatization'),\n",
       "\tDataPool(id='63f35704-bbd5-4a27-b096-ba47a8c41de4', name='AF Tracking'),\n",
       "\tDataPool(id='2adc7131-7075-42f7-822d-bdfe546a07d7', name='Oracle EBS - Accounts Payable')\n",
       "]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pools = c.data_integration.get_data_pools()\n",
    "data_pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fb674a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' monitoring = data_pools.find(\"Sarvesh Monitoring Pool\")\\n\\n\\n# 1. LINEAGE TABLE (STUDIO)\\ncolumn_config_studio = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_combined.columns\\n]\\nmonitoring.create_table(\\n    df=df_combined, \\n    table_name=\\'lineage_frontend\\', \\n    column_config=column_config_studio, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded frontend lineage table\")\\n\\n# 2. MAPPING NODES\\ncolumn_config_nodes = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_mapping_nodes.columns\\n]\\nmonitoring.create_table(\\n    df=df_mapping_nodes, \\n    table_name=\\'mapping_nodes_frontend\\', \\n    column_config=column_config_nodes, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded frontend mapping nodes table\")\\n\\n# 3. BRIDGE TABLE\\ncolumn_config_bridge = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_bridge.columns\\n]\\nmonitoring.create_table(\\n    df=df_bridge, \\n    table_name=\\'bridge_lineage_mapping\\', \\n    column_config=column_config_bridge, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded bridge table\")\\n\\n# 4. USED REPORT TABLE\\ncolumn_config_report = [\\n    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \\n    for c in df_final.columns\\n]\\nmonitoring.create_table(\\n    df=df_final, \\n    table_name=\\'usedtables_report_frontend\\', \\n    column_config=column_config_report, \\n    drop_if_exists=True\\n)\\nprint(\" Uploaded used tables report\") '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" monitoring = data_pools.find(\"Sarvesh Monitoring Pool\")\n",
    "\n",
    "\n",
    "# 1. LINEAGE TABLE (STUDIO)\n",
    "column_config_studio = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_combined.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_combined, \n",
    "    table_name='lineage_frontend', \n",
    "    column_config=column_config_studio, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend lineage table\")\n",
    "\n",
    "# 2. MAPPING NODES\n",
    "column_config_nodes = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_mapping_nodes.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_mapping_nodes, \n",
    "    table_name='mapping_nodes_frontend', \n",
    "    column_config=column_config_nodes, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded frontend mapping nodes table\")\n",
    "\n",
    "# 3. BRIDGE TABLE\n",
    "column_config_bridge = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_bridge.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_bridge, \n",
    "    table_name='bridge_lineage_mapping', \n",
    "    column_config=column_config_bridge, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded bridge table\")\n",
    "\n",
    "# 4. USED REPORT TABLE\n",
    "column_config_report = [\n",
    "    ColumnTransport(column_name=c, column_type=ColumnType.STRING, field_length=255) \n",
    "    for c in df_final.columns\n",
    "]\n",
    "monitoring.create_table(\n",
    "    df=df_final, \n",
    "    table_name='usedtables_report_frontend', \n",
    "    column_config=column_config_report, \n",
    "    drop_if_exists=True\n",
    ")\n",
    "print(\" Uploaded used tables report\") \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
